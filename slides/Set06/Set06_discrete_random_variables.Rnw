\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set06 - Discrete Random Variables}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Random variables}
\begin{frame}
\frametitle{Random variables}
\setkeys{Gin}{width=0.15\textwidth}

\begin{definition}
A \alert{random variable} $X$ is a function $X:\mOmega \mapsto \mathbb{R}$.
\end{definition}
\pause
Intuitive idea:  \textcolor{cyan}{If the value of a numerical variable depends on the outcome of an experiment, we call the variable a {\it random variable}.}

\pause
\begin{example}
Simple dartboard:

\end{example}
\begin{center}
\includegraphics{dartboard}
\end{center}
Imagine we throw three darts on this board one by one and we are interested in the number of times the red area has been hit. \pause
This count is a random variable!
\end{frame}


\begin{frame}
\frametitle{Dartboard example (cont.)}

\begin{itemize}
\item Let $X$ the number of times that the red area is hit out of three throws. \pause
\item $X(\omega)=  x$, if the outcome $\omega$ has $x$ hits to the red area, and $3-x$ hits to the gray area. \pause
\item Thus, the set of possible values for $X(\omega)$ is $\{0,1,2,3\}$. \pause
\item To avoid cumbersome notation, we write
\[ \{X = x\} \]
for the event
\[
\{ \omega | \omega \in \mOmega\ \text{ and }\ X(\omega) = x \}.
\]
\item By convention, we use a capital letter to indicate the random variable and a lower case letter to indicate a realized value of the random variable.
\end{itemize}

\end{frame}





\begin{frame}
\frametitle{8 bit example}

\begin{example}
Suppose, 8 bits are sent through a communication channel.
Each bit has a certain probability to be received incorrectly. 
We are interested in the number of bits that are received incorrectly.
\end{example}

\pause 
\begin{itemize}
\item Let $X$ be the number of incorrect bits received. \pause 
\item The possible values for $X$ are $\{0,1,2,3,4,5,6,7,8\}$. \pause
\item Example events: 
	\begin{itemize}
	\item No incorrect bits received: \pause $\{X=0\}$. \pause
	\item At least one incorrect bit received: \pause $\{X\ge 1\}$. \pause
	\item Exactly two incorrect bits received: \pause $\{X=2\}$. \pause
	\item Between two and seven (inclusive) incorrect bits received: \pause $\{2\le X \le 7\}$.
	\end{itemize}
\end{itemize}
\end{frame}



\subsection{Image}
\begin{frame}
\frametitle{Image of random variables}

\begin{definition}
The \alert{image} of a random variable $X$ is defined as 
\[ 
Im(X) := \{x: x=X(\omega) \text{ for some }\omega \in \mOmega \}
\]
\pause
If the image is finite or countable, we have a \alert{discrete} random variable. \pause
If the image is uncountably infinite, we have a \alert{continuous} random variable.
\end{definition}

\pause

\begin{example}
\begin{itemize}
\item Put a disk drive into service, measure $Y$ = ``time till the first major failure'' \pause and thus
		$Im(Y) = (0, \infty)$. \pause Image of $Y$ is an interval (uncountable image), so $Y$ is a
  continuous random variable.
\item Communication channel: $X$ = ``\# of incorrectly received bits'' \pause with 
		$Im(X) = \{ 0,1,2,3,4,5,6,7,8 \}$. \pause Image of $X$ is a finite set, so $X$ is
	a discrete random variable.
\end{itemize}
\end{example}
\end{frame}



\section{Discrete random variables}
\subsection{Distribution}
\begin{frame}
\frametitle{Distribution}

The collection of all the probabilities related to $X$ is the \alert{distribution} of $X$. 

\vspace{0.2in} \pause 

For a discrete random variable, the function
\[ 
p_X(x) = P(X=x) 
\]
is the \alert{probability mass function} (pmf) \pause and the \alert{cumulative distribution function} (cdf) is 
\[ 
F(x) = P(X\le x) =\sum_{y\le x} P(y).
\]
\pause 
The set of possible values of $X$ is called the \alert{support} of the distribution $F$ and is the same as the image of $X$. 
\end{frame}




\begin{frame}
\frametitle{Examples}
A probability mass function is valid if it defines a valid set of probabilities, i.e. 
\begin{itemize}
\item the probabilities are non-negative,
\item the probabilities sum to 1. 
\end{itemize}

\vspace{0.2in} \pause

\begin{example}
Which of the following functions are a valid probability mass functions?
\begin{itemize}[<+->]
\item
    	\begin{tabular}{c|ccccc}
		$x$ & -3 & -1 & 0 & 5 & 7  \\
		\hline
		$P_{X}(x)$ & 0.1 & 0.45 & 0.15 & 0.25 & 0.05  \\
	\end{tabular}
    \item
    	\begin{tabular}{c|ccccc}
		$y$ & -1 & 0 & 1.5 & 3 & 4.5 \\
		\hline
		$P_{Y}(y)$ & 0.1 & 0.45 & 0.25 & -0.05 & 0.25  \\
	\end{tabular}
    \item
    	\begin{tabular}{c|ccccc}
		$z$ & 0 & 1 & 3 & 5 & 7  \\
		\hline
		$P_{Z}(z)$ & 0.22 & 0.18 & 0.24 & 0.17 & 0.18  \\
	\end{tabular}
\end{itemize}
\end{example}
\end{frame}


\subsection{Die rolling}
\begin{frame}[fragile]
\frametitle{Rolling fair 6-sided fair dice}
\begin{example}
Let $Y$ be the number of pips on the upturned face of a die. \pause 
The image of $Y$ is  \pause $\{1,2,3,4,5,6\}$.	\pause
The probability mass function for $Y$ therefore is 
\[ \arraycolsep=2pt\def\arraystretch{2.2} 
\begin{array}{c|cccccc}
\hline
x & 1 & 2 & 3 & 4 & 5 & 6 \pause \\
\hline
P(X=x) & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6}  & \frac{1}{6} \pause \\ 
P(X\le x) & \frac{1}{6} & \frac{2}{6} & \frac{3}{6} & \frac{4}{6} & \frac{5}{6}  & \frac{6}{6} \\
\hline
\end{array} \]
\end{example}
\end{frame}


\subsection{Dragonwood}
\begin{frame}[fragile]
\frametitle{Dragonwood}
\begin{example}
Dragonwood has 6-sided dice with the following \# on the 6 sides: $\{1,2,2,3,3,4\}$. 

\vspace{0.1in} \pause

What is the image, pmf, and cdf for the sum of the upturned numbers when rolling 3 Dragonwood dice? \pause
<<dragonwood, echo=TRUE, tidy=FALSE>>=
# Three dice
die   = c(1,2,2,3,3,4)
rolls = expand.grid(die1 = die, die2 = die, die3 = die)
sum   = rowSums(rolls); tsum = table(sum)
dragonwood3 = data.frame(x   = round(as.numeric(names(tsum)),3),
                         pmf = round(as.numeric(table(sum)/length(sum)),3)) %>%
	mutate(cdf = cumsum(pmf))

t(dragonwood3)
# round(dragonwood3$pmf,3)
# round(dragonwood3$cdf,3)
@
\end{example}
\end{frame}


\begin{frame}
\frametitle{Dragonwood - pmf and cdf}

<<dragonwood_plots, dependson='dragonwood', fig.width=8>>=
g_pmf = ggplot(dragonwood3, aes(x=x, y=pmf)) + 
	geom_point() + 
	scale_x_continuous(breaks=3:12) + 
	ylim(0,1)

# set  up cdf plot
cdf = bind_rows(data.frame(x=2,pmf=0,cdf=0), dragonwood3) %>% 
	mutate(xend = c(x[-1],13))


g_cdf = ggplot(cdf, aes(x=x, y=cdf, xend=xend, yend=cdf)) +
	#      geom_vline(aes(xintercept=x), linetype=2, color="grey") +
	geom_point() +  # Solid points to left
	geom_point(aes(x=xend, y=cdf), shape=1) +  # Open points to right
	geom_segment() + 
	scale_x_continuous(breaks=3:12) +
	coord_cartesian(xlim=c(3,12))

grid.arrange(g_pmf,g_cdf, nrow=1)
@

\end{frame}



\begin{frame}
\frametitle{Properties of pmf and cdf}

Properties of probability mass function $P(X=x)$:
\begin{itemize}
\item $0 \le P(X=x) \le 1$ for all $x \in \mathbb{R}$.
\item $\sum_{x\in S} P(X=x) = 1$ where $S$ is the support.
\end{itemize}

\vspace{0.2in} \pause

Properties of cumulative distribution function $F_X(x)$:
\begin{itemize}
\item $0 \le F_{X}(x) \le 1$ for all $x \in \mathbb{R}$
\item $F_{X}$ is nondecreasing, (i.e. if $x_{1} \le x_{2}$ then $F_{X}(x_{1}) \le F_{X}(x_{2})$.)
\item $\lim_{x \rightarrow -\infty}F_{X}(x) = 0$ and $\lim_{x \rightarrow \infty}F_{X}(x) = 1$.
\item %$F_{X}(t)$ has a positive jump equal to $p_{X}(x_{i})$ at $\{
    %x_{1}, x_{2}, x_{3}, \ldots \}$; $F_{X}$ is constant in the interval $[x_{i},
    %x_{i+1})$, i.e.
    $F_X(x)$ is right continuous with respect to $x$
\end{itemize}


\end{frame}

% \foilhead[-.8in]{\textcolor{blue}{Properties of $F_X$}}\vspace{.2cm}
% %\no  {$F_X(x)=P(X\leq x)$ is cumulative distribution function, cdf }\\[.2in]
% \no  \no {\textcolor{red}{Properties of $F_{X}$:} }   
% The following properties hold for the cdf
% $F_{X}$ of a  random variable $X$.
%  \begin{itemize}
%     \item $ 0 \le F_{X}(t) \le 1$ for all $t \in \mathbb{R}$
%     \item $F_{X}$ is nondecreasing, (i.e. if $x_{1} \le x_{2}$
%     then $F_{X}(x_{1}) \le F_{X}(x_{2})$.)
%     \item $\lim_{t \rightarrow -\infty}F_{X}(t) = 0$ and $\lim_{t \rightarrow \infty}F_{X}(t) = 1$.
%     \item %$F_{X}(t)$ has a positive jump equal to $p_{X}(x_{i})$ at $\{
%     %x_{1}, x_{2}, x_{3}, \ldots \}$; $F_{X}$ is constant in the interval $[x_{i},
%     %x_{i+1})$, i.e. 
%     $F_X(t)$ is right continuous with respect to $t$
% \end{itemize}
% %\no If $x\leq y$, what how will $F_X(x)$ and $F_Y(y)$ be related? 

 
\begin{frame}
\frametitle{Dragonwood (cont.)}

In Dragonwood, you capture monsters by rolling a sum equal to or greater than its defense. \pause
Suppose you have 3 dice and the following monsters are available to be captured:
\begin{itemize}
\item A monster worth 1 victory point with a defense of 3.
\item A monster worth 3 victory points with a defense of 7.
\item A monster worth 4 victory points with a defense of 8. 
\end{itemize}
\pause
Which monster should your attack?

\vspace{0.1in} \pause

We can calculate the probability of defeating each monster by computing one minus the cdf evaluated at ``defense minus 1''. \pause
Let $X$ be the sum of the number on 3 Dragonwood dice. \pause
Then
\begin{itemize}
\item $P(X\ge 3) = 1-P(X\le 2) = 1$
\item $P(X\ge 7) = 1-P(X\le 6) = 0.722$.
\item $P(X\ge 8) = 1-P(X\le 7) = 0.5$. 
\end{itemize}
\end{frame}



\subsection{Expectation}
\begin{frame}
\frametitle{Expectation}

\begin{definition}
Let $X$ be a random variable and $h$ be some function. 
The \alert{expected value} of a function of a (discrete) random variable is 
\[ 
E[h(X)] = \sum_i h(x_i) \cdot p_X(x_i).
\]
\pause
If $h(x)=x$, then 
\[ 
E[X] = \sum_i x_i \cdot p_X(x_i)
\]
\pause
and we call this the \alert{expectation} of $X$. 
\end{definition}

\pause

Expected values are \emph{weighted averages} of the possible values weighted by their probability. 
\end{frame}




\begin{frame}[fragile]
\frametitle{Dragonwood (cont.)}

What is the expectation of the sum of 3 Dragonwood dice? 
<<dragonwood_expectation, dependson='dragonwood', echo=TRUE>>=
expectation = with(dragonwood3, sum(x*pmf))
expectation
@

\pause

The expectation can be thought of as the \emph{center of gravity} of masses $P(X=x)$ placed at corresponding points $x$. \pause
<<dragonwood_center_of_gravity, dependson='dragonwood_expectation', fig.height=2>>=
ggplot(dragonwood3 %>% mutate(y=0), aes(x=x, xend=x, y=y, yend=pmf)) + 
	geom_segment(size=2) + 
  geom_point(data = data.frame(x=7.5,y=-0.01,pmf=0), aes(x,y), shape=24, color='red', fill='red') + 
	scale_x_continuous(breaks=3:12)
@
\end{frame}



\begin{frame}
\frametitle{Biased coin}

Suppose we have a biased coin represented by the following probability mass function:
\[ \begin{array}{ccc}
\hline
y & 0 & 1 \\
p_Y(y) & 1-p & p \\
\hline
\end{array} \]
\pause
What is the expected value? % E[Y] = p

\vspace{0.2in} \pause

If $p=0.9$, 
<<biased_coin, fig.height=2>>=
d = data.frame(x=c(0,1), pmf=c(0.1,0.9))

ggplot(d %>% mutate(y=0), aes(x=x, xend=x, y=y, yend=pmf)) + 
	geom_segment(size=2) + 
  geom_point(data = data.frame(x=0.9,y=-0.04,pmf=0), aes(x,y), shape=24, color='red', fill='red') + 
	scale_x_continuous(breaks=3:12) + 
	labs(x='y', y='pmf')
@
\end{frame}


\subsection{Properties of expectations}
\begin{frame}
\frametitle{Properties of expectations}

\begin{theorem}
Let $X$ and $Y$ be random variables and $a$, $b$, and $c$ be constants. \pause
Then 
\[ \begin{array}{rl}
E[aX + bY+c] &= aE[X] + bE[Y] + c.
\end{array} \]
\end{theorem}

\pause

In particular
\begin{corollary}
\begin{itemize}[<+->]
\item $E[X+Y] = E[X]+E[Y]$,
\item $E[aX] = a E[X]$, and
\item $E[c] = c$.
\end{itemize}
\end{corollary}

\end{frame}



\begin{frame}
\frametitle{Dragonwood (cont.)}

Enhancement cards in Dragonwood allow you to improve your rolls. \pause Here are two enhancement cards:
\begin{itemize}
\item \emph{Cloak of Darkness} adds 2 points to all capture attempts \pause and
\item \emph{Friendly Bunny} allows you (once) to roll an extra die. 
\end{itemize}

\vspace{0.1in} \pause

What is the expected attack roll total if you had 3 Dragonwood dice, the Cloak of Darkness, and are using the Friendly Bunny? 

\vspace{0.2in} \pause

Let 
\begin{itemize}
\item $X$ be the sum of 3 Dragonwood dice (we know $E[X]=7.5$), 
\item $Y$ be the sum of 1 Dragonwood die \pause which has $E[X] = 2.5$.
\end{itemize}

\pause

Then the attack roll total is $X+Y+2$ and the \emph{expected} attack roll total is 
\[ 
E[X+Y+2] = E[X]+E[Y]+2 = 7.5+2.5+2 = 12,
\]
\pause
or the attack roll is $4Y+2$ and the \emph{expected} attack roll total is 
\[ 
E[4Y+2] = 4E[Y]+2 = 12.
\]

\end{frame}



\subsection{Variance}
\begin{frame}
\frametitle{Variance}

\begin{definition}
The \alert{variance} of a random variable is defined as the expected squared deviation from the mean. \pause 
For discrete random variables, variance is
\[
Var[X] = E[(X-\mu)^2] = \sum_i (x_i-\mu)^2 \cdot p_X(x_i)
\]
where $\mu = E[X]$. \pause
The symbol $\sigma^2$ is commonly used for the variance.
\end{definition}

\pause

\begin{definition}
The \alert{standard deviation} is the positive square root of the variance
\[
SD[X] = \sqrt{Var[X]}.
\]
\pause
The symbol $\sigma$ is commonly used for the standard deviation.
\end{definition}
\end{frame}



\begin{frame}[fragile]
\frametitle{Dragonwood (cont.)}

What is the variance for the sum of the 3 Dragonwood dice?

<<dragonwood_variance, dependson='dragonwood_expectation', echo=TRUE>>=
variance = with(dragonwood3, sum((x-expectation)^2*pmf))
variance
@

\pause

What is the standard deviation for the sum of the pips on 3 Dragonwood dice?
<<dragonwood_standard_deviation, dependson='dragonwood_variance', echo=TRUE>>=
sqrt(variance)
@
\end{frame}



\begin{frame}
\frametitle{Biased coin}

Suppose we have a biased coin represented by the following probability mass function:
\[ \begin{array}{ccc}
\hline
y & 0 & 1 \\
p_Y(y) & 1-p & p \\
\hline
\end{array} \]
\pause
What is the variance?

\pause

\begin{enumerate}
\item $E[Y] = p$ 
\item $V[y] = (0-p)^2(1-p) + (1-p)^2\times p = p-p^2 = p(1-p)$
\end{enumerate}

\pause
When is this variance maximized?

\pause
<<biased_coin_variance, fig.height=2>>=
opar = par(mar=c(4,4,1,0)+.1)
variance = function(p) p*(1-p)
curve(variance, xlab='y', ylab='variance')
par(opar)
@
\end{frame}




% \begin{frame}
% \frametitle{Multiple random variables}
% 
% \begin{definition}
% The \alert{joint probability mass function} of two random variables $X$ and $Y$ is 
% \[ 
% p_{X,Y}(x,y) = P(X=x,Y=y).
% \]
% \pause
% The \alert{joint probability mass function} of a random vector $X=(X_1,\ldots,X_p)$ is 
% \[ 
% P_X(x) = P(X=x). 
% \]
% \end{definition}
% 
% \end{frame}
% 
% 
% \subsection{Covariance}
% \begin{frame}
% \frametitle{Covariance}
% 
% \begin{definition}
% The \alert{covariance} between two random variables $X$ and $Y$ is 
% \[ \begin{array}{rl}
% Cov[X,Y] 
% &= E\left[ (X-E[X])(Y-E[Y]) \right] \\
% &= E[XY] - E[X]E[Y]
% \end{array} \]
% \end{definition}
% 
% \pause
% 
% \begin{definition}
% The \alert{correlation} coefficient between two random variables $X$ and $Y$ is 
% \[ \begin{array}{rl}
% \rho = \frac{Cov[X,Y]}{\sqrt{Var[X] Var[Y]}} = \frac{Cov[X,Y]}{SD[X] SD[Y]}.
% \end{array} \]
% \pause
% If $rho=0$, we say that $X$ and $Y$ are uncorrelated. 
% \end{definition}
% 
% \end{frame}
% 
% 
% 
% 
% 
% \subsection{Properties of variances}
% \begin{frame}
% \frametitle{Properties of variance}
% 
% \begin{theorem}
% Let $X$ and $Y$ be random variables and $a$, $b$, and $c$ be constants. \pause
% Then 
% \[ \begin{array}{rl}
% Var[aX + bY+c] &= a^2Var[X] + b^2Var[Y] + 2ab Cov(X,Y).
% \end{array} \]
% \end{theorem}
% 
% \pause
% 
% % needs to be fixed
% % In particular
% % \begin{corollary}
% % \begin{itemize}[<+->]
% % \item $Var[X+Y] = Var[X]+Var[Y]$,
% % \item $Var[aX] = a Var[X]$, and
% % \item $Var[c] = c$.
% % \end{itemize}
% % \end{corollary}
% 
% \end{frame}


\end{document}


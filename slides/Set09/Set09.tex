\documentclass[20pt,landscape]{foils}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{pause}
\usepackage{graphicx}
\usepackage{epsfig}
%\usepackage{geometry}
%\geometry{headsep=3ex,hscale=0.9}
\newcommand{\bd}{\textbf}
\newcommand{\no}{\noindent}
\newcommand{\un}{\underline}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand \h {\hspace*{.3in}}
\newcommand{\bul}{\hspace*{.3in}{\textcolor{red}{$\bullet$ \ }}}
\newcommand{\xbar}{\bar{x}}
\rightheader{Stat 330 (Fall 2015): slide set 9}

\begin{document}
\LogoOff

\foilhead[1.3in]{}
\centerline{\LARGE \textcolor{blue}{Slide set 9}}
\vspace{0.3in}
\centerline{\large Stat 330 (Fall 2015)}
\vspace{0.2in}
\centerline{\tiny Last update: \today}
\setcounter{page}{0}

\foilhead[-.8in]{\textcolor{blue}{Special discrete pmfs}}
\no  {\textcolor{magenta}{Intuitive idea:}  {\textcolor{cyan}{In many theoretical and practical problems, several probability mass functions occur often enough to be worth exploring.}} \\[.1in]
\no  {\textcolor{red}{Common feature:}  {\textcolor{black}{The sample space is always finite or countably many.}} \\[.1in]
\no   {\textcolor{magenta}{Example: }
 \begin{enumerate}
\item   Bernoulli distribution
\item Binomial distribution
\item Geometric distribution
\item Poisson distribution
\item Compounded distribution
\end{enumerate}


\foilhead[-.8in]{\textcolor{blue}{Bernoulli distribution}}
\no  {\textcolor{magenta}{Situation:}  {\textcolor{cyan}{Bernoulli experiment (only two outcomes: {\it success/failure}) with}}\\[.1in]
\no \hspace*{1.1in}  {\textcolor{cyan}{$P(\text{ success }) = p$}}\\[.2in]
\no We define a random variable $X$ as:
\no $$ X( \text{ failure }) = 0 \hspace{2cm}  X( \text{ success }) = 1 $$
\no The probability mass function $p_{X}$ of $X$ is then:
$$ p_{X}(0) = 1-p \hspace{2cm} p_{X}(1) = p $$
%This probability mass function is called the  {\textcolor{cyan}{{\it Bernoulli mass
%function}}
The random variable is said to follow Bernoulli distribution
}.

\foilhead[-.8in]{\textcolor{blue}{Bernoulli experiment: Examples}}
\begin{itemize}
\item[\bul] Toss a coin: $\Omega = \{H,T\}$
\item[\bul] Throw a fair die and ask if the face value is a six: $\Omega = \{{\text{face value is a six},\text{face value is not a six}}\}$
\item[\bul] Sent a message through a network and record whether or not it is received: $\Omega = \{ \text{successful transmission}, \text{ unsuccessful transmission}\}$
\item[\bul] Draw a part from an assembly line and record whether or not it is defective: $\Omega = \{\text{defective }, \text{good}\}$
\item[\bul] Response to the question ``Are you in favor of the above measure''? (in reference to a new tax levy to  be imposed on city residents): $\Omega = \{\text{yes}, \text{no}\}$
\end{itemize}
\foilhead[-.8in]{\textcolor{blue}{Bernoulli random variable (cont'd)}}
\no {\textcolor{magenta}{The cdf of the Bernoulli distribution}, $F_{X}$ is then:}
\[
F_{X}(t) =P(X\leq t)= \left \{
\begin{array}{cl}
    0 & t < 0 \\
    1-p & 0 \le t < 1 \\
    1 & 1 \le t
\end{array}
\right .
\]
This function is called the {\textcolor{red}{ Bernoulli cumulative distribution
function (cdf)}}.\\[.2in]
\no {\textcolor{magenta}{Expected value and Variance of a Bernoulli random variable:}}\\[.2in]
\no \h \h \bul $E(X)=\sum\limits_{x} p_X(x)=0(1-p)+1\cdot p=p$\\[.1in]
\no \h  \h \bul $\text{Var}(X)=\sum\limits_{x}(x-E(X))^2p_X(x)= (0-p)^2\cdot (1-p)+(1-p)^2 \cdot p$\\[.1in]
\no \hspace*{1in} $= p(1-p)$



\foilhead[-.8in]{\textcolor{blue}{Sequence of Bernoulli Experiments}}
\no A compound experiment consisting of $n$ {\textcolor{cyan}{independent and identically distributed}} Bernoulli experiments.\\[.1in]
\no \no  {\textcolor{magenta}{Example:}} {\textcolor{cyan}{Sequences of Bernoulli experiments}}\\[-.2in]
\begin{itemize} 
\item[\bul] Toss a coin $n$ times. 
\item[\bul] Send 23 identical messages through the network independently.
\item[\bul] Draw 5 cards from a standard deck with replacement and record whether or not the card is a king.
\end{itemize}
\no What does {\textcolor{cyan}{independent and identically distributed}} mean?\\[.1in]
\begin{itemize}
\item[\bul] Saying that the trials are independent means, for example, that 
\begin{eqnarray*}P(\text{ trial 1 a success } \text{ and} \text{ trial 2 a failure }, \text{and} \ldots \text{ trial }k\text{ a failure}) = \\
P(\text{ trial 1 a success})P(\text{ trial 2 a failure })\ldots P(\text{ trial }k\text{ a failure}).\
\end{eqnarray*}
\item[\bul] Saying that the trials are identically distributed means that 
\begin{eqnarray*}
P(\text{ trial 1 a success})=P(\text{ trial 2 a success }) = \ldots\\ =P(\text{ trial }k\text{ a success}) = p \\
P(\text{ trial 1 a failure})=P(\text{ trial 2 a failure }) = \ldots \\=P(\text{ trial }k\text{ a failure}) = q=1-p 
\end{eqnarray*}
\item[\bul] Shorthand for {\textcolor{cyan}{independent and identically distributed}} is {\textcolor{red}{iid}}.
\end{itemize}



\foilhead[-.8in]{\textcolor{blue}{Binomial distribution}}
\no  {\textcolor{magenta}{Situation:}  {\textcolor{cyan}{$n$ sequential Bernoulli experiments, with success rate
$p$ for a single trial. Single trials are independent from each other.}} \\[.1in]
\no We are only interested in the number of successes in total after $n$ trials, random variable $X$ is then:
\[
X = \text{``number of successes in } n \text{ trials''}
\]\\[-.5in]
\no This leads to a sample space of
\[
\Omega = \{ 0 , 1 , 2 , \ldots, n \}
\]\\[-.5in]
\no  {\textcolor{magenta}{ What is the general expression for $p_{X}(k)$ for all possible
$k = 0, \ldots, n$}
$$p_{X}(k) = P( X = k)$$



\foilhead[-.8in]{\textcolor{blue}{Derivation of Binomial pmf }}
\no We want to find the probability, that
in a sequence of $n$ trials there are exactly $k$ successes. If $s$ is a
particular sequence with $k$ successes and $n-k$ failures, we
already know the probability:
\[
P(s) = p^{k}(1-p)^{n-k}.
\]
\no  {\textcolor{red}{ Think:} Now we need to know, how
many} possibilities there are, to have $k$ successes in $n$ trials:}
think of the $n$ trials as numbers from 1 to $n$. \\[.1in]
\no There are $n \choose k$ possible ways of selecting $k$  numbers out of the
$n$ possible numbers. $p_{X}(k)$ is therefore (by summation rule):
\[
p_{X}(k) = {n \choose k} p^{k}(1-p)^{n-k}.
\]
This probability mass function is called the  {\textcolor{red}{  Binomial mass
function.}


\foilhead[-.8in]{\textcolor{blue}{Binomial distribution (cont'd)}}
\no  {\textcolor{red} {Remark:}} Any Binomial variable $X$ can be represented as a sum of \emph{iid} Bernoulli variables:\\[.1in]
\no \hspace*{1.5in}$X=X_1+\cdots+X_n$\\[.1in]
\no where $X_i\sim$ Bernoulli$(p)$\\[.1in]
\no   {\textcolor{magenta}{Expected value and Variance of Binomial random variable:}}\\[.1in]
\no \h  \bul $E(X)=\sum\limits_{i=1}^n E(X_i)=np$\\[.1in]
\no \h  \bul $\text{Var}(X)=\sum\limits_{i=1}^n\text{Var}(X_i)=np(1-p)$ (since $X_1, X_2,\ldots,x_n$ are iid)\\[.1in]
\no The cdf of X, $F_{X}$ is:
\[
F_{X}(t) = P(X\leq t)=\sum_{i=0}^{\lfloor{t}\rfloor} {n \choose i} p^{i}(1-p)^{n-i} =:
B_{n,p}(t)
\]


\foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example}}
\no  {\textcolor{blue} {Example:}} Compute the probabilities for the following events: \\[.1in]
    A box contains 15 components that each have a failure rate of 5\%.
    What is the probability that
    \begin{enumerate}
	\item exactly two out of the fifteen components are defective?
	\item at most two components are defective?
	\item more than three components are defective?
	\item more than 1 but less than 4 are defective?
    \end{enumerate}
 \no   {\textcolor{blue} {Solution:}}   Let $X$ be the number of defective components. Then $X$ has a
    $B_{15,0.05}$ distribution.
    %We shall look-up values of $B_{15,0.05}$ from Table A2.  of Baron.


\foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example (Continued...)}}
 \begin{enumerate}
	\item
	$\displaystyle P( \text{exactly two out of the fifteen components are
	defective} ) = p_{X}(2) = {15 \choose 2} 0.05^{2} 0.95^{13} = 0.1348.$
	\item
	$\displaystyle P( \text{at most two components are defective} ) = P(X \le 2) =
	B_{15,0.05}(2) = 0.9638.$
	\item $\displaystyle P(\text{ more than three components are defective }) = P(X > 3) =
	1 - P( X \le 3) = 1 - 0.9945 = 0.0055.$
	\item $\displaystyle P(\text{ more than 1 but less than 4 are defective }) = P( 1 <
	X < 4) = P(X \le 3) - P(X \le 1) = 0.9945 - 0.8290 = 0.1655.$
    \end{enumerate}
\foilhead[-.8in]{\textcolor{blue}{Example from Baron}}
\no {\textcolor{magenta}{Example from Baron: }}{\textcolor{cyan}{As part of a business strategy, randomly selected 20\% of new internet service subscribers receive a special promotion from the provider. A group of 10 neighbors signs up for the service. What is the probability that at least 4 of them get a special promotion?}}\\[.1in]
\no {\textcolor{magenta}{Solution:}} \\[.1in]
\no Need to calculate $P(X \ge 4)$ where $X$ is the number of people out of 10 who will receive the special promotion.\\[.1in]
\no Modelling $X$ as the number of successes in 10 Bernoulli trials each with $P(\text{success})=0.2$, we have that $X \sim Bin(10,0.2)$\\[.1in]
\no Thus
$$ P(X \ge 4) = 1-P(X\le 3)= 1- B_{10,0.2}(3)=1-0.8791=0.1209$$

\foilhead[-.8in]{\textcolor{blue}{Geometric distribution}}\vspace{5pt}
\no  {\textcolor{magenta}{Situation:} \\[.1in] {\textcolor{cyan}{1. We have a single Bernoulli experiment with probability for
success $p$. \\[.1in]
2. Now, we repeat this experiment until we have a first success. \\[.1in]
3. Denote by $X$ the number of repetitions of the experiment  until we
have the first success, i.e  $X = k$ implies that, we  have $k-1$ failures and the first
success in the $k$th repetition of the experiment.}} \\[.1in]
\no  {\textcolor{red}{Question:}  It is very natural to ask what is the probability distribution of $X$? \\[.1in]
\no {\textcolor{red}{Pmf: }The sample space $\Omega$ is infinite and starts at 1 (we
need at least one success):
\[
\Omega = \{ 1, 2, 3, 4, \ldots \}
\] and the pmf
\[
p_{X}(k) = P(X=k) = \underbrace{(1-p)^{k-1}}_{k-1 \text{ failures}} %\qquad \text{for} k=1,2,3,\ldots
\cdot \underbrace{p}_{\text{success!}}
\]
 
 

\foilhead[-.75in]{\textcolor{blue}{Geometric distribution (cont'd)}}
\no  This probability mass function is called the {\textcolor{red} {Geometric mass function.}} \\[.15in]
\no   {\textcolor{magenta}{Expectation and Variance of the Geometric random variable:}}\\[-.3in]
\begin{eqnarray*}
    E[X] &=& \sum_{i=1}^{\infty} i (1-p)^{i}p = \ldots = \frac{p}{(1-q)^2}=\frac{1}{p},
    \\
    Var[X] &=& \sum_{i=1}^{\infty} (i-\frac{1}{p})^{2} (1-p)^{i}p =
    \ldots = \frac{1-p}{p^{2}}.
\end{eqnarray*}
\no The {\textcolor{magenta} {c.d.f.} is given by :
\[
F_{X}(t) = 1 - (1-p)^{\lfloor {t}\rfloor} =: Geo_{p}(t)
\]
\no  {\textcolor{magenta}{Proof:}} If $X$ is greater than $t$, this means
that the first $\lfloor{t}\rfloor$ trials yields failures. This probability is easy to
compute: just $(1-p)^{\lfloor{t}\rfloor}$. 
 
 
\end{document}





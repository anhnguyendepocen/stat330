\documentclass[20pt,landscape]{foils}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{pause}
\usepackage{graphicx}
\usepackage{epsfig}
%\usepackage{geometry}
%\geometry{headsep=3ex,hscale=0.9}
\newcommand{\bd}{\textbf}
\newcommand{\no}{\noindent}
\newcommand{\un}{\underline}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand \h {\hspace*{.3in}}
\newcommand{\bul}{\hspace*{.1in}{\textcolor{red}{$\bullet$ \ }}}
\newcommand{\xbar}{\bar{x}}
\rightheader{Stat 330 (Fall 2015): slide set 25}

\begin{document}
\LogoOff

\foilhead[1.3in]{}
\centerline{\LARGE \textcolor{blue}{Slide set 25}}
\vspace{0.3in}
\centerline{\large Stat 330 (Fall 2015)}
\vspace{0.2in}
\centerline{\tiny Last update: \today}
\setcounter{page}{0}

\foilhead[-.8in]{\textcolor{blue}{Estimators (Cont'd)}}
\no {\textcolor{blue}{Review:} What is an estimator? What are estimates? What are the properties we use to compare estimators?\\[.1in]
\no  {\textcolor{magenta}{Example:}  The sample mean $\bar{x}$ is consistent for $\mu$. That means that, as the sample size gets large, then $\bar{X}$ gets very close to $\mu$ in the sense of probability. \\[.1in]
\no  {\textcolor{magenta}{Derivation}: Using Chebyshev's inequality,
$$P(|\bar{X}-\mu|>\epsilon)\leq \frac{\text{Var}(\bar{X})}{\epsilon^2}=\frac{\sigma^2}{n\epsilon^2}$$
so that if $n\to \infty$
$$P(|\bar{X}-\mu|>\epsilon)\to 0$$ which means $\bar{X}$ is consistent for $\mu$.

\foilhead[-.8in]{\textcolor{blue}{Estimating parameters}}
\no  {\textcolor{magenta}{Method of moments:} is one of the methods available for estimate parameters. The basic idea is to {\textcolor{red}{equate} the sample moments with the population moments based on the sample $(x_1,\cdots, x_n)$, where $x_i$ are the observed values of $X_i$ for $i=1,\ldots,n$. \\[.1in]
\no  $\clubsuit$  The {\textcolor{magenta}{k-th population moment} is defined as }\\[.2in]
\hspace*{2in} $\mu_k=E(X^k)$\\[.2in]
\no $\diamondsuit$ {\textcolor{blue}{For example:} $ E(X)=\mu$, $\text{Var}(X)=\mu_2-\mu_1^2$\\[.1in]
\no $\clubsuit$ The {\textcolor{magenta}{k-th sample moments} is defined as\\[.1in]
\hspace*{2in} $m_k=n^{-1}\sum\limits_{i=1}^n x_i^k, $\\[.2in]
\no $\diamondsuit$ {\textcolor{blue}{For example:} $m_1= \sum\limits_{i=1}^n x_i/n =\bar{x}$
\foilhead[-.8in]{\textcolor{blue}{Estimating parameters (Cont'd)}}
\no  $\clubsuit$  The {\textcolor{magenta}{k-th population central moments} is defined as }$$\mu_k'=E((X-\mu)^k)$$
\no $\clubsuit$ The {\textcolor{magenta}{k-th sample central moments} is defined as
$$m_k'=n^{-1}\sum\limits_{i=1}^n (x_i-\bar{x})^k,\ x_i\ \text{is the realization/sample value of}\ X_i,$$ where $\bar{x}$ is the sample mean.

\no $\spadesuit$ To estimate $k$ parameters, equate the first $k$ population and sample moments
$$\mu_i=m_i,\ i=1,2,\cdots,k$$
So we have $k$ equations to solve.

\foilhead[-.8in]{\textcolor{blue}{Estimating parameters: Examples}}
\no {\textcolor{magenta}{Example 1:} To estimate the parameter $\lambda$ of Poisson distribution, we need to set
$$\mu_1=m_1\iff \lambda=m_1=\bar{x} \text{ since } \mu_1=E(X)=\lambda$$
Only one unknown is there; solving for $\lambda$ we obtain
$$\hat{\lambda}=\bar{x}$$
which is the method of moment estimator (short as {\textcolor{magenta}{MoM}) of $\lambda$. \\[.1in]
\no {\textcolor{magenta}{Example 2:} (Roll a die) A die is rolled until a face shows a $6$. Repeating this experiment 20 times, and the number of trials needed to show a 6 are
$$3,9,1,6,11,10,4,1,1,4,4,10,17,1,23,3,2,7,2,3$$ Estimate the probability of getting $6$, say $\theta$ when rolling a die using method of moments. \\[.1in]
\no {\textcolor{magenta}{Solution}: $X$ is the number of trials needed for getting a 6 and $x$ is the value of $X$ for each realization. This is a geometric random variable with parameter $\theta$, i.e. $P(X=x)=\theta(1-\theta)^{x-1}$. The method of moment gives equation $$\mu_1=m_1\iff 1/\theta=m_1=\frac{1}{20}\sum\limits_{i=1}^{20} x_i \text{ since } \mu_1=E(X)=1/\theta$$
so that for given realization $x_i$, $\hat{\theta}=20/(\sum\limits_{i=1}^{20}x_i)=0.16393$\\[.1in]
\no {\textcolor{magenta}{Example 3:} Assume that IQ score follows a normal distribution. Suppose we already know that the mean IQ score of human population is $\mu=100$ and would like to estimate the variance $\sigma^2$ based on the IQ score of randomly selected 20 people:
$92,115,103,81,107,95,92,118,99,124,90,87,108,103,91,74,84,124,81,100$\\[.1in]
\no {\textcolor{magenta}{Solution}: Only one parameter is unknown, $\sigma^2$. We know that
$\mu_2=E(X^2)=\sigma^2+E(X)^2=\sigma^2+\mu_1^2$
and $$m_2=\frac{1}{20}\sum\limits_{i=1}^{20} x_i^2, \ \text{for given realization}\ m_2=9879.5$$
so that
$$\mu_2=m_2\Leftrightarrow \hat{\sigma^2}=m_2-\mu_1^2.$$ If using $\mu_1=100$ then
$\hat{\sigma^2}=9879.5-100^2<0!!$\ \ \ How to resolve this problem?\\
{\textcolor{magenta}{Using central moment},
$$\mu_2'=m_2'\Leftrightarrow \sigma^2=E((X-\mu)^2)=\frac1{20}\sum(x_i-\bar{x})^2$$
so that $\hat{\sigma^2}=196.94$.
\no That is an estimate of $\sigma^2$ is obtained directly.


\foilhead[-.8in]{\textcolor{blue}{Estimating parameters: Maximum likelihood estimation (MLE)}}
\no {\textcolor{magenta}{Situation and motivation:}   We have $n$ data values (some numbers, e.g.), and assume that these data values are realizations of $n$ i.i.d.
random variables $X_{1}, \ldots, X_{n}$ with distribution
$F_{\theta}$. Unfortunately the value for $\theta$ is unknown. \\[.1in]
\no {\textcolor{magenta}{Motivation:} By changing the value for $\theta$ we can "move the density function $f_{\theta}$ around"  (below), so that the density function fits the data best.
\begin{figure}[h]
  \centering
  \epsfig{file=f6.pdf, height=7cm}
\end{figure}
\no {\textcolor{magenta}{Principle:} since we do not know the true value $\theta$ of the
distribution, we take that value $\hat{\theta}$ that most likely
produced the observed values $x_1,\cdots, x_n$\\[.1in]
\no {\textcolor{blue}{Sketch of idea:} "Most likely" is equivalent to maximize
\begin{eqnarray*}
&&P_\theta( X_{1} = x_{1} \cap X_{2} = x_{2} \cap \ldots \cap X_{n} = x_{n})\\
&\stackrel{X_{i} \text{ are independent!}}{=}&
 P_\theta( X_{1} = x_{1}) \cdot P_\theta( X_{2} = x_{2}) \cdot \ldots \cdot P_\theta(
X_{n} = x_{n}) \\
&=& \prod_{i=1}^{n} P_\theta( X_{i} = x_{i})\\
\text{or} &=& \prod_{i=1}^n f_\theta(x_i)\vspace{-.2cm}
\end{eqnarray*} i.e.
$\hat{\theta}$ is the maximizer of $\prod_{i=1}^n f_\theta(x_i) $. \\
\no $\spadesuit$ {\textcolor{magenta}{Summary: $\hat{\theta}=\text{argmax}_{\theta}\prod_{i=1}^n f_\theta(x_i) $}


\foilhead[-.8in]{\textcolor{blue}{Estimating parameters: Maximum likelihood estimation}}
\no {\textcolor{magenta}{Maximum likelihood:}  \\[.1in]
\no {\textcolor{red}{(a)} The joint probability density function (for continuous random variables) or joint probability mass function (for discrete random variables) for a randomm sample $X_1,\cdots, X_n$ is defined as\\[.2in]
\hspace*{1in}$L(\theta)=L(\theta;X_1=x_1,\cdots, X_n=x_n)=\prod_{i=1}^{n} P_\theta( x_{i}) \ \text{or}\ \prod_{i=1}^n f_\theta(x_i) $\\[.2in]
\no The above function is a function of $\theta$\\[.1in]
\no $\spadesuit$ $L(\theta)$ is called the {\textcolor{magenta}{Likelihood} function of $X_1,\cdots, X_n$.\\[.1in]
\no {\textcolor{red}{(b)} The maximum likelihood estimator of $\theta$ is the maximizer of likelihood $L(\theta)$ on its parameter space $\Theta$\\[.1in]
\no {\textcolor{red}{(c)} How do we get a maximum of $L(\theta)$? Differentiate it and set it to zero, or try to find the $\theta$ to maximize the likelihood directly. 
\newpage
\no {\textcolor{red}{(d)} Very often, it is difficult to find a derivative of
$L(\theta)$ - instead we use another trick, and find a maximum for
$l(\theta)=\log L(\theta)$, the {\it Log-Likelihood function}.\\[.1in]
\no $\clubsuit$ {\textcolor{magenta}{Five steps for finding MLE:}
\begin{enumerate}
\item Find Likelihood function $L(\theta)$.
\item Get natural log of Likelihood function $l(\theta)=\log L(\theta)$.
\item Differentiate log-Likelihood function with respect to $\theta$.
\item Set derivative to zero.
\item Solve for $\theta$.
\end{enumerate}

\foilhead[-.8in]{\textcolor{blue}{Examples: MLE}}
\no {\textcolor{magenta}{Example (Rolling dies):} Find the MLE for the probability of getting a face 6, $\theta$, in the "Roll a Die" example: repeating this experiment 100 times gave the following results,\\[.1in] {\tiny\begin{tabular}{c||rrrrrrrrrrrrrrrrrr}
$x$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 11 & 14 & 15 & 16 & 17 & 20 & 21 & 27 & 29 \\ \hline
\# & 18 & 20 & 8 & 9 & 9 & 5 & 8 & 3 & 5 & 3 & 3 & 3 & 1 & 1 & 1 & 1 & 1 & 1
\end{tabular} }\\[.1in]
\no $\clubsuit$ Recall: We know, that $X$, the number of rolls until a 6 shows up has a geometric distribution $Geo_p$ and $x$ in the table is its realization. \\[.1in]
\no $\clubsuit$  For a fair die, $p$ is 1/6, and the Geometric distribution has probability mass function $p(x) = (1-p)^{x-1} \cdot p$.\\[.1in]
\no {\textcolor{magenta}{Step 1: Find the likelihood function $L(\theta)$:}
Since we have observed 100 outcomes $x_1, ..., x_{100}$, the likelihood function  $L(p) = \prod_{i=1}^{100} p(x_i)$,
$$L(p) = \prod_{i=1}^{100}f_p(x_i)=\prod_{i=1}^{100} (1-p)^{x_i - 1} p = p^{100} \cdot  \prod_{i=1}^{100} (1-p)^{x_i - 1}$$
$$ =p^{100} \cdot  (1-p)^{\sum_{i=1}^{100} (x_i - 1)} = p^{100} \cdot  (1-p)^{\sum_{i=1}^{100} x_i - 100}.
$$
\no {\textcolor{magenta}{Step 2: log of Likelihood function $\log L(p)$: }
\begin{eqnarray*}
l(p)=\log L(p) &=& \log \left( p^{100} \cdot  (1-p)^{\sum_{i=1}^{100} x_i - 100} \right )  \\
&=& \log \left ( p^{100} \right ) + \log \left ( (1-p)^{\sum_{i=1}^{100} x_i - 100} \right ) \\
&=& 100 \log p + \left ( \sum_{i=1}^{100} x_i - 100 \right ) \log (1 - p).
\end{eqnarray*}
\no {\textcolor{magenta}{Step 3: Differentiate log-Likelihood with respect to $p$:}
$$
\frac{d}{dp} l(p)=\frac{d}{dp} \log L(p)= \frac {100}{p} -\left ( \sum_{i=1}^{100} x_i - 100 \right ) \frac{1}{1-p} $$\begin{eqnarray*}
&=& \frac{1}{p(1-p)} \left ( 100 (1-p) - \left ( \sum_{i=1}^{100} x_i - 100 \right )p \right )  \\
&=& \frac{1}{p(1-p)} \left ( 100 - p \sum_{i=1}^{100} x_i \right ).
\end{eqnarray*}
\no {\textcolor{magenta}{Step 4: Set derivative to zero:}
For the estimate $\hat{p}$ the derivative must be zero:
$$ \frac{d}{dp} \log L(\hat{p})=0
\iff \frac{1}{\hat{p}(1-\hat{p})} \left ( 100 - \hat{p} \sum_{i=1}^{100} x_i \right ) = 0
$$
\no {\textcolor{magenta}{Step 5: Solve for $\hat{p}$:}
$$
\frac{1}{\hat{p}(1-\hat{p})} \left ( 100 - \hat{p} \sum_{i=1}^{100} x_i \right ) =0 \iff
100 - \hat{p} \sum_{i=1}^{100} x_i   = 0 $$
so that $$\hat{p} = \frac{100}{\sum_{i=1}^{100} x_i } = \frac{1}{\frac{1}{100} \sum_{i=1}^{100} x_i }.$$\\
\no $\spadesuit$ In total, we have an estimate $$\hat{p} = \frac{100}{568} = 0.1710$$ for the given $x_i$ from the above table.


\end{document}





\documentclass[20pt,landscape]{foils}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{amstext}
\usepackage{amsgen}
\usepackage{amsxtra}
\usepackage{amsgen}
\usepackage{amsthm}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{pause}
\usepackage{graphicx}
\usepackage{epsfig}
%\usepackage{geometry}
%\geometry{headsep=3ex,hscale=0.9}
\newcommand{\bd}{\textbf}
\newcommand{\no}{\noindent}
\newcommand{\un}{\underline}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand \h {\hspace*{.3in}}
\newcommand{\bul}{\hspace*{.3in}{\textcolor{red}{$\bullet$ \ }}}
\newcommand{\xbar}{\bar{x}}
\rightheader{Stat 330 (Fall 2015): slide set 12}

\begin{document}
\LogoOff

\foilhead[1.3in]{}
\centerline{\LARGE \textcolor{blue}{Slide set 12}}
\vspace{0.3in}
\centerline{\large Stat 330 (Fall 2015)}
\vspace{0.2in}
\centerline{\tiny Last update: \today}
\setcounter{page}{0}

\foilhead[-.7in]{\textcolor{blue}{Continuous Random Variables}}
\no All properties of discrete RVs  have direct counterparts for coninuous RVs.\\[.1in]
\no  {\textcolor{magenta}{One basic difference:}}  {\textcolor{cyan}{summations used in the case of discrete RVs are replaced by integrals.}}\\[.1in]
\no Summing over (uncountable) infinite many values corresponds to an integral.\\[.2in]
\no For e.g.,we define a cumulative distribution 
function (cdf) as follows:\\[.1in]
\no {\textcolor{magenta}{Definition: }}{\textcolor{cyan}{ CDF of a $X$ is a continuous random variable:}}\\[.1in]
    The function $F_{X}(t) := P(X \le t)$ is called the {\it 
    cumulative distribution function of} $X$.\\[.15in]
\no The only difference to the discrete case is that the cdf of a continuous variable is not a stairstep function.
\foilhead[-.7in]{\textcolor{blue}{Properties of $F_{X}$}}
\no The following properties hold for the cumulative distribution function 
$F_{X}$ for random variable $X$.
\begin{itemize}
    \item[\bul] $ 0 \le F_{X}(t) \le 1$ for all $t \in \mathbb{R}$
    \item[\bul] $F_{X}$ is monotone increasing, (i.e. if $x_{1} \le x_{2}$ 
    then $F_{X}(x_{1}) \le F_{X}(x_{2})$.)
    \item[\bul] $\lim_{t \rightarrow -\infty}F_{X}(t) = 0$ and $\lim_{t \rightarrow \infty}F_{X}(t) = 1$.
\end{itemize}
\no However, there is slight difference from the discrete 
case:\\[.1in]
\no {\textcolor{magenta}{Definition:} {\textcolor{cyan}{Probability Density Function}}
For a continuous variable $X$ with cumulative distribution function $F_{X}$ the 
{\it density function of $X$} is defined as:
\[
f_{X}(x) := F_{X}'(x).
\]
\foilhead[-.75in]{\textcolor{blue}{Properties of density function $f(x)$}}\vspace{1mm}
\no A function $f_{X}$ is a density function of a random variable $X$, if
\vspace*{-.2in}

\begin{itemize}
\addtolength{\itemsep}{-0.6\baselineskip}
    \item[(i)] $f_{X}(x) \ge 0$ for all $x$,
    \item[(ii)] $\int_{-\infty}^{\infty}f(x)dx =1$.
\end{itemize}
\no {\textcolor{magenta}{Relationship between $f_{X}$ and $F_{X}$}}\\[.1in]
Since the density function $f_{X}$ is defined as the derivative of 
the cumulative distribution function, we can obtain the cumulative distribution function 
from the density by integrating:\\[.1in]
\no \bul $F_{X}(t) = P(X \le t) = \int_{-\infty}^{t}f(x) dx$\\[.1in]
\no \bul $P(a \le X \le b) = \int_{a}^{b} f(x) dx$\\[.2in]
It follows that \\[.1in]
\h \h \h $P(X=a) = P( a \le X \le a) = \int_{a}^{a}f(x) dx = 0.$





\foilhead[-.75in]{\textcolor{blue}{Example: pdf}}}\vspace{1mm}
\no {\textcolor{magenta}{Let $Y$ be the time until the first major failure of a new disk 
    drive.}}\\[.1in]
\no A possible density function for $Y$ is
    \[
    f(y) = \left \{ 
    \begin{array}{ll}
	e^{-y} & y > 0\\
	0 & \text{ otherwise}
    \end{array} \right .
    \]
\no {\textcolor{cyan}{
    First, we need to check, that $f(y)$ is actually a density 
    function. Obviously, $f(y)$ is a non-negative function on whole 
    of $\Re$.}}
    
\no {\textcolor{cyan}{ The second condition, $f$ must fulfill to be a density of $Y$ 
    is }}
    \[
    \int_{-\infty}^{\infty} f(y)dy = \int_{0}^{\infty} e^{-y}dy = - 
    e^{-y} |_{0}^{\infty} = 0 - (-1) = 1
    \]
  
  \foilhead[-.75in]{\textcolor{blue}{Continuing the disk drive example...}}\vspace{1mm}  
\no    {\textcolor{magenta}{What is the probability that the first major disk drive failure 
    occurs within the first year?}}\\[.1in]  
    \[
    P(Y \le 1) = \int_{0}^{1} e^{-y}dy = -e^{-y} |_{0}^{1} = 1 - 
    e^{-1} \approx 0.63.
    \]
    
\no {\textcolor{magenta} {What is the cumulative distribution function of $Y$?}}
   \[
    F_{Y}(t) = \int_{\infty}^{t} f(y) dy = \int_{0}^{t}e^{-y}dy = 1 - 
    e^{-t} \text{ for all } t \ge 0.
    \]
\foilhead[-.75in]{\textcolor{blue}{Continuing the disk drive example...}}\vspace{1mm}  
\no    {\textcolor{magenta}{Density and Distribution functions of the random variable $Y$.}\\[.1in]
  \centerline {\includegraphics[scale=.6]{density.pdf}}


 \foilhead[-.8in]{\textcolor{blue}{Compare discrete and continuous RVs}}
%\begin{center}
    \begin{tabular}{p{4.5in}|p{4in}}
  {\textcolor{magenta}{discrete random variable}} &  {\textcolor{magenta}{continuous random variable}} \\ \hline
	image $Im(X)$ finite or countable infinite &  image $Im(X)$ uncountable 
	\\[5pt]
	{\textcolor{magenta}{cumulative distribution function:}} \\
	$F_{X}(t) = P(X \le t) = \sum_{k \le {t}}p_{X}(k)$ & 
	$F_{X}(t) = P(X \le t) = \int_{\infty}^{t} f(x) dx$ \\ [5pt]
	 {\textcolor{magenta}{probability mass function:}} &  {\textcolor{magenta}{probability 
	density function:}}\\
	\centerline{$p_{X}(x) = P(X = x)$} &
	\centerline{$f_{X}(x) = F_{X}^{'}(x)$} \\ [5pt]
	 {\textcolor{magenta}{expected value:}}\\
	$E[h(X)] = \sum_{x} h(x) \cdot p_{X}(x)$ & 	$E[h(X)] = \int_{x} h(x) \cdot 
	f_{X}(x)$ \\[5pt]
	$E[X] = \sum_{x} x \cdot p_{X}(x)$ & 	$E[X] = \int_{-\infty}^{\infty} x \cdot 
	f_{X}(x) dx$ \\[5pt]
  {\textcolor{magenta}{variance:}} \\
	$Var[X] = E[(X-E[X])^{2}] = \sum_{x}(x-E[X])^{2}p_{X}(x)$ & $Var[X] = 
	E[(X-E[X])^{2}] = \int_{-\infty}^{\infty}(x-E[X])^{2}f_{X}(x)dx$ \\
    \end{tabular}
%\end{center}


  
 \foilhead[-.75in]{\textcolor{blue}{Some special continuous density functions}}
\no 	 {\textcolor{magenta}{Uniform Density}}\\[.1in]
\no  One of the most basic continuous density is the {\it 
uniform density}.\\[.1in] 
\no 	 {\textcolor{magenta}{The pdf is:}\\[-.2in]
\[
f(x) = \left \{ 
\begin{array}{cl}
    \frac{1}{b-a} & \text{ if } a < x < b \\
    0 & \text{ otherwise}
\end{array} \right .
\]
\no We say that $X \sim U(a,b)$ i.e., the random variable $X$ is distributed as the \emph{Uniform distribution with parameters $a$ and $b$}\\[.15in]
%
\centerline{\includegraphics[scale=0.6]{uniform.pdf}}
%
\foilhead[-.75in]{\textcolor{blue}{Properties of the Uniform distribution}}
\no The cumulative distribution function $F_{X}$ is
\vspace*{-.3in}

\[
U_{a,b}(x) := F_{X}(x) = \left \{ 
\begin{array}{cl}
    0 & \text{ if } x \le a \\
    \frac{x-a}{b-a} & \text{ if } a < x < b \\
    1 & \text{ if } x \ge b.
\end{array} \right .
\]
\vspace*{-.25in}

\no We now compute the expected value and variance of a 
 a uniform distribution on $(a,b)$.
\begin{eqnarray*}
    E[X] &=& \int_{a}^{b} x \frac{1}{b-a} dx = \frac{1}{b-a} 
    \frac{1}{2} x^{2}|_{a}^{b} = \\
    &=& \frac{b^{2}-a^{2}}{2(b-a)} = 
    \underline{\frac{1}{2}(a+b)}.  \\
    Var[X] &=& \int_{a}^{b}(x - \frac{a+b}{2})^{2} \frac{1}{b-a} dx = 
    \ldots = \underline{\frac{(b-a)^{2}}{12}}.
\end{eqnarray*}
\foilhead[-.75in]{\textcolor{blue}{Uniform distribution: Example}}
\no    {\textcolor{cyan}{The(pseudo) random number generator on my calculator is supposed to create 
realizations of $U(0,1)$ random variables.}\\[.15in]
\no Define $U$ as the next random number the calculator produces.\\[.15in]
\no What is the probability, that the next number is larger than 0.85?\\[.15in]
\no To answer that, we will compute $P( U \ge 0.85)$.\\[.15in]
\no We know the density function of $U$: $f_{U}(u) = \frac{1}{1-0} = 1$.\\[.1in]
\no Therefore
\[
P(U \ge 0.85) = \int_{0.85}^{1}1 du = 1 - 0.85 = 0.15.
\]

\end{document}








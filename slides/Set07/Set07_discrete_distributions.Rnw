\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set07 - Discrete distributions}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Discrete distributions}
\begin{frame}
\frametitle{Special discrete distributions}

\begin{itemize}
\item Bernoulli distribution
\item Binomial distribution
\item Geometric distribution
\item Poisson distribution
\item Compounded distribution
\end{itemize}

\vspace{0.1in} \pause

Note: The sample space is always finite or countable.

\end{frame}


\subsection{Bernoulli}
\begin{frame}
\frametitle{Bernoulli distribution}

A Bernoulli experiment has only two outcomes: success/failure. 

\vspace{0.1in} \pause

Let 
\begin{itemize}
\item $X=1$ represent success and
\item $X=0$ represent failure. 
\end{itemize}

\vspace{0.1in} \pause

The probability mass function $p_X(x)$ is 
\[ 
p_X(0) = 1-p \quad p_X(1) = p.
\]

\vspace{0.1in} \pause

We use the notation $X\sim Ber(p)$ to denote a random variable $X$ that follows a Bernoulli distribution with success probability $p$, i.e. $P(X=1)=p$. 
\end{frame}


\begin{frame}
\frametitle{Bernoulli experiment examples}

\begin{example}
\begin{itemize}
\item Toss a coin: $\mOmega = \{H,T\}$
\item Throw a fair die and ask if the face value is a six: $\mOmega = \{{\text{face value is a six},\text{face value is not a six}}\}$
\item Sent a message through a network and record whether or not it is received: $\mOmega = \{ \text{successful transmission}, \text{ unsuccessful transmission}\}$
\item Draw a part from an assembly line and record whether or not it is defective: $\mOmega = \{\text{defective}, \text{ good}\}$
\item Response to the question ``Are you in favor of the above measure''? (in reference to a new tax levy to  be imposed on city residents): $\mOmega = \{\text{yes}, \text{no}\}$
\end{itemize}
\end{example}
\end{frame}



\begin{frame}
\frametitle{Bernoulli distribution (cont.)}

The cdf of the Bernoulli distribution is
\[
F_X(t) =P(X\leq t)= \left \{
\begin{array}{cl}
    0 & t < 0 \\
    1-p & 0 \le t < 1 \\
    1 & 1 \le t
\end{array}
\right .
\]

\vspace{0.1in} \pause

The expected value is 
\[ 
E[X]=\sum\limits_{x} p_X(x)=0(1-p)+1\cdot p=p.
\]

\vspace{0.1in} \pause

The variance is 
\[ Var[X]=\sum\limits_{x}(x-E[X])^2p_X(x)= (0-p)^2\cdot (1-p)+(1-p)^2 \cdot p = p (1-p). \]

\end{frame}




\begin{frame}
\frametitle{Sequence of Bernoulli experiments}

A compound experiment consisting of $n$ \alert{independent and identically distributed} Bernoulli experiments. \pause
E.g.
\begin{itemize}
\item Toss a coin $n$ times.
\item Send 23 identical messages through the network independently.
\item Draw 5 cards from a standard deck with replacement (and reshuffling) and record whether or not the card is a king.
\end{itemize}

\vspace{0.2in} \pause

What does \alert{independent and identically distributed} mean? 

\end{frame}

\begin{frame}
\frametitle{Independent and identically distributed}

Let $X_i$ represent the $i^{th}$ Bernoulli experiment. 

\vspace{0.1in} \pause 

\alert{Independence} means 

\[ 
P(X_1=x_1,\ldots,X_n=x_n) = \prod_{i=1}^n P(X_i=x_i),
\]
i.e. the joint probability is the product of the individual probabilities. 

\vspace{0.1in} \pause

\alert{Identically distributed} (for Bernoulli random variables) means 
\[ 
P(X_i=1) = p \quad \forall\, i,
\]
and more generally, the distribution is the same for all the random variables.

\vspace{0.1in} \pause

We will use $iid$ as a shorthand for \emph{independent and identically distributed}, although I will often use $ind$ to indicate \emph{independent} and let \emph{identically distributed} be clear from context. 

\end{frame}



\begin{frame}
\frametitle{Sequences of Bernoulli experiments}

Let $X_i$ denote the outcome of the $i^{th}$ Bernoulli experiment. \pause
We use the notation 
\[ 
X_i \iid Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
to indicate a sequence of $n$ independent and identically distributed Bernoulli experiments.

\vspace{0.1in}\pause

We could write this equivalently as 
\[ 
X_i \ind Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
\pause
but this is different than 
\[ 
X_i \ind Ber(p_i), \quad \mbox{for }i=1,\ldots,n
\]
as the latter has a different success probability for each experiment. 

\end{frame}


\subsection{Binomial distribution}
\begin{frame}
\frametitle{Binomial distribution}

Suppose we perform a sequence of $n$ $iid$ Bernoulli experiments and only record the number of successes, i.e. 
\[ 
Y = \sum_{i=1}^n X_i.
\]

\vspace{0.1in} \pause

Then we use the notation $Y\sim Bin(n,p)$ to indicate a binomial distribution with 
\begin{itemize}
\item $n$ attempts and 
\item probability of success $p$. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Binomial probability mass function}

We need to obtain 
\[ 
p_Y(y) = P(Y=y) \quad \forall y\in \mOmega \pause = \{0,1,2,\ldots,n\}.
\]

\pause
The probability of obtaining a particular sequence of $y$ success and $n-y$ failures is 
\[ 
p^y(1-p)^{n-y}
\]
since the experiments are $iid$ with success probability $p$. \pause But there are 
\[
{n\choose y} 
\]
ways of obtaining a sequence of $y$ success and $n-y$ failures. \pause Thus, the binomial probability mass function is 
\[ 
P_Y(y) = P(Y=y) = {n\choose y} p^y(1-p)^{n-y}.
\]

\end{frame}


\begin{frame}
\frametitle{Properties of the binomial distribution}

The expected value is 
\[ 
E[Y] = E\left[ \sum_{i=1}^n X_i \right] \pause = \sum_{i=1}^n E[X_i] \pause= \sum_{i=1}^n p = np.
\]

\vspace{0.1in} \pause

The variance is 
\[ 
Var[Y] = \sum_{i=1}^n Var[X_i] = np(1-p)
\]
since the $X_i$ are independent. 

\vspace{0.1in} \pause

The cumulative distribution function is 
\[ 
F_Y(y) = P(Y\le y) = \sum_{x=0}^{\lfloor y\rfloor} {n\choose x} p^x(1-p)^{n-x}.
\]

\end{frame}



\begin{frame}
\frametitle{Component failure rate}

Suppose a box contains 15 components that each have a failure rate of 5\%. 

\vspace{0.1in} \pause

What is the probability that 
\begin{enumerate}
	\item exactly two out of the fifteen components are defective?
	\item at most two components are defective?
	\item more than three components are defective?
	\item more than 1 but less than 4 are defective?
\end{enumerate}

\vspace{0.1in} \pause

Let $Y$ be the number of defective components. \pause Then $Y\sim Bin(15,0.05)$. \pause

{\small
\[ \begin{array}{lll}
1. & P(Y=2)&= P(Y=2) = {15\choose 2} (0.05)^2(1-0.05)^{15-2} \pause \\
2. & P(Y\le 2) &= \sum_{x=0}^2 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
3. & P(Y>3) &= 1-P(Y\le 3) = 1-\sum_{x=0}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
4. & P(1<Y<4) &= \sum_{x=2}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \\
\end{array} \]
}
\end{frame}



\begin{frame}[fragile]
\frametitle{Component failure rate (solutions in R)}

<<component_failure_rate,echo=TRUE>>=
n = 15
p = 0.05
choose(15,2)
dbinom(2,n,p)           # P(Y=2)
pbinom(2,n,p)           # P(Y<=2)
1-pbinom(3,n,p)         # P(Y>3) 
sum(dbinom(c(2,3),n,p)) # P(1<Y<4) = P(Y=2)+P(Y=3)
@

\end{frame}



% \foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example (Continued...)}}
%  \begin{enumerate}
% 	\item
% 	$\displaystyle P( \text{exactly two out of the fifteen components are
% 	defective} ) = p_X(2) = {15 \choose 2} 0.05^{2} 0.95^{13} = 0.1348.$
% 	\item
% 	$\displaystyle P( \text{at most two components are defective} ) = P(X \le 2) =
% 	B_{15,0.05}(2) = 0.9638.$
% 	\item $\displaystyle P(\text{ more than three components are defective }) = P(X > 3) =
% 	1 - P( X \le 3) = 1 - 0.9945 = 0.0055.$
% 	\item $\displaystyle P(\text{ more than 1 but less than 4 are defective }) = P( 1 <
% 	X < 4) = P(X \le 3) - P(X \le 1) = 0.9945 - 0.8290 = 0.1655.$
%     \end{enumerate}


\begin{frame}
\frametitle{Geometric distribution}

Rather than attempting $n$ times, 
we continue attempting until we get a success (where each trial has success probability $p$) and we record the number of the successful attempt\pause, i.e. 
\[ 
Z  = \min \{i|X_i=1 \}.
\]

\vspace{0.1in} \pause

Then $Z$ is said to have a geometric distribution and we write $Z\sim Geo(p)$ \pause with 
\begin{itemize}
\item sample space $\mOmega = \pause \{ 1, 2, 3, 4, \ldots \}$ \pause and
\item probability mass function 
\[ 
P_Z(z) = P(Z=z) = p(1-p)^{z-1}.
\]
\end{itemize}

\pause

{\bf Note:} An alternative version of the Geometric distribution does not record the successful attempt, but rather the number of failures.

\end{frame}



\begin{frame}
\frametitle{Properties of the geometric distribution}

The expectation of a geometric random variable is 
\[ 
E[Z] = \sum_{z=1}^{\infty} z p(1-p)^{z-1} = \ldots %= \frac{p}{(1-q)^2}
= \frac{1}{p}.
\]
\pause
The variance of a geometric random variable is 
\[ 
Var[Z] = \sum_{z=1}^{\infty} \left[z-\frac{1}{p}\right]^{2} (1-p)^{z-1}p = \ldots = \frac{1-p}{p^2}.
\]
\pause
The cumulative distribution function is 
\[ 
F_Z(z) = P(Z\le z) = 1 - [1-p]^{\lfloor z \rfloor}
\]
but is actually easier to calculate the complement, i.e. 
\[ 
P(Z>z) = [1-p]^{\lfloor z \rfloor}.
\]

\end{frame}


\end{document}





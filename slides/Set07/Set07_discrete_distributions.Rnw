\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set07 - Discrete distributions}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Discrete distributions}
\begin{frame}
\frametitle{Special discrete distributions}

\begin{itemize}
\item Bernoulli distribution
\item Binomial distribution
\item Geometric distribution
\item Poisson distribution
%\item Compounded distribution
\end{itemize}

\vspace{0.1in} \pause

Note: The sample space is always finite or countable.

\end{frame}


\subsection{Bernoulli}
\begin{frame}
\frametitle{Bernoulli distribution}

A Bernoulli experiment has only two outcomes: success/failure. 

\vspace{0.1in} \pause

Let 
\begin{itemize}
\item $X=1$ represent success and
\item $X=0$ represent failure. 
\end{itemize}

\vspace{0.1in} \pause

The probability mass function $p_X(x)$ is 
\[ 
p_X(0) = 1-p \quad p_X(1) = p.
\]

\vspace{0.1in} \pause

We use the notation $X\sim Ber(p)$ to denote a random variable $X$ that follows a Bernoulli distribution with success probability $p$, i.e. $P(X=1)=p$. 
\end{frame}


\begin{frame}
\frametitle{Bernoulli experiment examples}

\begin{example}
\begin{itemize}
\item Toss a coin: $\mOmega = \{H,T\}$
\item Throw a fair die and ask if the face value is a six: $\mOmega = \{{\text{face value is a six},\text{face value is not a six}}\}$
\item Sent a message through a network and record whether or not it is received: $\mOmega = \{ \text{successful transmission}, \text{ unsuccessful transmission}\}$
\item Draw a part from an assembly line and record whether or not it is defective: $\mOmega = \{\text{defective}, \text{ good}\}$
\item Response to the question ``Are you in favor of the above measure''? (in reference to a new tax levy to  be imposed on city residents): $\mOmega = \{\text{yes}, \text{no}\}$
\end{itemize}
\end{example}
\end{frame}



\begin{frame}
\frametitle{Bernoulli distribution (cont.)}

The cdf of the Bernoulli distribution is
\[
F_X(t) =P(X\leq t)= \left \{
\begin{array}{cl}
    0 & t < 0 \\
    1-p & 0 \le t < 1 \\
    1 & 1 \le t
\end{array}
\right .
\]

\vspace{0.1in} \pause

The expected value is 
\[ 
E[X]=\sum\limits_{x} p_X(x)=0(1-p)+1\cdot p=p.
\]

\vspace{0.1in} \pause

The variance is 
\[ Var[X]=\sum\limits_{x}(x-E[X])^2p_X(x)= (0-p)^2\cdot (1-p)+(1-p)^2 \cdot p = p (1-p). \]

\end{frame}




\begin{frame}
\frametitle{Sequence of Bernoulli experiments}

A compound experiment consisting of $n$ \alert{independent and identically distributed} Bernoulli experiments. \pause
E.g.
\begin{itemize}
\item Toss a coin $n$ times.
\item Send 23 identical messages through the network independently.
\item Draw 5 cards from a standard deck with replacement (and reshuffling) and record whether or not the card is a king.
\end{itemize}

\vspace{0.2in} \pause

What does \alert{independent and identically distributed} mean? 

\end{frame}

\begin{frame}
\frametitle{Independent and identically distributed}

Let $X_i$ represent the $i^{th}$ Bernoulli experiment. 

\vspace{0.1in} \pause 

\alert{Independence} means 

\[ 
P(X_1=x_1,\ldots,X_n=x_n) = \prod_{i=1}^n P(X_i=x_i),
\]
i.e. the joint probability is the product of the individual probabilities. 

\vspace{0.1in} \pause

\alert{Identically distributed} (for Bernoulli random variables) means 
\[ 
P(X_i=1) = p \quad \forall\, i,
\]
and more generally, the distribution is the same for all the random variables.

\vspace{0.1in} \pause

We will use $iid$ as a shorthand for \emph{independent and identically distributed}, although I will often use $ind$ to indicate \emph{independent} and let \emph{identically distributed} be clear from context. 

\end{frame}



\begin{frame}
\frametitle{Sequences of Bernoulli experiments}

Let $X_i$ denote the outcome of the $i^{th}$ Bernoulli experiment. \pause
We use the notation 
\[ 
X_i \iid Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
to indicate a sequence of $n$ independent and identically distributed Bernoulli experiments.

\vspace{0.1in}\pause

We could write this equivalently as 
\[ 
X_i \ind Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
\pause
but this is different than 
\[ 
X_i \ind Ber(p_i), \quad \mbox{for }i=1,\ldots,n
\]
as the latter has a different success probability for each experiment. 

\end{frame}


\subsection{Binomial}
\begin{frame}
\frametitle{Binomial distribution}

Suppose we perform a sequence of $n$ $iid$ Bernoulli experiments and only record the number of successes, i.e. 
\[ 
Y = \sum_{i=1}^n X_i.
\]

\vspace{0.1in} \pause

Then we use the notation $Y\sim Bin(n,p)$ to indicate a binomial distribution with 
\begin{itemize}
\item $n$ attempts and 
\item probability of success $p$. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Binomial probability mass function}

We need to obtain 
\[ 
p_Y(y) = P(Y=y) \quad \forall y\in \mOmega \pause = \{0,1,2,\ldots,n\}.
\]

\pause
The probability of obtaining a particular sequence of $y$ success and $n-y$ failures is 
\[ 
p^y(1-p)^{n-y}
\]
since the experiments are $iid$ with success probability $p$. \pause But there are 
\[
{n\choose y} 
\]
ways of obtaining a sequence of $y$ success and $n-y$ failures. \pause Thus, the binomial probability mass function is 
\[ 
P_Y(y) = P(Y=y) = {n\choose y} p^y(1-p)^{n-y}.
\]

\end{frame}


\begin{frame}
\frametitle{Properties of the binomial distribution}

The expected value is 
\[ 
E[Y] = E\left[ \sum_{i=1}^n X_i \right] \pause = \sum_{i=1}^n E[X_i] \pause= \sum_{i=1}^n p = np.
\]

\vspace{0.1in} \pause

The variance is 
\[ 
Var[Y] = \sum_{i=1}^n Var[X_i] = np(1-p)
\]
since the $X_i$ are independent. 

\vspace{0.1in} \pause

The cumulative distribution function is 
\[ 
F_Y(y) = P(Y\le y) = \sum_{x=0}^{\lfloor y\rfloor} {n\choose x} p^x(1-p)^{n-x}.
\]

\end{frame}



\begin{frame}
\frametitle{Component failure rate}

Suppose a box contains 15 components that each have a failure rate of 5\%. 

\vspace{0.1in} \pause

What is the probability that 
\begin{enumerate}
	\item exactly two out of the fifteen components are defective?
	\item at most two components are defective?
	\item more than three components are defective?
	\item more than 1 but less than 4 are defective?
\end{enumerate}

\vspace{0.1in} \pause

Let $Y$ be the number of defective components. \pause Then $Y\sim Bin(15,0.05)$. \pause

{\small
\[ \begin{array}{lll}
1. & P(Y=2)&= P(Y=2) = {15\choose 2} (0.05)^2(1-0.05)^{15-2} \pause \\
2. & P(Y\le 2) &= \sum_{x=0}^2 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
3. & P(Y>3) &= 1-P(Y\le 3) = 1-\sum_{x=0}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
4. & P(1<Y<4) &= \sum_{x=2}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \\
\end{array} \]
}
\end{frame}



\begin{frame}[fragile]
\frametitle{Component failure rate (solutions in R)}

<<component_failure_rate,echo=TRUE>>=
n = 15
p = 0.05
choose(15,2)
dbinom(2,n,p)           # P(Y=2)
pbinom(2,n,p)           # P(Y<=2)
1-pbinom(3,n,p)         # P(Y>3) 
sum(dbinom(c(2,3),n,p)) # P(1<Y<4) = P(Y=2)+P(Y=3)
@

\end{frame}



% \foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example (Continued...)}}
%  \begin{enumerate}
% 	\item
% 	$\displaystyle P( \text{exactly two out of the fifteen components are
% 	defective} ) = p_X(2) = {15 \choose 2} 0.05^{2} 0.95^{13} = 0.1348.$
% 	\item
% 	$\displaystyle P( \text{at most two components are defective} ) = P(X \le 2) =
% 	B_{15,0.05}(2) = 0.9638.$
% 	\item $\displaystyle P(\text{ more than three components are defective }) = P(X > 3) =
% 	1 - P( X \le 3) = 1 - 0.9945 = 0.0055.$
% 	\item $\displaystyle P(\text{ more than 1 but less than 4 are defective }) = P( 1 <
% 	X < 4) = P(X \le 3) - P(X \le 1) = 0.9945 - 0.8290 = 0.1655.$
%     \end{enumerate}


\subsection{Geometric}
\begin{frame}
\frametitle{Geometric distribution}

Rather than attempting $n$ times, 
we continue attempting until we get a success (where each trial has success probability $p$) and we record the number of the successful attempt\pause, i.e. 
\[ 
Z  = \min \{i|X_i=1 \}.
\]

\vspace{0.1in} \pause

Then $Z$ is said to have a geometric distribution and we write $Z\sim Geo(p)$ \pause with 
\begin{itemize}
\item sample space $\mOmega = \pause \{ 1, 2, 3, 4, \ldots \}$ \pause and
\item probability mass function 
\[ 
P_Z(z) = P(Z=z) = p(1-p)^{z-1}.
\]
\end{itemize}

\pause

{\bf Note:} An alternative version of the Geometric distribution does not record the successful attempt, but rather the number of failures.

\end{frame}



\begin{frame}
\frametitle{Properties of the geometric distribution}

The expectation of a geometric random variable is 
\[ 
E[Z] = \sum_{z=1}^{\infty} z p(1-p)^{z-1} = \ldots %= \frac{p}{(1-q)^2}
= \frac{1}{p}.
\]
\pause
The variance of a geometric random variable is 
\[ 
Var[Z] = \sum_{z=1}^{\infty} \left[z-\frac{1}{p}\right]^{2} (1-p)^{z-1}p = \ldots = \frac{1-p}{p^2}.
\]
\pause
The cumulative distribution function is 
\[ 
F_Z(z) = P(Z\le z) = 1 - [1-p]^{\lfloor z \rfloor}
\]
but is actually easier to calculate the complement, i.e. 
\[ 
P(Z>z) = [1-p]^{\lfloor z \rfloor}.
\]

\end{frame}


\begin{frame}[fragile]
\frametitle{Geometric distribution - example}

Suppose a piece of code looks like 
<<echo=TRUE>>=
# Repeat S until B
@

\pause
If $P(B)=0.1$, how often is $S$ executed on average?

\vspace{0.2in} \pause

Let $X$ be the number of times $S$ is executed. \pause Then $X\sim Geo(0.1)$ and $E[X] = 1/0.1 = 10$. 

\end{frame}



\begin{frame}
\frametitle{Geometric distribution - example 2}

Watch the input queue at the alpha farm for a job that times out.
The probability that a job times out is 0.05. 
Let $Y$ be the index of the first job to time out, then $Y \sim Geo(0.05)$. 
\begin{itemize}
\item What is the probability that
  \begin{itemize}
	\item  the third job times out? \pause $P(Y = 3) = 0.95^{2} 0.05 = 0.045$ \pause
	\item  $Y$ is less than 3? \pause$P( Y < 3) = P( Y \le 2) = 1 - 0.95^{2} = 0.0975$ \pause
	\item  the first job to time out is between the third and the seventh? \pause 
	$P( 3 \le Y \le 7) = P(Y \le 7) - P(Y \le 2) = 1-0.95^{7} - (1 - 0.95^{2}) = 0.204$
  \end{itemize}
\item What is 
	\begin{itemize}
	\item expected value of $Y$. \pause $E[Y] = 1/0.05 = 20$ \pause
	\item variance of $Y$. pause $V[y] = \frac{1-0.05}{0.05^2} = 380$ \pause
	\end{itemize}
\end{itemize}
\pause



\end{frame}


\begin{frame}
\frametitle{Geometric distribution - memoryless property}

\begin{definition}[Memoryless property]
A discrete distribution is \alert{memoryless} if 
\[ 
P(X\ge i+j|X\ge j)= P(X\ge i) \mbox{ for } i,j=0,1,2,\ldots
\]
\end{definition}

\pause

\begin{theorem}
The geometric distribution is memoryless. \pause
\end{theorem}
\begin{proof}
\[ \begin{array}{rl}
P(X\ge i+j|X\ge j) &= \frac{P(X\ge i+j,X\ge j)}{P(X\ge j)} = \frac{P(X\ge i+j)}{P(X\ge j)} \\
&= \frac{(1-p)^{i+j}}{(1-p)^{j}} = (1-p)^i \\
&= P(X\ge i)
\end{array} \]
\end{proof}

\end{frame}


\subsection{Poisson}
\begin{frame}
\frametitle{Poisson distribution}

Many experiments can be thought of as ``how many \emph{rare} events will occur in a certain amount of time or space?'' \pause 
For example,
{\small
\begin{itemize}
\item \# of alpha particles emitted from a polonium bar in an 8 minute period
\item \# of flaws on a standard size piece of manufactured product, e.g., 100m coaxial cable, 100 sq.meter plastic sheeting
\item \# of hits on a web page in a 24h period
\end{itemize}
}

\pause

These situations can be effectly modeled using a Poisson distribution. \pause
The Poisson distribution has probability mass function 
\[
p(x) = \frac{e^{-\lambda}\lambda^{x}}{x!} \quad \text{ for } x = 0,1,2,3,\ldots
\]
where $\lambda$ is called the \alert{rate parameter}. \pause
We write $X\sim Po(\lambda)$ to represent this random variable. \pause
We can show that 
\[ E[X] = Var[X] = \lambda. \]

\end{frame}








% \foilhead[-.75in]{\textcolor{blue}{Poisson pmf (cont'd)}}   
% \no  {\textcolor{cyan}{Check that $p(x)$ defined above is actually a probability mass function. How?}} \\[.1in]
% \no 1. Obviously, all values of $p(x)\geq 0$ for $x\geq 0$. \\[.1in]
% \no 2. Do all probabilities sum to 1? 
% $$\sum\limits_{x=0}^\infty p(x)=\sum\limits_{x=0}^\infty e^{-\lambda}\frac{\lambda^x}{x!}
% =e^{-\lambda} \cdot \sum\limits_{x=0}^\infty \frac{\lambda^x}{x!} =e^{-\lambda}e^\lambda=1$$
% \no Expected Value and Variance of $X \sim Po_{\lambda}$ are:}\\[.1in]
% \no \h \h  \bul  $E[X]= \sum_{x=0}^{\infty} x\frac{e^{-\lambda}\lambda^{x}}{x!} = 0 + e^{-\lambda}\sum\limits_{\textcolor{magenta}{x=1}}^\infty\frac{\lambda^x}{(x-1)!}=e^{-\lambda}\lambda\sum\limits_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!}$\\[.1in]
% \no \hspace*{2in} $= e^{-\lambda}\lambda\sum\limits_{y=0}^\infty \frac{\lambda^y}{y!}=\lambda$\\[.1in]
% \no \h \h  \bul $\text{Var}[X] = \ldots = \lambda$ (left as an exercise)


\begin{frame}[fragile]
\frametitle{Poisson distribution - example}

Customers of an internet service provider initiate new accounts at the average rate of 10 accounts per day. \pause
What is the probability that more than 8 new accounts will be initiated today? \pause

\vspace{0.1in} 

Let $X$ be the number of initiated accounts today. \pause 
Assume $X\sim Po(10)$. \pause
\[ P(X>8) = 1-P(X\le 8) = 1- \sum_{x=0}^8 \frac{\lambda^x e^{-\lambda}}{x!} \approx 1-0.333 = 0.667 \]
\pause 
In R, 
<<echo=TRUE>>=
# Using pmf
1-sum(dpois(0:8,10))

# Using cdf
1-ppois(8,10)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Poisson distribution - example}

Customers of an internet service provider initiate new accounts at the average rate of 10 accounts per day. \pause
What is the probability that more than 16 new accounts will be initiated in two days?

\vspace{0.2in} \pause

Since the rate is 10/day, then for two days we expect, on average, to have 20. \pause
Let $Y$ be he number initiated in a two-day period and assume $Y\sim Po(20)$. \pause
Then 
\[ 
P(Y>16) = 1-P(Y\le 16) = \sum_{x=0}^{16} \frac{\lambda^x e^{-\lambda}}{x!} = 1-0.221 = 0.779.
\]
In R,
<<echo=TRUE>>=
# Using pmf
1-sum(dpois(0:16,20))

# Using cdf
1-ppois(16,20)
@

\end{frame}



\subsection{Poisson approximation to a binomial}
\begin{frame}
\frametitle{Manufacturing example}

A manufacturer produces 100 chips per day and, on average, 1\% of these chips are defective. \pause
What is the probability that no defectives are found in a particular day?

\vspace{0.1in} \pause

Let $X$ represent the number of defectives and assume $X\sim Bin(100,0.01)$. \pause
Then 
\[ 
P(X=0) = {100\choose 0}(0.01)^0(1-0.01)^{100} \approx 0.366.
\]
\pause
Alternatively,
let $Y$ represent the number of defectives and assume $Y\sim Po(100\times 0.01)$. \pause
Then 
\[ 
P(Y=0) = \frac{1^0 e^{-1}}{0!} \approx 0.368.
\]
\end{frame}



\begin{frame}
\frametitle{Poisson approximation to the binomial}

\begin{theorem}
Let $\{X_n\}$ be a sequence of random variables such that $X_n\sim Bin(N_n, p_n)$ with $N_n\to \infty$ and $N_np_n\to \lambda \in (0,\infty)$, then 
\[ X_n \to X \sim Po(\lambda)
\]
in distribution.
\end{theorem}

\pause


For large $n$, the binomial dsitribution, $Bin(n,p)$, can be approximated by a Poisson distribution, $Po(np)$, since 
\[ 
{n \choose k} p^{k}(1-p)^{n-k} \approx e^{-np}\frac{(np)^{k}}{k!}.
\]

\vspace{0.1in} \pause

Rule of thumb: use Poisson approximation if $n \ge 20$ and $p \le 0.05$.

\end{frame}



\begin{frame}[fragile]
\frametitle{Example}
Imagine you are supposed to proofread a paper. 
Let us assume that there are on average 2 typos on a page and a page has 1000 words. \pause
This gives a probability of 0.002 for each word to contain a typo. \pause
What is the probability the page has no typos?

\vspace{0.1in} \pause

Let $X$ represent the number of typos on the page and assume $X\sim Bin(1000,0.002)$. \pause
$P(X=0)$ using R is
<<typo_binomial, echo=TRUE>>=
n = 1000; p = 0.002
dbinom(0,n,p)
@
Alternatively, let $Y$ represent the number of defectives and assume $Y\sim Po(1000\times 0.002)$. \pause
$P(Y=0)$ using R is
<<typo_poisson, echo=TRUE, dependson='typo_binomial'>>=
dpois(0,n*p)
@
\end{frame}


\end{document}







\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set07 - Discrete distributions}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Discrete distributions}
\begin{frame}
\frametitle{Special discrete distributions}

\begin{itemize}
\item Bernoulli distribution
\item Binomial distribution
\item Geometric distribution
\item Poisson distribution
\item Compounded distribution
\end{itemize}

\vspace{0.1in} \pause

Note: The sample space is always finite or countable.

\end{frame}


\subsection{Bernoulli}
\begin{frame}
\frametitle{Bernoulli distribution}

A Bernoulli experiment has only two outcomes: success/failure. 

\vspace{0.1in} \pause

Let 
\begin{itemize}
\item $X=1$ represent success and
\item $X=0$ represent failure. 
\end{itemize}

\vspace{0.1in} \pause

The probability mass function $p_X(x)$ is 
\[ 
p_X(0) = 1-p \quad p_X(1) = p.
\]

\vspace{0.1in} \pause

We use the notation $X\sim Ber(p)$ to denote a random variable $X$ that follows a Bernoulli distribution with success probability $p$, i.e. $P(X=1)=p$. 
\end{frame}


\begin{frame}
\frametitle{Bernoulli experiment examples}

\begin{example}
\begin{itemize}
\item Toss a coin: $\mOmega = \{H,T\}$
\item Throw a fair die and ask if the face value is a six: $\mOmega = \{{\text{face value is a six},\text{face value is not a six}}\}$
\item Sent a message through a network and record whether or not it is received: $\mOmega = \{ \text{successful transmission}, \text{ unsuccessful transmission}\}$
\item Draw a part from an assembly line and record whether or not it is defective: $\mOmega = \{\text{defective}, \text{ good}\}$
\item Response to the question ``Are you in favor of the above measure''? (in reference to a new tax levy to  be imposed on city residents): $\mOmega = \{\text{yes}, \text{no}\}$
\end{itemize}
\end{example}
\end{frame}



\begin{frame}
\frametitle{Bernoulli distribution (cont.)}

The cdf of the Bernoulli distribution is
\[
F_X(t) =P(X\leq t)= \left \{
\begin{array}{cl}
    0 & t < 0 \\
    1-p & 0 \le t < 1 \\
    1 & 1 \le t
\end{array}
\right .
\]

\vspace{0.1in} \pause

The expected value is 
\[ 
E[X]=\sum\limits_{x} p_X(x)=0(1-p)+1\cdot p=p.
\]

\vspace{0.1in} \pause

The variance is 
\[ Var[X]=\sum\limits_{x}(x-E[X])^2p_X(x)= (0-p)^2\cdot (1-p)+(1-p)^2 \cdot p = p (1-p). \]

\end{frame}




\begin{frame}
\frametitle{Sequence of Bernoulli experiments}

A compound experiment consisting of $n$ \alert{independent and identically distributed} Bernoulli experiments. \pause
E.g.
\begin{itemize}
\item Toss a coin $n$ times.
\item Send 23 identical messages through the network independently.
\item Draw 5 cards from a standard deck with replacement (and reshuffling) and record whether or not the card is a king.
\end{itemize}

\vspace{0.2in} \pause

What does \alert{independent and identically distributed} mean? 

\end{frame}

\begin{frame}
\frametitle{Independent and identically distributed}

Let $X_i$ represent the $i^{th}$ Bernoulli experiment. 

\vspace{0.1in} \pause 

\alert{Independence} means 

\[ 
P(X_1=x_1,\ldots,X_n=x_n) = \prod_{i=1}^n P(X_i=x_i),
\]
i.e. the joint probability is the product of the individual probabilities. 

\vspace{0.1in} \pause

\alert{Identically distributed} (for Bernoulli random variables) means 
\[ 
P(X_i=1) = p \quad \forall\, i,
\]
and more generally, the distribution is the same for all the random variables.

\vspace{0.1in} \pause

We will use $iid$ as a shorthand for \emph{independent and identically distributed}, although I will often use $ind$ to indicate \emph{independent} and let \emph{identically distributed} be clear from context. 

\end{frame}



\begin{frame}
\frametitle{Sequences of Bernoulli experiments}

Let $X_i$ denote the outcome of the $i^{th}$ Bernoulli experiment. \pause
We use the notation 
\[ 
X_i \iid Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
to indicate a sequence of $n$ independent and identically distributed Bernoulli experiments.

\vspace{0.1in}\pause

We could write this equivalently as 
\[ 
X_i \ind Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
\pause
but this is different than 
\[ 
X_i \ind Ber(p_i), \quad \mbox{for }i=1,\ldots,n
\]
as the latter has a different success probability for each experiment. 

\end{frame}


\subsection{Binomial distribution}
\begin{frame}
\frametitle{Binomial distribution}

Suppose we perform a sequence of $n$ $iid$ Bernoulli experiments and only record the number of successes, i.e. 
\[ 
Y = \sum_{i=1}^n X_i.
\]

\vspace{0.1in} \pause

Then we use the notation $Y\sim Bin(n,p)$ to indicate a binomial distribution with 
\begin{itemize}
\item $n$ attempts and 
\item probability of success $p$. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Binomial probability mass function}

We need to obtain 
\[ 
p_Y(y) = P(Y=y) \quad \forall y\in \mOmega \pause = \{0,1,2,\ldots,n\}.
\]

\pause
The probability of obtaining a particular sequence of $y$ success and $n-y$ failures is 
\[ 
p^y(1-p)^{n-y}
\]
since the experiments are $iid$ with success probability $p$. \pause But there are 
\[
{n\choose y} 
\]
ways of obtaining a sequence of $y$ success and $n-y$ failures. \pause Thus, the binomial probability mass function is 
\[ 
P_Y(y) = P(Y=y) = {n\choose y} p^y(1-p)^{n-y}.
\]

\end{frame}


\begin{frame}
\frametitle{Properties of the binomial distribution}

The expected value is 
\[ 
E[Y] = E\left[ \sum_{i=1}^n X_i \right] \pause = \sum_{i=1}^n E[X_i] \pause= \sum_{i=1}^n p = np.
\]

\vspace{0.1in} \pause

The variance is 
\[ 
Var[Y] = \sum_{i=1}^n Var[X_i] = np(1-p)
\]
since the $X_i$ are independent. 

\vspace{0.1in} \pause

The cumulative distribution function is 
\[ 
F_Y(y) = P(Y\le y) = \sum_{x=0}^{\lfloor y\rfloor} {n\choose x} p^x(1-p)^{n-x}.
\]

\end{frame}



\begin{frame}
\frametitle{Component failure rate}

Suppose a box contains 15 components that each have a failure rate of 5\%. 

\vspace{0.1in} \pause

What is the probability that 
\begin{enumerate}
	\item exactly two out of the fifteen components are defective?
	\item at most two components are defective?
	\item more than three components are defective?
	\item more than 1 but less than 4 are defective?
\end{enumerate}

\vspace{0.1in} \pause

Let $Y$ be the number of defective components. \pause Then $Y\sim Bin(15,0.05)$. \pause

{\small
\[ \begin{array}{lll}
1. & P(Y=2)&= P(Y=2) = {15\choose 2} (0.05)^2(1-0.05)^{15-2} \pause \\
2. & P(Y\le 2) &= \sum_{x=0}^2 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
3. & P(Y>3) &= 1-P(Y\le 3) = 1-\sum_{x=0}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
4. & P(1<Y<4) &= \sum_{x=2}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \\
\end{array} \]
}
\end{frame}



\begin{frame}[fragile]
\frametitle{Component failure rate (solutions in R)}

<<component_failure_rate,echo=TRUE>>=
n = 15
p = 0.05
choose(15,2)
dbinom(2,n,p)           # P(Y=2)
pbinom(2,n,p)           # P(Y<=2)
1-pbinom(3,n,p)         # P(Y>3) 
sum(dbinom(c(2,3),n,p)) # P(1<Y<4) = P(Y=2)+P(Y=3)
@

\end{frame}



% \foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example (Continued...)}}
%  \begin{enumerate}
% 	\item
% 	$\displaystyle P( \text{exactly two out of the fifteen components are
% 	defective} ) = p_X(2) = {15 \choose 2} 0.05^{2} 0.95^{13} = 0.1348.$
% 	\item
% 	$\displaystyle P( \text{at most two components are defective} ) = P(X \le 2) =
% 	B_{15,0.05}(2) = 0.9638.$
% 	\item $\displaystyle P(\text{ more than three components are defective }) = P(X > 3) =
% 	1 - P( X \le 3) = 1 - 0.9945 = 0.0055.$
% 	\item $\displaystyle P(\text{ more than 1 but less than 4 are defective }) = P( 1 <
% 	X < 4) = P(X \le 3) - P(X \le 1) = 0.9945 - 0.8290 = 0.1655.$
%     \end{enumerate}


\begin{frame}
\frametitle{Geometric distribution}

Rather than attempting $n$ times, 
we continue attempting until we get a success (where each trial has success probability $p$) and we record the number of the successful attempt\pause, i.e. 
\[ 
Z  = \min \{i|X_i=1 \}.
\]

\vspace{0.1in} \pause

Then $Z$ is said to have a geometric distribution and we write $Z\sim Geo(p)$ \pause with 
\begin{itemize}
\item sample space $\mOmega = \pause \{ 1, 2, 3, 4, \ldots \}$ \pause and
\item probability mass function 
\[ 
P_Z(z) = P(Z=z) = p(1-p)^{z-1}.
\]
\end{itemize}

\pause

{\bf Note:} An alternative version of the Geometric distribution does not record the successful attempt, but rather the number of failures.

\end{frame}



\begin{frame}
\frametitle{Properties of the geometric distribution}

The expectation of a geometric random variable is 
\[ 
E[Z] = \sum_{z=1}^{\infty} z p(1-p)^{z-1} = \ldots %= \frac{p}{(1-q)^2}
= \frac{1}{p}.
\]
\pause
The variance of a geometric random variable is 
\[ 
Var[Z] = \sum_{z=1}^{\infty} \left[z-\frac{1}{p}\right]^{2} (1-p)^{z-1}p = \ldots = \frac{1-p}{p^2}.
\]
\pause
The cumulative distribution function is 
\[ 
F_Z(z) = P(Z\le z) = 1 - [1-p]^{\lfloor z \rfloor}
\]
but is actually easier to calculate the complement, i.e. 
\[ 
P(Z>z) = [1-p]^{\lfloor z \rfloor}.
\]

\end{frame}





% \foilhead[-.8in]{\textcolor{blue}{Geometric distribution}}
% \no  {\textcolor{magenta}{Review:} $X=$number of repetitions of the experiment  until we
% have the first success in a {\textcolor{red}{Bernoulli experiment.}   \\[.1in]
% \no  1. The {\textcolor{magenta}{pmf}} is: $p_{X}(k) = P(X=k) = \underbrace{(1-p)^{k-1}}_{k-1 \text{ failures}}
% \cdot \underbrace{p}_{\text{success!}}$\\[.1in]
% \no 2. Expectation $E[X]=\frac{1}{p}$,\h \h  Variance $\text{Var}[X]={\frac{1-p}{p^{2}}}$\\[.1in]
% \no 3. The {\textcolor{magenta}{cdf}} is: $F_{X}(t) =P(X\leq t)= 1 - (1-p)^{\lfloor {t}\rfloor}$ \\[.1in]
% \no  {\textcolor{magenta}{Example 1:} {\textcolor{cyan}{Examine the following programming statement:}}\\[.1in]
% \no \hspace*{2in} {\textcolor{red}{ {\tt Repeat S until B}}}\\[.1in]
% \no  {\textcolor{magenta}{Solution:}   Assume $P(\tt{B} = \text{true}) = 0.1$ and let $X$ be the number of times {\tt S} is executed. Then, $X$ has a geometric distribution  with pmf:\\[.1in]
% \no \hspace*{1in} $P(X = k) = p_{X}(k) = 0.9^{k-1}\cdot 0.1 $\\[.1in]
% \no How often is {\tt S} executed on average? - What is $E[X]$?
% 
% \foilhead[-.8in]{\textcolor{blue}{Geometric distribution Example 2}}
%  \no  {\textcolor{magenta}{Example 2.}   {\textcolor{cyan}{Watch the input queue at the alpha farm for a job that times out.
%     The probability that a job times out is 0.05. Let $Y$  be the index of the first job to time out,
%     then $Y \sim Geo_{0.05}$. What's then the probability that}}
%     \begin{itemize}
% 	\item  {\textcolor{cyan}{the third job times out?}}
% 	
% 	{\it $P(Y = 3) = 0.95^{2} 0.05 = 0.045$
% 	}
% 	\item  {\textcolor{cyan}{$Y$ is less than 3?}}
% 	
% 	{\it $P( Y < 3) = P( Y \le 2) = 1 - 0.95^{2} = 0.0975$
% 	}
% 	\item  {\textcolor{cyan}{the first job to time out is between the third and the seventh?}}
% 	
% 	{\it $P( 3 \le Y \le 7) = P(Y \le 7) - P(Y \le 2) = 1-0.95^{7} - (1 -
% 	0.95^{2}) = 0.204$}
%     \end{itemize}
% \foilhead[-.8in]{\textcolor{blue}{Geometric distribution Example 2 (cont'd)}}    
% \no   {\textcolor{cyan}{ What are the expected value for $Y$, what is $Var[Y]$?}}\\[.1in]
% \no {\it Plugging in $p=0.05$ in the above formulas gives us:
% \vspace*{-.2in}
% \begin{eqnarray*}
% 	E[Y] &=& \frac{1}{p} = 20 \hspace{2cm} \text{\small we expect the
% 	20th job to be the first to time out}\\
% 	Var[Y] &=& \frac{1-p}{p^{2}} = 380 \hspace{2cm} \text{\small very
% 	 spread out!}
% \end{eqnarray*}} 
% 
% \no  {\textcolor{magenta}{Interesting property of the Geometric distribution}}\\[.1in]
% \no If $X \sim Geo_p$, then $P(X\ge i+j|X\ge i)= P(X\ge j)$ for $i,j=0,1,2,\ldots$\\[.1in]
% %\no   {\textcolor{cyan}{That is , the fact that  one has already observed $i$ successive failures does not change the the distribution of the number of trials required to obtain the first success.}}
% \no {\textcolor{cyan}{That is, $X$ is memoryless: ``does not remember that it counts up to $i$ already"!}}
% 
% \foilhead[-.8in]{\textcolor{blue}{Poisson distribution}}
% \no  {\textcolor{magenta}{Situation:}} {\textcolor{cyan}{The Poisson distribution follows from a certain set of assumptions about the occurrence of ``rare'' events in time or space.}} \\[.1in]
% \no  {\textcolor{magenta}{Examples:} \\[.1in]
% \no $X$ = \# of alpha particles emitted from a polonium bar in an 8 minute
% period.\\[.1in]
% \no $Y$ = \# of flaws on a standard size piece of manufactured product
% (e.g., 100m coaxial cable, 100 sq.meter plastic sheeting)\\[.2in]
% \no $Z$ = \# of hits on a web page in a 24h period.\\[.1in]
% \no {\textcolor{red}{Definition:}  The Poisson probability mass function {\textcolor{magenta}{(pmf)}} is defined as:\\[.15in]
% \no \hspace*{1.5in} $p(x) = \frac{e^{-\lambda}\lambda^{x}}{x!} \hspace{1cm} \text{ for }
% x = 0,1,2,3,\ldots$\\[.15in]
% \no \hspace*{1.5in}  $\lambda$ is called the {\textcolor{magenta}{{\it rate parameter}}}.\\[.1in]
% \no  We denote the {\textcolor{magenta}{cdf} by $Po_{\lambda}(t)$ 
% 
% 
% \foilhead[-.75in]{\textcolor{blue}{Poisson pmf (cont'd)}}   
% \no  {\textcolor{cyan}{Check that $p(x)$ defined above is actually a probability mass function. How?}} \\[.1in]
% \no 1. Obviously, all values of $p(x)\geq 0$ for $x\geq 0$. \\[.1in]
% \no 2. Do all probabilities sum to 1? 
% $$\sum\limits_{x=0}^\infty p(x)=\sum\limits_{x=0}^\infty e^{-\lambda}\frac{\lambda^x}{x!}
% =e^{-\lambda} \cdot \sum\limits_{x=0}^\infty \frac{\lambda^x}{x!} =e^{-\lambda}e^\lambda=1$$
% \no Expected Value and Variance of $X \sim Po_{\lambda}$ are:}\\[.1in]
% \no \h \h  \bul  $E[X]= \sum_{x=0}^{\infty} x\frac{e^{-\lambda}\lambda^{x}}{x!} = 0 + e^{-\lambda}\sum\limits_{\textcolor{magenta}{x=1}}^\infty\frac{\lambda^x}{(x-1)!}=e^{-\lambda}\lambda\sum\limits_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!}$\\[.1in]
% \no \hspace*{2in} $= e^{-\lambda}\lambda\sum\limits_{y=0}^\infty \frac{\lambda^y}{y!}=\lambda$\\[.1in]
% \no \h \h  \bul $\text{Var}[X] = \ldots = \lambda$ (left as an exercise)
% \foilhead[-.7in]{\textcolor{blue}{Poisson distribution: Example 3.22 (Baron) }}
% \no {\textcolor{magenta}{New Accounts} {\textcolor{cyan}{Customers of an internet service provider initiate new accounts at the average rate of 10 accounts per day.}}\\[.1in] 
% \no {\textcolor{magenta}{Part (a)}} {\textcolor{cyan}{ What is the probability that more than 8 new accounts will be initiated today?}}\\[.1in]
% \no The number of initiation per day $X$ has a Poisson distribution with parameter $\lambda=10$.\\[.1in]
% \no ( The above assumes that account initiations is a rare event within the time period of one day because no two customers can open an account at the same time.)\\[.1in]
% \no Then we have $P(X>8)= 1-Po_{10}(8)=1-0.333=.667$\\[.1in]
% \foilhead[-.7in]{\textcolor{blue}{Poisson distribution: Example 3.22 (Baron) (Cont'd }}
% \no {\textcolor{magenta}{Part (b)}} {\textcolor{cyan}{ What is the probability that more than 16 new accounts will be initiated in two days?}}\\[.1in]
% \no The number of initiation in a two-day period $Y$ has a Poisson distribution with parameter $\lambda=20$\\[.1in]
% \no (Note carefully that the average number of initiation for a two-day period is 20)\\[.1in]
% \no Then we have $$P(Y>16)= 1-Po_{20}(16)=1-0.221=.779$$
% \no Note that $X$ and $Y$ are random variables with different Poisson distributions because the events they represent occur during different time intervals.\\[.1in]
% \no This is a key step in solving Poisson distribution related problems.
% 
% \foilhead[-.8in]{\textcolor{blue}{Poisson distribution: Another Example }}
% \no How do we choose $\lambda$ in an example? - look at the expected
% value! \\[.1in]
% \no  {\textcolor{magenta}{Example:} {\textcolor{cyan}{ A manufacturer of chips produces 1\%  defectives.
%     What is the probability that in a box of 100 chips no defective is found?}}\\[.1in] 
% \no  {\textcolor{magenta}{Solution:}     Let $X$ be the number of defective chips found in the box.
% Model $X$ as a \textbf{Binomial} variable with distribution $B_{100,0.01}$.
% Then $$P(X=0) = {100 \choose 0} 0.99^{100} 0.01^{0} = 0.366.$$\\[-.2in] 
% \no  {\textcolor{magenta}{Approximation:}  On the other hand, a defective chip
%   can be
% considered to be a rare event, since $p$ is small ($p = 0.01$). So, approximate $X$
% as \textbf{Poisson} variable.\\[.1in] 
% \no We need to obtain a value for $\lambda$!
% \foilhead[-.8in]{\textcolor{blue}{Poisson distribution: Example (cont'd)}}
% \no Note that we expect $100 \cdot 0.01$ = 1 chip out of the box to be defective.\\[.1in] 
% \no We know that the expected value of $X$ is $\lambda$. In this example, therefore, we take $\lambda = 1$.\\[.1in] 
% \no Then $$P(X=0) = \frac{e^{-1}1^{0}}{0!} = 0.3679.$$\\[.1in] 
% %\no  {\textcolor{red}{Remark!} {\textcolor{cyan}{ No big differences between the two approaches!}}\\[.1in] 
% 
% \no  {\textcolor{red}{Ramification:} For larger $k$, however, the binomial coefficient ${n \choose k}$
%      becomes hard to compute, and it is easier to use the Poisson
%      distribution instead of the Binomial distribution. 
% 
% \foilhead[-.8in]{\textcolor{blue}{Poisson to approximate Binomial}}
% \no  {\textcolor{red}{Result (not a theorem):} For large $n$, the Binomial distribution can be  approximated by the
% Poisson distribution, where $\lambda$ is taken as $np$:
% \[
% {n \choose k} p^{k}(1-p)^{n-k} \approx e^{-np}\frac{(np)^{k}}{k!}
% \]
% \no {\textcolor{red}{Rule of thumb:} use Poisson approximation if $n \ge 20$ and (at the
% same time) $p \le 0.05$.\\[.1in] 
% \no {\textcolor{red}{Theorem:}} $\{X_n\}$ is a sequence of random variables s.t. $X_n\sim \text{Bin}(N_n,p_n)$ with $N_n\to \infty$, $p_n\to 0$ and $N_np_n\to \lambda\in (0,\infty)$, then\\[.1in]
% \hspace*{2in} $X_n\to X\sim \text{Poisson}(\lambda)$\\[.1in]
%  in distribution.\\[.1in] 
% \no Such a beautiful result requires very delicate mathematics.
% %: Levy-Cramer continuity theorem and concept of weak convergence on metric spaces. 
% 
% \foilhead[-.8in]{\textcolor{blue}{Poisson to approximate Binomial (example)}}
% \no {\textcolor{magenta}{Example: (Typos)} } {\textcolor{cyan}{Imagine you are supposed to proofread a paper. Let us assume that there are on average 2 typos on a page and a page has 1000 words. This gives a probability of 0.002 for each word to contain a typo. The number of typos on a page $X$ is then a Binomial random variable, i.e. $X\sim B_{1000, 0.002}$.}}\\[.1in] 
% \no The probability for no typo on a page is $P(X=0)$, i.e 
% $$P(X=0)=(1-0.002)^{1000}=0.998^{1000}=0.13506$$
% alternatively 
% $$P(X=0)=\Big(1-\frac2{1000}\Big)^{1000}\approx e^{-2}=0.13534$$
% since $(1-x/n)^n\to e^x$. \\[.1in] 
% \no The probability of one typo on a page is 
% $$P(X=1)= {1000 \choose 1}0.002\cdot 0.998^{999}=0.27067$$
% and 
% $$P(X=1)=1000\cdot \frac2{1000}\Big(1-\frac2{1000}\Big)^{999}\approx 2\cdot e^{-2}=0.27067!$$
% \no {\textcolor{cyan}{So basically, we are calculating this probability using the Poisson pmf with $\lambda=1000 \cdot 0.002=2$}}\\[.1in]
% \no That is use  $P(X=x)= \frac{e^{-\lambda}\lambda^{x}}{x!}$ to calculate 
% $$ P(X=1) \approx \frac{e^{-2} 2^{1}}{1!}=2\cdot e^{-2}=0.27067$$ 
% 
% \foilhead[-.8in]{\textcolor{blue}{Poisson to approximate Binomial (example cont'd)}}
% \no The probability for two typos on a page is $P(X=2)$, i.e
% $$P(X=2)={1000 \choose 2}(1-0.002)^{998}0.002^2=0.27094$$
% \no alternatively, using $X \approx Po_2$
% $$P(X=2) \approx \frac{e^{-2} 2^{2}}{2!}=0.27067$$





\end{document}







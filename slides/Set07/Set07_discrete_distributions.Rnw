\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set07 - Discrete distributions}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Discrete distributions}
\begin{frame}
\frametitle{Special discrete distributions}

\begin{itemize}
\item Bernoulli distribution
\item Binomial distribution
\item Geometric distribution
\item Poisson distribution
\item Compounded distribution
\end{itemize}

\vspace{0.1in} \pause

Note: The sample space is always finite or countable.

\end{frame}


\subsection{Bernoulli}
\begin{frame}
\frametitle{Bernoulli distribution}

A Bernoulli experiment has only two outcomes: success/failure. 

\vspace{0.1in} \pause

Let 
\begin{itemize}
\item $X=1$ represent success and
\item $X=0$ represent failure. 
\end{itemize}

\vspace{0.1in} \pause

The probability mass function $p_X(x)$ is 
\[ 
p_X(0) = 1-p \quad p_X(1) = p.
\]

\vspace{0.1in} \pause

We use the notation $X\sim Ber(p)$ to denote a random variable $X$ that follows a Bernoulli distribution with success probability $p$, i.e. $P(X=1)=p$. 
\end{frame}


\begin{frame}
\frametitle{Bernoulli experiment examples}

\begin{example}
\begin{itemize}
\item Toss a coin: $\mOmega = \{H,T\}$
\item Throw a fair die and ask if the face value is a six: $\mOmega = \{{\text{face value is a six},\text{face value is not a six}}\}$
\item Sent a message through a network and record whether or not it is received: $\mOmega = \{ \text{successful transmission}, \text{ unsuccessful transmission}\}$
\item Draw a part from an assembly line and record whether or not it is defective: $\mOmega = \{\text{defective}, \text{ good}\}$
\item Response to the question ``Are you in favor of the above measure''? (in reference to a new tax levy to  be imposed on city residents): $\mOmega = \{\text{yes}, \text{no}\}$
\end{itemize}
\end{example}
\end{frame}



\begin{frame}
\frametitle{Bernoulli distribution (cont.)}

The cdf of the Bernoulli distribution is
\[
F_X(t) =P(X\leq t)= \left \{
\begin{array}{cl}
    0 & t < 0 \\
    1-p & 0 \le t < 1 \\
    1 & 1 \le t
\end{array}
\right .
\]

\vspace{0.1in} \pause

The expected value is 
\[ 
E[X]=\sum\limits_{x} p_X(x)=0(1-p)+1\cdot p=p.
\]

\vspace{0.1in} \pause

The variance is 
\[ Var[X]=\sum\limits_{x}(x-E[X])^2p_X(x)= (0-p)^2\cdot (1-p)+(1-p)^2 \cdot p = p (1-p). \]

\end{frame}




\begin{frame}
\frametitle{Sequence of Bernoulli experiments}

A compound experiment consisting of $n$ \alert{independent and identically distributed} Bernoulli experiments. \pause
E.g.
\begin{itemize}
\item Toss a coin $n$ times.
\item Send 23 identical messages through the network independently.
\item Draw 5 cards from a standard deck with replacement (and reshuffling) and record whether or not the card is a king.
\end{itemize}

\vspace{0.2in} \pause

What does \alert{independent and identically distributed} mean? 

\end{frame}

\begin{frame}
\frametitle{Independent and identically distributed}

Let $X_i$ represent the $i^{th}$ Bernoulli experiment. 

\vspace{0.1in} \pause 

\alert{Independence} means 

\[ 
P(X_1=x_1,\ldots,X_n=x_n) = \prod_{i=1}^n P(X_i=x_i),
\]
i.e. the joint probability is the product of the individual probabilities. 

\vspace{0.1in} \pause

\alert{Identically distributed} (for Bernoulli random variables) means 
\[ 
P(X_i=1) = p \quad \forall\, i,
\]
and more generally, the distribution is the same for all the random variables.

\vspace{0.1in} \pause

We will use $iid$ as a shorthand for \emph{independent and identically distributed}, although I will often use $ind$ to indicate \emph{independent} and let \emph{identically distributed} be clear from context. 

\end{frame}



\begin{frame}
\frametitle{Sequences of Bernoulli experiments}

Let $X_i$ denote the outcome of the $i^{th}$ Bernoulli experiment. \pause
We use the notation 
\[ 
X_i \iid Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
to indicate a sequence of $n$ independent and identically distributed Bernoulli experiments.

\vspace{0.1in}\pause

We could write this equivalently as 
\[ 
X_i \ind Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
\pause
but this is different than 
\[ 
X_i \ind Ber(p_i), \quad \mbox{for }i=1,\ldots,n
\]
as the latter has a different success probability for each experiment. 

\end{frame}


\subsection{Binomial distribution}
\begin{frame}
\frametitle{Binomial distribution}

Suppose we perform a sequence of $n$ $iid$ Bernoulli experiments and only record the number of successes, i.e. 
\[ 
Y = \sum_{i=1}^n X_i.
\]

\vspace{0.1in} \pause

Then we use the notation $Y\sim Bin(n,p)$ to indicate a binomial distribution with 
\begin{itemize}
\item $n$ attempts and 
\item probability of success $p$. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Binomial probability mass function}

We need to obtain 
\[ 
p_Y(y) = P(Y=y) \quad \forall y\in \mOmega \pause = \{0,1,2,\ldots,n\}.
\]

\pause
The probability of obtaining a particular sequence of $y$ success and $n-y$ failures is 
\[ 
p^y(1-p)^{n-y}
\]
since the experiments are $iid$ with success probability $p$. \pause But there are 
\[
{n\choose y} 
\]
ways of obtaining a sequence of $y$ success and $n-y$ failures. \pause Thus, the binomial probability mass function is 
\[ 
P_Y(y) = P(Y=y) = {n\choose y} p^y(1-p)^{n-y}.
\]

\end{frame}


\begin{frame}
\frametitle{Properties of the binomial distribution}

The expected value is 
\[ 
E[Y] = E\left[ \sum_{i=1}^n X_i \right] \pause = \sum_{i=1}^n E[X_i] \pause= \sum_{i=1}^n p = np.
\]

\vspace{0.1in} \pause

The variance is 
\[ 
Var[Y] = \sum_{i=1}^n Var[X_i] = np(1-p)
\]
since the $X_i$ are independent. 

\vspace{0.1in} \pause

The cumulative distribution function is 
\[ 
F_Y(y) = P(Y\le y) = \sum_{x=0}^{\lfloor y\rfloor} {n\choose x} p^x(1-p)^{n-x}.
\]

\end{frame}



% \foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example}}
%  {\textcolor{blue} {Example:}} Compute the probabilities for the following events: \\[.1in]
%     A box contains 15 components that each have a failure rate of 5\%.
%     What is the probability that
%     \begin{enumerate}
% 	\item exactly two out of the fifteen components are defective?
% 	\item at most two components are defective?
% 	\item more than three components are defective?
% 	\item more than 1 but less than 4 are defective?
%     \end{enumerate}
%    {\textcolor{blue} {Solution:}}   Let $X$ be the number of defective components. Then $X$ has a
%     $B_{15,0.05}$ distribution.
%     %We shall look-up values of $B_{15,0.05}$ from Table A2.  of Baron.
% 
% 
% \foilhead[-.8in]{\textcolor{blue}{Binomial distribution: Example (Continued...)}}
%  \begin{enumerate}
% 	\item
% 	$\displaystyle P( \text{exactly two out of the fifteen components are
% 	defective} ) = p_X(2) = {15 \choose 2} 0.05^{2} 0.95^{13} = 0.1348.$
% 	\item
% 	$\displaystyle P( \text{at most two components are defective} ) = P(X \le 2) =
% 	B_{15,0.05}(2) = 0.9638.$
% 	\item $\displaystyle P(\text{ more than three components are defective }) = P(X > 3) =
% 	1 - P( X \le 3) = 1 - 0.9945 = 0.0055.$
% 	\item $\displaystyle P(\text{ more than 1 but less than 4 are defective }) = P( 1 <
% 	X < 4) = P(X \le 3) - P(X \le 1) = 0.9945 - 0.8290 = 0.1655.$
%     \end{enumerate}
% \foilhead[-.8in]{\textcolor{blue}{Example from Baron}}
% {\textcolor{magenta}{Example from Baron: }}{\textcolor{cyan}{As part of a business strategy, randomly selected 20\% of new internet service subscribers receive a special promotion from the provider. A group of 10 neighbors signs up for the service. What is the probability that at least 4 of them get a special promotion?}}\\[.1in]
% {\textcolor{magenta}{Solution:}} \\[.1in]
% Need to calculate $P(X \ge 4)$ where $X$ is the number of people out of 10 who will receive the special promotion.\\[.1in]
% Modelling $X$ as the number of successes in 10 Bernoulli trials each with $P(\text{success})=0.2$, we have that $X \sim Bin(10,0.2)$\\[.1in]
% Thus
% $$ P(X \ge 4) = 1-P(X\le 3)= 1- B_{10,0.2}(3)=1-0.8791=0.1209$$
% 
% \foilhead[-.8in]{\textcolor{blue}{Geometric distribution}}\vspace{5pt}
%  {\textcolor{magenta}{Situation:} \\[.1in] {\textcolor{cyan}{1. We have a single Bernoulli experiment with probability for
% success $p$. \\[.1in]
% 2. Now, we repeat this experiment until we have a first success. \\[.1in]
% 3. Denote by $X$ the number of repetitions of the experiment  until we
% have the first success, i.e  $X = k$ implies that, we  have $k-1$ failures and the first
% success in the $k$th repetition of the experiment.}} \\[.1in]
%  {\textcolor{red}{Question:}  It is very natural to ask what is the probability distribution of $X$? \\[.1in]
% {\textcolor{red}{Pmf: }The sample space $\mOmega$ is infinite and starts at 1 (we
% need at least one success):
% \[
% \mOmega = \{ 1, 2, 3, 4, \ldots \}
% \] and the pmf
% \[
% p_X(k) = P(X=k) = \underbrace{(1-p)^{k-1}}_{k-1 \text{ failures}} %\qquad \text{for} k=1,2,3,\ldots
% \cdot \underbrace{p}_{\text{success!}}
% \]
%  
%  
% 
% \foilhead[-.75in]{\textcolor{blue}{Geometric distribution (cont'd)}}
%  This probability mass function is called the {\textcolor{red} {Geometric mass function.}} \\[.15in]
%   {\textcolor{magenta}{Expectation and Variance of the Geometric random variable:}}\\[-.3in]
% \begin{eqnarray*}
%     E[X] &=& \sum_{i=1}^{\infty} i (1-p)^{i}p = \ldots = \frac{p}{(1-q)^2}=\frac{1}{p},
%     \\
%     Var[X] &=& \sum_{i=1}^{\infty} (i-\frac{1}{p})^{2} (1-p)^{i}p =
%     \ldots = \frac{1-p}{p^{2}}.
% \end{eqnarray*}
% The {\textcolor{magenta} {c.d.f.} is given by :
% \[
% F_X(t) = 1 - (1-p)^{\lfloor {t}\rfloor} =: Geo_{p}(t)
% \]
%  {\textcolor{magenta}{Proof:}} If $X$ is greater than $t$, this means
% that the first $\lfloor{t}\rfloor$ trials yields failures. This probability is easy to
% compute: just $(1-p)^{\lfloor{t}\rfloor}$. 
%  
 
\end{document}





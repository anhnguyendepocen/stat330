\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\newtheorem{principle}[theorem]{Principle}

\title{Set 20 - Confidence interval}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=TRUE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE, echo=FALSE>>=
library(dplyr)
library(ggplot2)
@

<<set_seed, echo=FALSE>>=
set.seed(1)
@


\begin{frame}
\maketitle
\end{frame}

\section{Confidence intervals}
\begin{frame}
\frametitle{Uncertainty estimation}

When we have an estimate $\hat{\theta}$ of a population parameter $\theta$, we
know almost surely that 
\[ 
\hat{\theta} \ne \theta.
\]
\pause
Thus, we'd like to provide an estimate of the uncertainty our knowledge of 
$\theta$.
\pause
\begin{definition}
An interval $[L,U]$ is a $100(1-a)$\% \alert{confidence level} for the parameter 
$\theta$ if it contains the parameter with probability $(1-a)$ (when the data 
are considered random), \pause
\[ 
P(L\le \theta \le U) = 1-a.
\]
\pause
The \alert{coverage probability} $(1-a)$ is also called the 
\alert{confidence level}.
\end{definition}

\end{frame}



\begin{frame}
\frametitle{Normal-based confidence intervals}

Suppose we have an estimator $\hat{\theta}$ for a population parameter $\theta$
with 
\[ 
\hat{\theta} \sim N\left(\theta,\sigma(\hat{\theta})^2\right)
\]
\pause
i.e. when the data are considered random, the estimator is unbiased and has a
normal distribution.
\pause
Then a $100(1-a)$ confidence interval can be constructed by using 
\[ \begin{array}{rl}
1-a &= P\left( -z_{a/2} \le \frac{\hat{\theta}-\theta}{\sigma(\hat{\theta})} \le z_{a/2} \right) \pause \\
&= P\left(\hat{\theta}-z_{a/2} \sigma(\hat{\theta}) \le \theta \le \hat{\theta}+z_{a/2} \sigma(\hat{\theta}) \right)
\end{array} \]
\pause 
where the \alert{critical value} $z_{a/2}$ is the $(a/2)$-quantile of the standard 
normal distribution. \pause
Thus
\[ 
\hat{\theta} \pm z_{a/2}\sigma(\hat{\theta})
\]
is a $100(1-a)$\% confidence interval.
\end{frame}


%' \begin{frame}
%' \frametitle{Critical values}
%' <<>>=
%' a = .05
%' z_crit = qnorm(a/2)
%' curve(dnorm, -3, 3)
%' abline(v=z_crit)
%' 
%' @
%' \end{frame}


\subsection{Normal mean with known variance}
\begin{frame}
\frametitle{Normal mean with known variance}

Suppose $X_1,\ldots,X_n \ind N(\mu,s^2)$ and we use $\hat{\mu} = 
\overline{X}$. \pause
Then 
\[ 
\overline{X} \pause \sim N(\mu,s^2/n)
\]
\pause 
and thus 
\[
Z = \frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1).
\]
Using the formula on the previous slide, we have 
\[ 
\overline{x} \pm z_{a/2} s/\sqrt{n}.
\]
is a $100(1-a)$\% confidence interval for $\mu$. 
\end{frame}




\begin{frame}
\frametitle{Mean height of US women}

Suppose we are interested in the mean height $(\mu)$ of women in the U.S. \pause
Let $X_i$ represent the height (inches) for woman $i$. \pause
Assume 
\[ 
X_i \ind N(\mu,s^2)
\]
\pause
where $s^2 = 3.5^2$ inches$^2$. \pause
In our random sample of 100 women, we find the sample average to be 
$\overline{x}=64.66$ inches. \pause
A 95\% confidence interval for $\mu$ is 
\[ 
\overline{x} \pm z_{a/2} s/\sqrt{n} \pause 
= 64.66 \pm 1.96 \cdot 3.5/\sqrt{100} \pause
= (63.97,65.35).
\]
\pause 
An interpretation for this confidence interval is 
\begin{quote}
the mean height $(\mu)$ would be within the intervals constructed using this 
procedure in 95\% of the samples that we could have taken.
\end{quote}
\end{frame}




\begin{frame}[fragile]
<<echo=FALSE, fig.height=5>>=
n  = 100
mu = 65
s  = 3.5
a  = 0.05
z  = -qnorm(a/2)
d = data.frame(sample = 1:n, 
               xbar = rnorm(100, mu, s/sqrt(n))) %>%
  mutate(L  = xbar - z*s/sqrt(n),
         U  = xbar + z*s/sqrt(n),
         success = (L<mu) & (mu<U))
ggplot(d, aes(sample, ymin=L, ymax=U,color=success)) + 
  geom_errorbar() + 
  theme_bw() +
  geom_hline(yintercept = mu) + 
  coord_flip()
@
\end{frame}



\subsection{Normal mean with unknown variance}
\begin{frame}
\frametitle{Normal mean with unknown variance}

Typically, we will not know the variance and thus we assume
\[ 
X_i \ind N(\mu,\sigma^2)
\]
for some unknown $\mu$ and $\sigma^2$. 
\pause
We obtain a sample mean $(\overline{x})$ and sample variance $s^2$.

\vspace{0.1in} \pause

Standardizing using the sample standard deviation results in  
\[ 
\frac{\overline{x}-\mu}{s/\sqrt{n}} \sim t_{n-1}
\]
\pause
i.e. a $t$-distribution with $n-1$ degrees of freedom
\pause
and construction of a $100(1-a)$\% confidence interval for $\mu$ using 
\[
\overline{x} \pm t_{n-1,a/2} s/\sqrt{n}
\]
where $t_{n-1,a/2}$ is the $(a/2)$-quantile of a $t$-distribution with $n-1$ 
degrees of freedom.

\end{frame}


\begin{frame}
\frametitle{$t$-distribution}

\vspace{-0.05in}

\small

If $T$ is a random variable with a $t$-distribution, then $T$ is a continuous
random variable with the parameter $\nu$, called the \alert{degrees of freedom},
\pause 
and we use the notation
\[ 
T \sim t_\nu \mbox{ as }\nu\to\infty.
\]
\pause 
A $t$-distributed random variable has the following probability density function 
with support over the real line
\[ 
f(t) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}\left( 1+\frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2}}
\]
\pause
and moments

\[ \begin{array}{rcl}
E[T] =& 0 & \mbox{for }\nu>1 \mbox{ and} \\
Var[T] =& \frac{\nu}{\nu-2} & \mbox{for }\nu>2.
\end{array} \]

\pause

If we use $T_\nu$ to denote a $t$-distributed random variable with degrees
of freedom $\nu$. 
Then 
\[
T_\nu \to Z 
\]
\pause 
i.e. $t$-distribution converges to a standard normal distribution the degrees of 
freedom increase.

\end{frame}


\begin{frame}
\frametitle{$t$ probability density functions}

<<echo=FALSE>>=
d = data.frame(x = seq(-3,3,length=101)) %>%
  mutate(normal = dnorm(x),
         t_1 = dt(x,1),
         t_5 = dt(x,5),
         t_30 = dt(x,30)) %>%
  tidyr::gather(distribution, density, -x) %>%
  mutate(distribution = factor(distribution, levels=c("normal","t_1","t_5","t_30")))

ggplot(d, aes(x, density, color=distribution, group=distribution)) +
  geom_line() +
  theme_bw()
@

\end{frame}




\begin{frame}
\frametitle{Mean height of US women}

Suppose we are interested in the mean height $(\mu)$ of women in the U.S. \pause
Let $X_i$ represent the height (inches) for woman $i$. \pause
Assume 
\[ 
X_i \ind N(\mu,\sigma^2)
\]
\pause
In a random sample of 10 women, We find the sample average to be 
$\overline{x}=64.66$ inches and sample standard deviation to be $s^2=4.06$. \pause
A 95\% confidence interval for $\mu$ is 
\[ 
\overline{x} \pm t_{n-1,a/2} s/\sqrt{n} \pause 
= 64.66 \pm 2.26 \cdot 4.06/\sqrt{100} \pause
= (63.74,65.58).
\]
\pause 
An interpretation for this confidence interval is 
\begin{quote}
the mean height $(\mu)$ would be within the intervals constructed using this 
procedure in 95\% of the samples that we could have taken.
\end{quote}
\end{frame}





\begin{frame}[fragile]
<<echo=FALSE, fig.height=5>>=
n  = 10
mu = 65
sigma  = 3.5
a  = 0.05
t  = -qt(a/2,n-1)
d = plyr::rdply(100, {
  data.frame(x = rnorm(10, mu, sigma))
}, .id = "sample") %>%
  group_by(sample) %>%
  summarize(xbar = mean(x),
            s    = sd(x),
            L  = xbar - t*s/sqrt(n),
            U  = xbar + t*s/sqrt(n),
            success = (L<mu) & (mu<U))
  
ggplot(d, aes(sample, ymin=L, ymax=U,color=success)) + 
  geom_errorbar() + 
  theme_bw() +
  geom_hline(yintercept = mu) + 
  coord_flip()
@
\end{frame}




\subsection{Asymptotic confidence intervals}
\begin{frame}
\frametitle{Large sample (asymptotic) confidence intervals}

As the sample size gets larger, many estimators have an asymptotically normal
distribution (when the data are considered random) centered on the true\pause,
i.e.
\[ 
\hat{\theta} \stackrel{\cdot}{\sim} N\left(\theta,\sigma(\hat{\theta})^2\right)
\]
\pause
If we have a consistent estimator of the standard error, say $s(\hat{\theta})$ 
then we can replace the unknown standard error with the consistent estimator\pause, i.e.
\[ 
\hat{\theta} \stackrel{\cdot}{\sim} N\left(\theta,s(\hat{\theta}^2)\right).
\]
\pause
Thus, we can obtain an \alert{approximate} $100(1-a)$\% confidence interval by 
\[ 
\hat{\theta} \pm z_{a/2} s(\hat{\theta}).
\]
\end{frame}




\begin{frame}
\frametitle{Die rolling example}

Let $X_i \ind Ber(\theta)$ and estimate $\theta$ using 
\[ 
\hat{\theta} = \frac{1}{n}\sum_{i=1}^n X_i.
\]
\pause
We know that 
\[ 
E\left[\hat{\theta}\right] \pause = \theta 
\pause
\quad\mbox{and}\quad
Var\left[\hat{\theta}\right] \pause = \frac{\theta(1-\theta)}{n} .
\]
\pause
Thus, we estimate the standard error using 
\[ 
s\left(\hat{\theta}\right) = \sqrt{\frac{\hat\theta(1-\hat\theta)}{n}}
\]
\pause
and construct an approximate $100(1-a)$\% confidence interval using 
\[ 
\hat{\theta} \pm z_{a/2} \sqrt{\frac{\hat\theta(1-\hat\theta)}{n}}.
\]
\end{frame}






% \begin{frame}
% \frametitle{Difference in means}
% 
% Suppose independently
% \[ 
% X_1,\ldots,X_{n_X} \ind N(\theta_X,s_X^2) 
% \quad\mbox{and}\quad
% Y_1,\ldots,Y_{n_Y} \ind N(\theta_Y,s_Y^2)
% \]
% \pause 
% and we use $\widehat{\theta_X-\theta_Y} = \overline{X} - \overline{Y}$. 
% \pause 
% We can show that 
% \[ 
% \overline{X} - \overline{Y} \sim N\left(\theta_X-\theta_Y, \frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y} \right)
% \]
% \pause
% and thus 
% \[ 
% \overline{x} - \overline{y} \pm z_{a/2} \sqrt{\frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y}}
% \]
% is a $100(1-a)$\% confidence interval for $\theta_X-\theta_Y$. 
% \end{frame}



% \begin{frame}
% \frametitle{Sample size}
% 
% Suppose we believe $\Delta = \theta_X-\theta_Y$ and we are interested in how 
% many observations we need so that a $100(1-a)$\% confidence interval 
% 
% \end{frame}

\end{document}






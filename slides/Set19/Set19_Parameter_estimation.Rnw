\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\newtheorem{principle}[theorem]{Principle}

\title{Set19 - Parameter estimation (MOM and MLE)}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=TRUE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE, echo=FALSE>>=
library(dplyr)
library(ggplot2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@


\begin{frame}
\maketitle
\end{frame}


\section{Statistical inference}
\begin{frame}
\frametitle{Statistical inference}

Statistical inference involves: 

\vspace{0.1in} \pause
\begin{itemize}
\item parameter estimation, \pause
\item hypothesis testing, \pause and
\item predicting/forecasting.
\end{itemize}

\vspace{0.1in} \pause

We will start with parameter estimation.

\end{frame}



\begin{frame}
\frametitle{Probability models}

The basic approach to parameter estimation involves:

\vspace{0.1in} \pause

\begin{enumerate}
\item assuming a probability model for data, 
\item collecting the data, and 
\item inferring the parameters of the model from the data
\end{enumerate}

\end{frame}


\subsection{Die rolling example}
\begin{frame}
\frametitle{Rolling a die}

Suppose we roll a 6-sided die with values $\{1,2,3,4,5,6\}$ and will record
the number of rolls of 1 or 2. 

\vspace{0.1in} \pause

Let 
\[ 
X_i = \left\{ \begin{array}{ll}
1 & \mbox{if roll is a 1 or 2} \\
0 & \mbox{otherwise}
\end{array} \right.
\]
\pause
Then a reasonable probability model is 
\[ 
X_i \ind Ber(\theta).
\]
\pause
Based on a sample of $n$ rolls, a reasonable estimate for $\theta$ \pause is 
\[ 
\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i
\]
\pause
i.e. the proportion of 1s and 2s. 

\end{frame}



\subsection{U.S. male heights example}
\begin{frame}
\frametitle{U.S. Male Heights}

Suppose we are interested in the distribution of U.S. male heights \pause
and are able to obtain a random sample of U.S. male's and record their heights.

\vspace{0.1in} \pause

Let $X_i$ be the height in (meters) for U.S. male $i$.
\pause
Then a reasonable probability model is 
\[ 
X_i \ind N(\mu,\sigma^2)
\]
\pause
Based on a sample of $n$ rolls, a reasonable estimate for the parameters \pause is 
\[ 
\hat{\mu} = \overline{x} \quad \mbox{and} \quad \hat{\sigma}^2 = s^2
\]
\pause
i.e. the sample mean and sample variance.

\end{frame}


\begin{frame}
\frametitle{General methodology}

What if the data are from any other distribution, e.g. geometric, exponential,
gamma, uniform, etc? \pause 
Can we describe a general methodology that will allow us to estimate the 
parameter(s) in any model?

\vspace{0.1in} \pause

Here are some possibilites:
\begin{itemize}
\item Method of moments
\item Maximum likelihood
\item Least squares
\item Bayesian 
\end{itemize}

\vspace{0.1in} \pause

\begin{definition}
An \alert{estimator} is a rule to obtain an \alert{estimate} from a sample.
\pause
Thus an \alert{estimator} is a function of data whereas an \alert{estimate} is
a numeric quantity. 
\end{definition}

\end{frame}







\section{Method of moments}
\begin{frame}
\frametitle{Moments}

\begin{definition}
The \alert{$k^{th}$ population moment} is defined as
\[ 
\mu_k = E[X^k].
\]
\pause
The \alert{$k^{th}$ sample moment} is defined as 
\[ 
m_k = \frac{1}{n}\sum_{i=1}^n X_i^k
\]
\pause
and estimates $\mu_k$ based on a sample $(X_1,\ldots,X_n)$. \pause
The first sample moment is the sample mean, i.e. $m_1 = \overline{X}$. 
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Central Moments}
\begin{definition}
For $k\ge 2$, the \alert{$k^{th}$ population central moment} is defined as
\[ 
\mu_k' = E[(X-\mu_1)^k]
\]
\pause
The \alert{$k^{th}$ sample moment} is defined as 
\[ 
m_k' = \frac{1}{n}\sum_{i=1}^n (X_i-m_1)^k = \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})^k
\]
\pause
and estimates $\mu_k'$ based on a sample $(X_1,\ldots,X_n)$. 
\pause
The second population central moment is the variance, i.e. $\mu_2' = Var[X]$.
\pause
The second sample central moment is the alternative sample variance, i.e. 
\[ m_2' = \frac{1}{n} \sum_{i=1}^n (X_i-\overline{X})^2 = \frac{n-1}{n} s^2. \] 
\end{definition}
\end{frame}



\begin{frame}
\frametitle{Method of moments}

\begin{definition}
The \alert{method of moments (MOM) estimator} for a $k$ parameter probability 
model equates the first $k$ population and sample moments\footnote{sometimes 
more than $k$ equations will be needed}, i.e. 
\[ \begin{array}{rl}
\mu_1 &= m_1 \\
\mu_2 &= m_2 \\
&\vdots \\
\mu_k &= m_k
\end{array} \]
and solves for the $k$ parameters. \pause
Alternatively, the method of moments estimator could use the central moments.
\end{definition}

\pause

Typically method of moments estimators are consistent, but sometimes they are 
biased.
\end{frame}



\subsection{Die rolling example}
\begin{frame}
\frametitle{Die rolling example}

Let 
\[ 
X_i = \left\{ \begin{array}{ll}
1 & \mbox{if roll is a 1 or 2} \\
0 & \mbox{otherwise}
\end{array} \right.
\]
\pause
with 
\[ 
X_i \ind Ber(\theta).
\]

\vspace{0.1in} 

Then equating the first population and sample moments gives us 
\[ \mu_1 = E[X_i] = \theta \quad \mbox{and} \quad m_1 = \overline{x}. \]
\pause
Thus, the method of moments estimator is 
\[ 
\hat{\theta} = \overline{x}.
\]
\pause
i.e. the proportion of 1s and 2s. 
\end{frame}



\subsection{U.S. male heights example}
\begin{frame}
\frametitle{U.S. Male Heights}

Let $X_i$ be the height in (meters) for U.S. male $i$ \pause and
\[ 
X_i \ind N(\mu,\sigma^2).
\]
\pause
Then equating the first two ``central'' moments gives us 
\[ \begin{array}{rl}
\mu_1 &= m_1 = \overline{x} \\
\sigma^2 = \mu_2' &= m_2' = \frac{n-1}{n} s^2.
\end{array} \]
\pause
Thus the method of moments estimator is 
\[ 
\hat{\mu} = \overline{x} \quad \mbox{and} \quad \hat{\sigma}^2 = \frac{n-1}{n} s^2.
\]
\pause
\end{frame}



\subsection{German tank problem example}
\begin{frame}
\frametitle{German tank problem}
\small

The German tank problem involves estimating the number of German tanks given
the serial numbers of observed tanks. 
\pause 
Germany used sequential serial numbers and therefore the goal is to estimate
the highest existing serial number based on the observed serial numbers. 
\pause
Let $X_i$ be the serial number for tank $i$, then we assume the serial numbers
are independent and 
\[
P(X_i=x) = \frac{1}{\theta}
\]
were $\theta$ is the maximum serial number. \pause
We can show that 
\[ 
E[X_i] = \frac{\theta+1}{2}
\]
\pause
and thus the method of moments estimator is 
\[ 
\hat{\theta} = 2\overline{X}-1.
\]
\pause
But if we observe the serial numbers 1, 2, and 50, \pause the method of moments 
estimate for $\theta$ is 35 \pause but we know $\theta$ must be at least 50. 

\end{frame}



\section{Maximum likelihood}
\begin{frame}
\frametitle{Maximum likelihood estimation}
\small
\begin{definition}
The \alert{likelihood} is the probability mass or density function when viewed 
as a function of the parameter(s) for a given set of data. 
The \alert{maximum likelihood estimator (MLE)} is the parameter value that maximizes
the likelihood. 
\end{definition}

\vspace{0.1in} \pause

Typically, we assume $X_i\ind p(x_i|\theta)$ for some parameter $\theta$. \pause
Then the likelihood is 
\[ 
L(\theta) = \prod_{i=1}^n p(x_i|\theta)
\]
\pause
but it is typically easier to maximize the log-likelihood, i.e. 
\[ 
\ell(\theta) = \log L(\theta) \pause = \sum_{i=1}^n \log p(x_i|\theta)
\]
\pause 
which will result in the same MLE because $\log$, the natural logarithm, is a 
monotonic function.
\end{frame}


\subsection{Die rolling example}
\begin{frame}
\frametitle{Die rolling example}

Let 
\[ 
X_i = \left\{ \begin{array}{ll}
1 & \mbox{if roll is a 1 or 2} \\
0 & \mbox{otherwise}
\end{array} \right.
\quad
\mbox{and} \quad
X_i \ind Ber(\theta).
\]

\pause

The likelihood is 
\[ 
L(\theta) = \prod_{i=1}^n p(x_i|\theta) 
\pause = \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} 
\pause = \theta^{n\overline{x}}(1-\theta)^{n(1-\overline{x})} 
\]
and the log-likelihood is 
\[ 
\ell(\theta) = n\overline{x} \log(\theta) + n(1-\overline{x}) \log(1-\theta).
\]
To find the MLE, we take the derivative, set it equal to zero, and solve for 
$\theta$, i.e. 
\[ 
\frac{d}{d\theta} \ell(\theta) 
= \frac{n\overline{x}}{\theta} - \frac{n(1-\overline{x})}{1-\theta} \implies
\hat{\theta}_{MLE} = \overline{x}.
\]
\end{frame}


\begin{frame}[fragile]
\frametitle{Visualing the Bernoulli likelihood}
<<die_rolling_data>>=
x = rbinom(10,1,.3); x
@
<<die_rolling_likelihood, dependson="die_rolling_data", echo=FALSE>>=
like = Vectorize(function(theta) prod(dbinom(x,1,theta)))
curve(like, ylab=expression(L(theta)), xlab=expression(theta))
abline(v=mean(x), col="red")
@
\end{frame}




\subsection{U.S. male heights example}
\begin{frame}
\frametitle{U.S. Male Heights}

Let $X_i$ be the height in (meters) for U.S. male $i$ \pause and
\[ 
X_i \ind N(\mu,\sigma^2).
\]
The log-likelihood is 
\[ 
\ell(\mu,\sigma^2) = \sum_{i=1}^n \log p(x_i|\theta) 
= -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2.
\]
Taking partial derivatives with respect to $\mu$ and $\sigma^2$ and setting 
equal to zero, we have 
\[ \begin{array}{rl}
0 &= \sum_{i=1}^n (x_i-\mu)^2 \\
0 &= -n + \frac{1}{\sigma^2} \sum_{i=1}^n (x_i-\mu)^2.
\end{array} \]
\pause
Solving for $\mu$ in the first equation and plugging the result in to the 
second equation results in 
\[ 
\hat{\mu}_{MLE} = \overline{x} 
\quad \mbox{and} \quad
\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i-\overline{x})^2.
\]
\end{frame}


\begin{frame}[fragile]
\frametitle{Visualing the normal likelihood}
<<height_data>>=
x = rnorm(10); mle = c(mean(x), (length(x)-1)/length(x)*var(x)); mle
@

<<height_likelihood, dependson="height_data", echo=FALSE>>=
n = 101
mu = seq(-2,2,length=n)
sigma = seq(0,3,length=n)
log_like = Vectorize(function(mu,sigma2) sum(dnorm(x,mu,sqrt(sigma2),log=TRUE)))
z = outer(mu, sigma, FUN=log_like)
contour(mu,sigma,exp(z), xlab=expression(mu), ylab=expression(sigma^2), 
				main = "Normal likelihood",
				drawlabels = FALSE)
points(mle[1], mle[2], pch='x', col="red")
@
\end{frame}



\subsection{German tank problem example}
\begin{frame}
\frametitle{German tank problem}
\small

Let $X_i$ be the serial number for tank $i$, then we assume the serial numbers
are independent and 
\[
P(X_i=x) = \frac{1}{\theta}
\]
were $\theta$ is the maximum serial number. \pause
The likelihood is 
\[ 
L(\theta) = \prod_{i=1}^n \frac{1}{\theta} \mathrm{I}(x_i\le \theta) 
= \frac{1}{\theta^n} \mathrm{I}(x_{max}\le \theta)
\]
\pause
where $\mathrm{I}(\cdot)$ is an indicator function that is 1 if the logical 
statement is true and 0 otherwise \pause and $x_{max}$ is the maximum of the 
$x_i$. \pause
This is maximized when $\theta$ is $x_{max}$ and thus 
\[ 
\hat{\theta}_{MLE} = x_{max}.
\]
\pause
But this is also weird, because it seems more likely that the true value for 
$\theta$ is somewhat larger than $x_{max}$, i.e. what is the probability that 
the largest serial number is in the set of serial numbers observed?
\end{frame}


\begin{frame}[fragile]
\frametitle{Visualizing discrete uniform likelihood}
<<german_tank_data>>=
x = sample(100, 10); maxx = max(x); maxx
@

<<german_tank_likelihood, echo=FALSE>>=
like = function(theta) theta^-length(x) * (maxx <= theta)
curve(like, 80, 120, n=121, ylab=expression(L(theta)), xlab=expression(theta))
abline(v=maxx, col='red')
@
\end{frame}



\end{document}






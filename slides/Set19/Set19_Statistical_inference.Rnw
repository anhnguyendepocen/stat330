\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\newtheorem{principle}[theorem]{Principle}

\title{Set17 - Statistical inference}

\begin{document}

\begin{frame}
\maketitle
\end{frame}


\section{Statistical inference}
\begin{frame}
\frametitle{Statistical inference}

Statistical inference involves: 

\vspace{0.1in} \pause
\begin{itemize}
\item parameter estimation, \pause
\item hypothesis testing, \pause and
\item predicting/forecasting.
\end{itemize}

\vspace{0.1in} \pause

We will start with parameter estimation.

\end{frame}



\begin{frame}
\frametitle{Probability models}

The basic approach to parameter estimation involves:

\vspace{0.1in} \pause

\begin{enumerate}
\item Assuming a probability model for data, 
\item Collecting the data, and 
\item Inferring the parameters in the model from the data obtained.
\end{enumerate}

\end{frame}


\subsection{Die rolling example}
\begin{frame}
\frametitle{Rolling a die}

Suppose we roll a 6-sided die with values $\{1,2,3,4,5,6\}$ and will record
the number of rolls of 1 or 2. 

\vspace{0.1in} \pause

Let 
\[ 
X_i = \left\{ \begin{array}{ll}
1 & \mbox{if roll is a 1 or 2} \\
0 & \mbox{otherwise}
\end{array} \right.
\]
\pause
Then a reasonable probability model is 
\[ 
X_i \stackrel{ind}{\sim} Ber(\theta).
\]
\pause
Based on a sample of $n$ rolls, a reasonable estimate for $\theta$ \pause is 
\[ 
\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i
\]
\pause
i.e. the proportion of 1s and 2s. 

\end{frame}



\subsection{U.S. male heights example}
\begin{frame}
\frametitle{U.S. Male Heights}

Suppose we are interested in the distribution of U.S. male heights \pause
and are able to obtain a random sample of U.S. male's and record their heights.

\vspace{0.1in} \pause

Let $H_i$ be the height in (meters) for U.S. male $i$.
\pause
Then a reasonable probability model is 
\[ 
X_i \stackrel{ind}{\sim} N(\mu,\sigma^2)
\]
\pause
Based on a sample of $n$ rolls, a reasonable estimate for the parameters \pause is 
\[ 
\hat{\mu} = \overline{x} \quad \mbox{and} \quad \hat{\sigma}^2 = s^2
\]
\pause
i.e. the sample mean and sample variance.

\end{frame}


\begin{frame}
\frametitle{General methodology}

What if the data are from any other distribution, e.g. geometric, exponential,
gamma, uniform, etc? \pause 
Can we describe a general methodology that will allow us to estimate the 
parameter(s) in any model?

\vspace{0.1in} \pause

Here are some possibilites:
\begin{itemize}
\item Method of moments
\item Maximum likelihood
\item Least squares
\item Bayesian 
\end{itemize}

\vspace{0.1in} \pause

\begin{definition}
An \alert{estimator} is a rule to obtain an \alert{estimate} from a sample.
\pause
Thus an \alert{estimator} is a function of data whereas an \alert{estimate} is
a numeric quantity. 
\end{definition}

\end{frame}







\section{Method of moments}
\begin{frame}
\frametitle{Moments}

\begin{definition}
The \alert{$k^{th}$ population moment} is defined as
\[ 
\mu_k = E[X^k].
\]
\pause
The \alert{$k^{th}$ sample moment} is defined as 
\[ 
m_k = \frac{1}{n}\sum_{i=1}^n X_i^k
\]
\pause
and estimates $\mu_k$ based on a sample $(X_1,\ldots,X_n)$. \pause
The first sample moment is the sample mean, i.e. $m_1 = \overline{X}$. 
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Central Moments}
\begin{definition}
For $k\ge 2$, the \alert{$k^{th}$ population central moment} is defined as
\[ 
\mu_k' = E[(X-\mu_1)^k]
\]
\pause
The \alert{$k^{th}$ sample moment} is defined as 
\[ 
m_k' = \frac{1}{n}\sum_{i=1}^n (X_i-m_1)^k = \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})^k
\]
\pause
and estimates $\mu_k'$ based on a sample $(X_1,\ldots,X_n)$. 
\pause
The second population central moment is the variance, i.e. $\mu_2' = Var[X]$.
\pause
The second sample central moment is the alternative sample variance, i.e. 
\[ m_2' = \frac{1}{n} \sum_{i=1}^n (X_i-\overline{X})^2 = \frac{n-1}{n} s^2. \] 
\end{definition}
\end{frame}



\begin{frame}
\frametitle{Method of moments}

\begin{definition}
The \alert{method of moments (MOM) estimator} for a $k$ parameter probability 
model equates the first $k$ population and sample moments\footnote{sometimes 
more than $k$ equations will be needed}, i.e. 
\[ \begin{array}{rl}
\mu_1 &= m_1 \\
\mu_2 &= m_2 \\
&\vdots \\
\mu_k &= m_k
\end{array} \]
and solves for the $k$ parameters. \pause
Alternatively, the method of moments estimator could use the central moments.
\end{definition}

\pause

Typically method of moments estimators are consistent, but sometimes they are 
biased.
\end{frame}



\subsection{Die rolling example}
\begin{frame}
\frametitle{Die rolling example}

Let 
\[ 
X_i = \left\{ \begin{array}{ll}
1 & \mbox{if roll is a 1 or 2} \\
0 & \mbox{otherwise}
\end{array} \right.
\]
\pause
with 
\[ 
X_i \stackrel{ind}{\sim} Ber(\theta).
\]

\vspace{0.1in} 

Then equating the first population and sample moments gives us 
\[ \mu_1 = E[X_i] = \theta \quad \mbox{and} \quad m_1 = \overline{x}. \]
\pause
Thus, the method of moments estimator is 
\[ 
\hat{\theta} = \overline{x}.
\]
\pause
i.e. the proportion of 1s and 2s. 
\end{frame}



\subsection{U.S. male heights example}
\begin{frame}
\frametitle{U.S. Male Heights}

Let $H_i$ be the height in (meters) for U.S. male $i$ \pause and
\[ 
X_i \stackrel{ind}{\sim} N(\mu,\sigma^2).
\]
\pause
Then equating the first two ``central'' moments gives us 
\[ \begin{array}{rl}
\mu_1 &= m_1 = \overline{x} \\
\sigma^2 = \mu_2' &= m_2' = s^2.
\end{array} \]
\pause
Thus the method of moments estimator is 
\[ 
\hat{\mu} = \overline{x} \quad \mbox{and} \quad \hat{\sigma}^2 = s^2
\]
\pause
i.e. the sample mean and sample variance.
\end{frame}



\subsection{German tank problem example}
\begin{frame}
\frametitle{German tank problem}
\small

The German tank problem involves estimating the number of German tanks given
the serial numbers of observed tanks. 
\pause 
Germany used sequential serial numbers and therefore the goal is to estimate
the highest existing serial number based on the observed serial numbers. 
\pause
Let $X_i$ be the serial number for tank $i$, then we assume the serial numbers
are independent and 
\[
P(X_i=x) = \frac{1}{\theta}
\]
were $\theta$ is the maximum serial number. \pause
We can show that 
\[ 
E[X_i] = \frac{\theta+1}{2}
\]
\pause
and thus the method of moments estimator is 
\[ 
\hat{\theta} = 2\overline{X}-1.
\]
\pause
But if we observe the serial numbers 1, 2, and 50, \pause the method of moments 
estimate for $\theta$ is 35 \pause but we know $\theta$ must be at least 50. 

\end{frame}





\end{document}






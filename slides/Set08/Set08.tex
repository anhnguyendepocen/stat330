\documentclass[20pt,landscape]{foils}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{amstext}
\usepackage{amsgen}
\usepackage{amsxtra}
\usepackage{amsgen}
\usepackage{amsthm}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{pause}
\usepackage{graphicx}
\usepackage{epsfig}
%\usepackage{geometry}
%\geometry{headsep=3ex,hscale=0.9}
\newcommand{\bd}{\textbf}
\newcommand{\no}{\noindent}
\newcommand{\un}{\underline}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand \h {\hspace*{.3in}}
\newcommand{\bul}{\hspace*{.3in}{\textcolor{red}{$\bullet$ \ }}}
\newcommand{\xbar}{\bar{x}}
\rightheader{Stat 330 (Fall 2016): slide set 8}

\begin{document}
\LogoOff

\foilhead[1.3in]{}
\centerline{\LARGE \textcolor{blue}{Slide set 8}}
\vspace{0.3in}
\centerline{\large Stat 330 (Fall 2016)}
\vspace{0.2in}
\centerline{\tiny Last update: \today}
\setcounter{page}{0}

\foilhead[-.7in]{\textcolor{blue}{Statistics of R.V.s}}
\no  {\textcolor{magenta}{Expectation}  {\textcolor{cyan}{The {\it expected value} of a function $h(X)$ is defined as}}\\[.2in]
\hspace*{2in} $E[h(X)] := \sum_{i} h(x_{i}) \cdot p_{X}(x_{i}).$\\[.2in]
\no {\textcolor{cyan}{The most important version of this is the case $h(x)=x$:}\\[.2in]
\hspace*{2in} $E[X] = \sum_{i}x_{i} \cdot p_{X}(x_{i}) =: \mu$\\[.25in]
\no $E[X]$ is usually denoted by the symbol $\mu$.\\[.15in]
\no The expected value of a random variable  $E[X]$  is a measure of the average value of the possible values of the random variable. \\[.15in]
\no We see that it is actually a \emph{weighted average} of the values of $X$, weighted by the point masses $p_{X}(x_i)$'s.

\foilhead[-.8in]{\textcolor{blue}{Example}}
\no  {\textcolor{magenta}{Toss a Die}\\[.1in]
\no  {\textcolor{cyan}{Toss a fair die, and denote by $X$ the number of spots on the
upturned face.}\\[.1in]
\no What is the expected value for $X$?\\[.1in]
\no The probability mass function of $X$ is $p_{X}(i) = \frac{1}{6}$ for
all $i \in \{1,2,3,4,5,6\}$.\\[.1in]
\no Therefore, using the definition
\[
E(X) = \sum_{i=1}^{6} i p_{X}(i) = 1 \cdot \frac{1}{6} + 2 \cdot
\frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot
\frac{1}{6} + 6 \cdot \frac{1}{6} = 3.5.
\]
\no This formula shows that $E(X)$ is also the  {\textcolor{cyan}{center of gravity}} of masses $p_{X}$  placed at corresponding points $x$.
  
\foilhead[-.7in]{\textcolor{blue}{Statistics of R.V.s}}
\no  {\textcolor{magenta}{Variance}  {\textcolor{cyan}{ A second common measure for describing a random variable is a measure of how far its values are spread out.}\\[.2in]
\no The {\textcolor{magenta}{variance}} of a random variable $X$ is defined as:\\[.2in]
\hspace*{2in} $Var(X) := E[\{X-E(X)\}^{2}] = \sum_{i}\{x_{i} - E(X)\}^{2} \cdot p_{X}(x_{i})$\\[.2in]
\no $ Var(X)$ is usually denoted by the symbol $\sigma^2$\\[.2in]
\no The variance is measured in squared units of $X$.\\[.2in]
\no $\sigma := \sqrt{Var(X)}$ is called the {\textcolor{magenta}{standard deviation}}
    of $X$, its units are the original units of the values of $X$.


\foilhead[-.8in]{\textcolor{blue}{Example: Toss a die (continued...)}}
\no  {\textcolor{cyan}{Toss a fair die, and denote with $X$ the number of spots on the
upturned face.}}\\[.1in]
\no What is the variance for $X$?\\[.1in]
\no Looking at the above definition for $Var(X)$, we see that we need to
know the probability mass function and the value of $E(X)$ for this computation.\\[.1in]
\no {\textcolor{magenta}{Recall:}} {\textcolor{cyan}{The probability mass function of $X$ is $p_{X}(i) = \frac{1}{6}$ for
all $i \in \{1,2,3,4,5,6\}$; and  $E(X) = 3.5$. Therefore,}}\\[.15in]
\hspace*{.5in} $Var(X) = \sum_{i=1}^{6} (x_{i} - 3.5)^{2} p_{X}(i)$\\[.1in]
\hspace*{1in}  $  =  6.25 \cdot \frac{1}{6} +2.25 \cdot\frac{1}{6} + 0.25 \cdot \frac{1}{6} + 0.25 \cdot \frac{1}{6} + 2.25 \cdot \frac{1}{6} + 6.25 \cdot \frac{1}{6} $\\[.1in]
\hspace*{1in} $= 2.917 \text{ \  (spots}^{2}).$\\[.1in]
\no The standard deviation for $X$ is:\\[.1in]
\hspace*{.5in} $\sigma = \sqrt{Var(X)} = 1.71 \text{ \  (spots)}.$


\foilhead[-.8in]{\textcolor{blue}{Example: Toss the doctored die}}
\no Recall the example of the {\textcolor{cyan}{doctored die}} we talked about earlier.\\[.1in]
\no In that example, $Z$ denoted the number of spots on the upturned face after toss of the die, the  {\textcolor{magenta}{pmf}} of $Z$ was shown to be
    \[
	    \begin{array}{l||c|c|c|c}
		z & 1 & 2 & 3 & 4 \\ \hline
		p(z) & 1/6 & 1/3 & 1/3 & 1/6 
	    \end{array}
	    \]	    
\no Calculate $E(Z)$ and $Var(Z)$?\\[.25in]
\no $E(Z)=1 \cdot \frac{1}{6}+ 2 \cdot \frac{1}{3} + 3 \cdot \frac{1}{3}  + 4 \cdot \frac{1}{6} =\frac{15}{6}=2.5$\\[.25in]
\no $Var(Z)=(1-2.5)^2 \cdot \frac{1}{6}+ (2-2.5)^2 \cdot \frac{1}{3} + (3-2.5)^2 \cdot \frac{1}{3}  + (4-2.5)^2 \cdot \frac{1}{6} =0.9167$

\foilhead[-.8in]{\textcolor{blue}{Some properties of $E(X)$ and $Var(X)$}}\vspace{.2cm}
\no  {\textcolor{magenta}{Theorem}   For two random variables $X$ and $Y$ and two real numbers $a,b$
    following holds:
    \[
    E(aX + bY) = a E(X) + b E(Y).
    \]}\\[.1in]
\no \no {\textcolor{magenta}{Theorem}}
  For a random variable $X$ and a real number $a$, following hold:
    \begin{itemize}
	\item[(i)] $ Var(X) =E(X^{2}) - (E(X))^{2}$
	\item[(ii)] $Var(aX) = a^{2} Var(X)$
    \end{itemize}
    
 \no \no {\textcolor{magenta}{Chebyshev's Inequality}}  
    For any positive real number $k$, and random variable $X$ with
    variance $\sigma^{2}$:
    \[
    P( | X - E(X) | \le k \sigma) \ge 1 - \frac{1}{k^{2}}
    \]
    
  \foilhead[-.7in]{\textcolor{blue}{Cumulative Distribution Function}}
\no  {\textcolor{magenta}{Motivation:}  Very often we are interested in the probability of a whole range of
values, like $P(X \le 5)$ or $P(4 \le X \le 16)$.  }

\no Assume $X$ is a discrete random variable:

\no The function $F_{X}(t) := P(X \le t)$ is called {\textcolor{magenta}{the 
cumulative distribution function or the cdf}} of $X$.\\[.1in]
\no {\textcolor{cyan}{(Note that in Hofmann's notes a different terminology is used; she calls it the  probability distribution function. But we will use the terminology used in Baron's book)}}\\[.1in]
\no  {\textcolor{red}{What is the relationship between cdf and pmf, $p_X$ and $F_X$?}}\\[.1in]
\no Since $X$ is a discrete random variable, the image of $X$ can be
written as $\{ x_{1}, x_{2}, x_{3}, \ldots \}$, we are therefore
interested in all $x_{i}$ with $x_{i} \le t$:\\[.15in]
\hspace*{1in}$F_X(t) = P(X \le t) = P( \{ x_i | x_i \le t\}) = \sum_{i,\text{with } x_i \le t}p_{X}(x_i).$

 
 
  \foilhead[-.8in]{\textcolor{blue}{Example of a cdf}}
\no  {\textcolor{magenta}{Roll a fair die:} }
\no   $X$ = number of spots on upturned face \\[.1in]
\no   Sample Space is $\Omega = \{1,2,3,4,5,6\}$ \\[.1in]
\no The {\textcolor{magenta}{probability mass function}} for $X$ therefore is \\[.1in] 
\hspace*{2in}\begin{tabular}{c|cccccc}
		$x$ & 1 & 2 & 3 & 4 & 5& 6 \\
		\hline 
		$p_{X}(x)$ & $\frac{1}{6}$  & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $ \frac{1}{6} $ \\[.1in]
\end{tabular}

\no What is the {\textcolor{magenta}{cumulative distribution function}}?\\[.1in]
\no  {\textcolor{magenta}{Solution:} }   $F_X(t) = \sum_{i \le t}p_{X}(i) = \sum_{i =1}^{\lfloor{t}\rfloor}p_{X}(i) =
    \frac{\lfloor{t}\rfloor}{6}$, where $\lfloor{t}\rfloor$ is the truncated value of
    $t$.\\[.2in]
 \hspace*{1in}\begin{tabular}{c|ccccccc}
		$t$ & $(-\infty,1)$ & $ [1,2)$ & $[2,3)$ & $[3,4)$ & $[4,5)$ & $[5,6)$ & $[6,\infty)$\\
		\hline 
		$F_X(t)$ & 0  &$\frac{1}{6}$  & $\frac{2}{6}$ & $\frac{3}{6}$ & $\frac{4}{6}$ & $\frac{5}{6}$ & $ 1$ \\[.1in]
\end{tabular}   
\no {\textcolor{cyan}{See Example 3.1, 3.2 in Baron and Figure 3.1 for another example.}}
  \foilhead[-.76in]{\textcolor{blue}{Example of a cdf (continued...)}}  
\no  {\textcolor{magenta}{Plot of the cdf for the fair die tossing problem:} } \\[.01in]  
\hspace*{1.5in}\includegraphics*[scale=.6]{cdf_die1.pdf}


\foilhead[-.78in]{\textcolor{blue}{Plots of pmf and cdf for the doctored die example}}  
\no Let $Z$ denote the number of spots on the upturned face after toss of the doctored die. The  
{\textcolor{magenta}{pmf}} of $Z$ was shown to be \\[-.3in]
    \[
	    \begin{array}{l||c|c|c|c}
		z & 1 & 2 & 3 & 4 \\ \hline
		p(z) & 1/6 & 1/3 & 1/3 & 1/6 
	    \end{array}
	    \]	    
\hspace*{2in}\includegraphics*[scale=.6]{cdf_die2.pdf}
\foilhead[-.78in]{\textcolor{blue}{Example of computing CDFs}} 
\no Let $X$ be a random variable with the pmf:
  \[
	    \begin{array}{l||c|c|c|c}
		x & 0 & 1 & 2 & 3 \\ \hline
		p(x) & 1/8 & 1/2 & 1/4 & 1/8 
	    \end{array}
	    \]	    
\no  Evaluate $F_X(0.5),F_X(1), F_X(3)$\\[.25in]
\no  $F_X(0.5)=P(X\le0.5)=P(X=0)=\frac{1}{8}$\\[.2in]
\no  $F_X(1)=P(X\le 1)=P(X=0\ \text{or}\ X=1)=P(X=0)+P(X=1)$\\[.1in]
\no \h \h \h $=\frac{1}{8}+\frac{1}{2}=\frac{5}{8}$\\[.2in]
\no  $F_X(3)=P(X\le 3)=P(X=0)+P(X=1)+P(X=2)+P(X=3)$\\[.1in]
\no \h \h \h $=1.0$
 
\foilhead[-.8in]{\textcolor{blue}{Properties of $F_X$}}\vspace{.2cm}
%\no  {$F_X(x)=P(X\leq x)$ is cumulative distribution function, cdf }\\[.2in]
\no  \no {\textcolor{red}{Properties of $F_{X}$:} }   
The following properties hold for the cdf
$F_{X}$ of a  random variable $X$.
 \begin{itemize}
    \item $ 0 \le F_{X}(t) \le 1$ for all $t \in \mathbb{R}$
    \item $F_{X}$ is nondecreasing, (i.e. if $x_{1} \le x_{2}$
    then $F_{X}(x_{1}) \le F_{X}(x_{2})$.)
    \item $\lim_{t \rightarrow -\infty}F_{X}(t) = 0$ and $\lim_{t \rightarrow \infty}F_{X}(t) = 1$.
    \item %$F_{X}(t)$ has a positive jump equal to $p_{X}(x_{i})$ at $\{
    %x_{1}, x_{2}, x_{3}, \ldots \}$; $F_{X}$ is constant in the interval $[x_{i},
    %x_{i+1})$, i.e. 
    $F_X(t)$ is right continuous with respect to $t$
\end{itemize}
%\no If $x\leq y$, what how will $F_X(x)$ and $F_Y(y)$ be related? 

\end{document}





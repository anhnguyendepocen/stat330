\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set08 - Continuous random variables}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Continuous random variables}
\subsection{Cumulative distribution function}
\begin{frame}
\frametitle{Cumulative distribution function}

All properties of discrete random variables have direct counterparts for continuous random variables.

\vspace{0.1in} \pause

In particular, 
\begin{definition}
The \alert{cumulative distribution function} for a continuous random variable is 
\[ 
F_X(x) = P(X\le x).
\]
\end{definition}

we still have the properties 

\begin{itemize}
\item $ 0 \le F_{X}(x) \le 1$ for all $x \in \mathbb{R}$
\item $F_{X}$ is monotone increasing, i.e. if $x_{1} \le x_{2}$ then $F_{X}(x_{1}) \le F_{X}(x_{2})$.
\item $\lim_{x \to -\infty}F_{X}(x) = 0$ and $\lim_{x \to \infty}F_{X}(x) = 1$.
\end{itemize}
\pause
\end{frame}




\subsection{Probability density functions}
\begin{frame}
\frametitle{Probability density function}

\pause

\begin{definition}
The \alert{probability density function (pdf)} for a continuous random variable is 
\[ 
f_X(x) = \frac{d}{dx} F_X(x)
\]
\pause
and 
\[ 
F_X(x) = \int_{-\infty}^x f_X(t) dt.
\]
\end{definition}

\vspace{0.1in} \pause

Thus, the probability density function has the following properties
\begin{itemize}
\item $f_X(x) \ge 0$ for all $x$ \pause and
\item $\int_{-\infty}^\infty f(x) dx = 1.$
\end{itemize}
\end{frame}


\subsection{Example}
\begin{frame}
\frametitle{Example}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

$f_X(x)$ defines a valid probability density function because $f_X(x) \ge 0$ for all $x$ \pause and
\[ 
\int_{-\infty}^\infty f_X(x) dx = \int_0^1 3x^2 dx = x^3 |_0^1 = 1.
\]

\pause
The cumulative distribution function is 
\[ 
F_X(x) = \left\{ \begin{array}{ll}
0 & x<0 \\
x^3 & 0\le x \le 1 \\
1 & x>1
\end{array} \right.
\]

\end{frame}


\subsection{Expectation}
\begin{frame}
\frametitle{Expected value}

\begin{definition}
Let $X$ be a continuous random variable and $h$ be some function. 
The \alert{expected value} of a function of a continuous random variable is 
\[ 
E[h(X)] = \int_{-\infty}^\infty h(x) \cdot f_X(x) dx.
\]
\pause
If $h(x)=x$, then 
\[ 
E[X] = \int_{-\infty}^\infty x \cdot f_X(x) dx.
\]
\pause
and we call this the \alert{expectation} of $X$. 
\end{definition}
\end{frame}



\begin{frame}
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

The expected value is 
\[ \begin{array}{rl}
E[X] &= \int_{-\infty}^\infty x \cdot f_X(x) dx \\
&= \int_0^1 3x^3 dx \\
&= 3\frac{x^4}{4} |_0^1 = \frac{3}{4}.
\end{array} \]
\end{frame}


\subsection{Variance}
\begin{frame}
\frametitle{Variance}

\begin{definition}
The \alert{variance} of a random variable is defined as the expected squared deviation from the mean. \pause 
For continuous random variables, variance is
\[
Var[X] = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2 f_X(x) dx
\]
where $\mu = E[X]$. \pause
The symbol $\sigma^2$ is commonly used for the variance.
\end{definition}

\pause

\begin{definition}
The \alert{standard deviation} is the positive square root of the variance
\[
SD[X] = \sqrt{Var[X]}.
\]
The symbol $\sigma$ is commonly used for the standard deviation.
\end{definition}

\end{frame}




\begin{frame}
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

The variance is 
\[ \begin{array}{rl}
Var[X] &= \int_{-\infty}^\infty \left(x-\mu\right)^2 f_X(x) dx \\
&= \int_0^1 \left(x-\frac{3}{4}\right)^2 3x^2 dx \\
&= \int_0^1 \left[x^2-\frac{3}{2}x + \frac{9}{16} \right] 3x^2 dx \\
&= \int_0^1 3x^4-\frac{9}{2}x^3 + \frac{27}{16}x^2 dx \\
&= \left[\frac{3}{5}x^5-\frac{9}{8}x^4 + \frac{9}{16}x^3\right]|_0^1 dx \\
&= \frac{3}{5}-\frac{9}{8}+\frac{9}{16} \\
&= \frac{3}{80}
\end{array} \]
\end{frame}



\begin{frame}[fragile]
\frametitle{Example (cont.)}

The inverse of the cumulative distribution function is 
\[ 
F^{-1}_X(u) = u^{1/3}.
\]
\pause
A uniform random number on the interval (0,1) evaluted with the inverse cdf produces a random draw of $X$. \pause
So, in R

<<echo=TRUE>>=
inverse_cdf = function(u) u^(1/3)
x = inverse_cdf(runif(1e6))
mean(x)
var(x); 3/80
@

\end{frame}

% \foilhead[-.75in]{\textcolor{blue}{Example: pdf}}}\vspace{1mm}
% \no {\textcolor{magenta}{Let $Y$ be the time until the first major failure of a new disk 
%     drive.}}\\[.1in]
% \no A possible density function for $Y$ is
%     \[
%     f(y) = \left \{ 
%     \begin{array}{ll}
% 	e^{-y} & y > 0\\
% 	0 & \text{ otherwise}
%     \end{array} \right .
%     \]
% \no {\textcolor{cyan}{
%     First, we need to check, that $f(y)$ is actually a density 
%     function. Obviously, $f(y)$ is a non-negative function on whole 
%     of $\Re$.}}
%     
% \no {\textcolor{cyan}{ The second condition, $f$ must fulfill to be a density of $Y$ 
%     is }}
%     \[
%     \int_{-\infty}^{\infty} f(y)dy = \int_{0}^{\infty} e^{-y}dy = - 
%     e^{-y} |_{0}^{\infty} = 0 - (-1) = 1
%     \]
%   
%   \foilhead[-.75in]{\textcolor{blue}{Continuing the disk drive example...}}\vspace{1mm}  
% \no    {\textcolor{magenta}{What is the probability that the first major disk drive failure 
%     occurs within the first year?}}\\[.1in]  
%     \[
%     P(Y \le 1) = \int_{0}^{1} e^{-y}dy = -e^{-y} |_{0}^{1} = 1 - 
%     e^{-1} \approx 0.63.
%     \]
%     
% \no {\textcolor{magenta} {What is the cumulative distribution function of $Y$?}}
%    \[
%     F_{Y}(t) = \int_{\infty}^{t} f(y) dy = \int_{0}^{t}e^{-y}dy = 1 - 
%     e^{-t} \text{ for all } t \ge 0.
%     \]
% \foilhead[-.75in]{\textcolor{blue}{Continuing the disk drive example...}}\vspace{1mm}  
% \no    {\textcolor{magenta}{Density and Distribution functions of the random variable $Y$.}\\[.1in]
%   \centerline {\includegraphics[scale=.6]{density.pdf}}
% 
% 
%  \foilhead[-.8in]{\textcolor{blue}{Compare discrete and continuous RVs}}
% %\begin{center}
%     \begin{tabular}{p{4.5in}|p{4in}}
%   {\textcolor{magenta}{discrete random variable}} &  {\textcolor{magenta}{continuous random variable}} \\ \hline
% 	image $Im(X)$ finite or countable infinite &  image $Im(X)$ uncountable 
% 	\\[5pt]
% 	{\textcolor{magenta}{cumulative distribution function:}} \\
% 	$F_{X}(t) = P(X \le t) = \sum_{k \le {t}}p_{X}(k)$ & 
% 	$F_{X}(t) = P(X \le t) = \int_{\infty}^{t} f(x) dx$ \\ [5pt]
% 	 {\textcolor{magenta}{probability mass function:}} &  {\textcolor{magenta}{probability 
% 	density function:}}\\
% 	\centerline{$p_{X}(x) = P(X = x)$} &
% 	\centerline{$f_{X}(x) = F_{X}^{'}(x)$} \\ [5pt]
% 	 {\textcolor{magenta}{expected value:}}\\
% 	$E[h(X)] = \sum_{x} h(x) \cdot p_{X}(x)$ & 	$E[h(X)] = \int_{x} h(x) \cdot 
% 	f_{X}(x)$ \\[5pt]
% 	$E[X] = \sum_{x} x \cdot p_{X}(x)$ & 	$E[X] = \int_{-\infty}^{\infty} x \cdot 
% 	f_{X}(x) dx$ \\[5pt]
%   {\textcolor{magenta}{variance:}} \\
% 	$Var[X] = E[(X-E[X])^{2}] = \sum_{x}(x-E[X])^{2}p_{X}(x)$ & $Var[X] = 
% 	E[(X-E[X])^{2}] = \int_{-\infty}^{\infty}(x-E[X])^{2}f_{X}(x)dx$ \\
%     \end{tabular}
% %\end{center}
% 
% 
%   
%  \foilhead[-.75in]{\textcolor{blue}{Some special continuous density functions}}
% \no 	 {\textcolor{magenta}{Uniform Density}}\\[.1in]
% \no  One of the most basic continuous density is the {\it 
% uniform density}.\\[.1in] 
% \no 	 {\textcolor{magenta}{The pdf is:}\\[-.2in]
% \[
% f(x) = \left \{ 
% \begin{array}{cl}
%     \frac{1}{b-a} & \text{ if } a < x < b \\
%     0 & \text{ otherwise}
% \end{array} \right .
% \]
% \no We say that $X \sim U(a,b)$ i.e., the random variable $X$ is distributed as the \emph{Uniform distribution with parameters $a$ and $b$}\\[.15in]
% %
% \centerline{\includegraphics[scale=0.6]{uniform.pdf}}
% %
% \foilhead[-.75in]{\textcolor{blue}{Properties of the Uniform distribution}}
% \no The cumulative distribution function $F_{X}$ is
% \vspace*{-.3in}
% 
% \[
% U_{a,b}(x) := F_{X}(x) = \left \{ 
% \begin{array}{cl}
%     0 & \text{ if } x \le a \\
%     \frac{x-a}{b-a} & \text{ if } a < x < b \\
%     1 & \text{ if } x \ge b.
% \end{array} \right .
% \]
% \vspace*{-.25in}
% 
% \no We now compute the expected value and variance of a 
%  a uniform distribution on $(a,b)$.
% \begin{eqnarray*}
%     E[X] &=& \int_{a}^{b} x \frac{1}{b-a} dx = \frac{1}{b-a} 
%     \frac{1}{2} x^{2}|_{a}^{b} = \\
%     &=& \frac{b^{2}-a^{2}}{2(b-a)} = 
%     \underline{\frac{1}{2}(a+b)}.  \\
%     Var[X] &=& \int_{a}^{b}(x - \frac{a+b}{2})^{2} \frac{1}{b-a} dx = 
%     \ldots = \underline{\frac{(b-a)^{2}}{12}}.
% \end{eqnarray*}
% \foilhead[-.75in]{\textcolor{blue}{Uniform distribution: Example}}
% \no    {\textcolor{cyan}{The(pseudo) random number generator on my calculator is supposed to create 
% realizations of $U(0,1)$ random variables.}\\[.15in]
% \no Define $U$ as the next random number the calculator produces.\\[.15in]
% \no What is the probability, that the next number is larger than 0.85?\\[.15in]
% \no To answer that, we will compute $P( U \ge 0.85)$.\\[.15in]
% \no We know the density function of $U$: $f_{U}(u) = \frac{1}{1-0} = 1$.\\[.1in]
% \no Therefore
% \[
% P(U \ge 0.85) = \int_{0.85}^{1}1 du = 1 - 0.85 = 0.15.
% \]

\end{document}








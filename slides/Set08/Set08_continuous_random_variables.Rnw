\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Set08 - Continuous random variables}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Continuous random variables}
\subsection{Cumulative distribution function}
\begin{frame}
\frametitle{Cumulative distribution function}

All properties of discrete random variables have direct counterparts for continuous random variables.

\vspace{0.1in} \pause

In particular, 
\begin{definition}
The \alert{cumulative distribution function} for a continuous random variable is 
\[ 
F_X(x) = P(X\le x).
\]
\end{definition}

we still have the properties 

\begin{itemize}
\item $ 0 \le F_{X}(x) \le 1$ for all $x \in \mathbb{R}$
\item $F_{X}$ is monotone increasing, i.e. if $x_{1} \le x_{2}$ then $F_{X}(x_{1}) \le F_{X}(x_{2})$.
\item $\lim_{x \to -\infty}F_{X}(x) = 0$ and $\lim_{x \to \infty}F_{X}(x) = 1$.
\end{itemize}
\pause
\end{frame}




\subsection{Probability density functions}
\begin{frame}
\frametitle{Probability density function}

\pause

\begin{definition}
The \alert{probability density function (pdf)} for a continuous random variable is 
\[ 
f_X(x) = \frac{d}{dx} F_X(x)
\]
\pause
and 
\[ 
F_X(x) = \int_{-\infty}^x f_X(t) dt.
\]
\end{definition}

\vspace{0.1in} \pause

Thus, the probability density function has the following properties
\begin{itemize}
\item $f_X(x) \ge 0$ for all $x$ \pause and
\item $\int_{-\infty}^\infty f(x) dx = 1.$
\end{itemize}
\end{frame}


\subsection{Example}
\begin{frame}
\frametitle{Example}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

$f_X(x)$ defines a valid probability density function because $f_X(x) \ge 0$ for all $x$ \pause and
\[ 
\int_{-\infty}^\infty f_X(x) dx = \int_0^1 3x^2 dx = x^3 |_0^1 = 1.
\]

\pause
The cumulative distribution function is 
\[ 
F_X(x) = \left\{ \begin{array}{ll}
0 & x<0 \\
x^3 & 0\le x \le 1 \\
1 & x>1
\end{array} \right.
\]

\end{frame}


\subsection{Expectation}
\begin{frame}
\frametitle{Expected value}

\begin{definition}
Let $X$ be a continuous random variable and $h$ be some function. 
The \alert{expected value} of a function of a continuous random variable is 
\[ 
E[h(X)] = \int_{-\infty}^\infty h(x) \cdot f_X(x) dx.
\]
\pause
If $h(x)=x$, then 
\[ 
E[X] = \int_{-\infty}^\infty x \cdot f_X(x) dx.
\]
\pause
and we call this the \alert{expectation} of $X$. 
\end{definition}
\end{frame}



\begin{frame}
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

The expected value is 
\[ \begin{array}{rl}
E[X] &= \int_{-\infty}^\infty x \cdot f_X(x) dx \\
&= \int_0^1 3x^3 dx \\
&= 3\frac{x^4}{4} |_0^1 = \frac{3}{4}.
\end{array} \]
\end{frame}


\subsection{Variance}
\begin{frame}
\frametitle{Variance}

\begin{definition}
The \alert{variance} of a random variable is defined as the expected squared deviation from the mean. \pause 
For continuous random variables, variance is
\[
Var[X] = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2 f_X(x) dx
\]
where $\mu = E[X]$. \pause
The symbol $\sigma^2$ is commonly used for the variance.
\end{definition}

\pause

\begin{definition}
The \alert{standard deviation} is the positive square root of the variance
\[
SD[X] = \sqrt{Var[X]}.
\]
The symbol $\sigma$ is commonly used for the standard deviation.
\end{definition}

\end{frame}




\begin{frame}
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

The variance is 
\[ \begin{array}{rl}
Var[X] &= \int_{-\infty}^\infty \left(x-\mu\right)^2 f_X(x) dx \\
&= \int_0^1 \left(x-\frac{3}{4}\right)^2 3x^2 dx \\
&= \int_0^1 \left[x^2-\frac{3}{2}x + \frac{9}{16} \right] 3x^2 dx \\
&= \int_0^1 3x^4-\frac{9}{2}x^3 + \frac{27}{16}x^2 dx \\
&= \left[\frac{3}{5}x^5-\frac{9}{8}x^4 + \frac{9}{16}x^3\right]|_0^1 dx \\
&= \frac{3}{5}-\frac{9}{8}+\frac{9}{16} \\
&= \frac{3}{80}
\end{array} \]
\end{frame}



\begin{frame}[fragile]
\frametitle{Example (cont.)}

The inverse of the cumulative distribution function is 
\[ 
F^{-1}_X(u) = u^{1/3}.
\]
\pause
A uniform random number on the interval (0,1) evaluted with the inverse cdf produces a random draw of $X$. \pause
So, in R

<<echo=TRUE>>=
inverse_cdf = function(u) u^(1/3)
x = inverse_cdf(runif(1e6))
mean(x)
var(x); 3/80
@

\end{frame}


\section{Comparison of discrete and continuous random variables}
\begin{frame}
\frametitle{Comparison of discrete and continuous random variables}

For simplicity here and later, we drop the subscript $X$. \pause


{\scriptsize
\begin{center}
\begin{tabular}{lcc}
& discrete & continuous \pause \\
\hline
image & finite or countable & uncountable \pause \\ \\
pmf &  $p(x) = P(X=x)$ & \pause \\ \\
pdf && $p(x) = f(x) = F'(x)$ \pause \\ \\
cdf & 
\[\begin{array}{rl}F(x) &= P(X\le x) \\
                        &= \sum_{t\le x} p(x)\end{array} \] & 
\[\begin{array}{rl}F(x) &= P(X\le x) \\
                        &= \int_{-\infty}^x p(t) dt\end{array} \]
\pause \\ \\
expected value & $E[h(X)] = \sum_x h(x) p(x)$ & $E[h(X)] = \int_x h(x) p(x)$ \pause \\ \\
expectation & $\mu = E[h(X)] = \sum_x x \, p(x)$ & $\mu = E[h(X)] = \int_x x \, p(x)$ \pause \\ \\
variance & 
\[ \begin{array}{rl} Var[X] &= E[(X-\mu)^2] \\
                            &= \sum_x (x-\mu)^2 p(x)
\end{array} \] & 
\[ \begin{array}{rl}
Var[X] &= E[(X-\mu)^2] \\
       &= \int_x (x-\mu)^2 p(x)dx
\end{array} \] \\ \\
\hline
\end{tabular}
\end{center}
}

\vspace{0.05in}\pause

Note: we replace summations with integrals when using continuous as opposed to discrete random variables



\end{frame}


% \foilhead[-.75in]{\textcolor{blue}{Example: pdf}}}\vspace{1mm}
% \no {\textcolor{magenta}{Let $Y$ be the time until the first major failure of a new disk 
%     drive.}}\\[.1in]
% \no A possible density function for $Y$ is
%     \[
%     f(y) = \left \{ 
%     \begin{array}{ll}
% 	e^{-y} & y > 0\\
% 	0 & \text{ otherwise}
%     \end{array} \right .
%     \]
% \no {\textcolor{cyan}{
%     First, we need to check, that $f(y)$ is actually a density 
%     function. Obviously, $f(y)$ is a non-negative function on whole 
%     of $\Re$.}}
%     
% \no {\textcolor{cyan}{ The second condition, $f$ must fulfill to be a density of $Y$ 
%     is }}
%     \[
%     \int_{-\infty}^{\infty} f(y)dy = \int_{0}^{\infty} e^{-y}dy = - 
%     e^{-y} |_{0}^{\infty} = 0 - (-1) = 1
%     \]
%   
%   \foilhead[-.75in]{\textcolor{blue}{Continuing the disk drive example...}}\vspace{1mm}  
% \no    {\textcolor{magenta}{What is the probability that the first major disk drive failure 
%     occurs within the first year?}}\\[.1in]  
%     \[
%     P(Y \le 1) = \int_{0}^{1} e^{-y}dy = -e^{-y} |_{0}^{1} = 1 - 
%     e^{-1} \approx 0.63.
%     \]
%     
% \no {\textcolor{magenta} {What is the cumulative distribution function of $Y$?}}
%    \[
%     F_{Y}(t) = \int_{\infty}^{t} f(y) dy = \int_{0}^{t}e^{-y}dy = 1 - 
%     e^{-t} \text{ for all } t \ge 0.
%     \]
% \foilhead[-.75in]{\textcolor{blue}{Continuing the disk drive example...}}\vspace{1mm}  
% \no    {\textcolor{magenta}{Density and Distribution functions of the random variable $Y$.}\\[.1in]
%   \centerline {\includegraphics[scale=.6]{density.pdf}}
% 
% 

\end{document}








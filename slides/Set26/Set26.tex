\documentclass[20pt,landscape]{foils}
\usepackage{amsmath,amssymb, amsthm}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{pause}
\usepackage{graphicx}
\usepackage{epsfig}
%\usepackage{geometry}
%\geometry{headsep=3ex,hscale=0.9}
\newcommand{\bd}{\textbf}
\newcommand{\no}{\noindent}
\newcommand{\un}{\underline}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand \h {\hspace*{.3in}}
\newcommand{\bul}{\hspace*{.1in}{\textcolor{red}{$\bullet$ \ }}}
\newcommand{\xbar}{\bar{x}}
\rightheader{Stat 330 (Fall 2015): slide set 26}

\begin{document}
\LogoOff

\foilhead[1.3in]{}
\centerline{\LARGE \textcolor{blue}{Slide set 26}}
\vspace{0.3in}
\centerline{\large Stat 330 (Fall 2015)}
\vspace{0.2in}
\centerline{\tiny Last update: \today}
\setcounter{page}{0}

\foilhead[-.8in]{\textcolor{blue}{Example for MLE: \vspace{.2cm}}}
\no {\textcolor{blue}{Review:} What is MLE? How to find it (5 steps)?\\[.1in]
\no $\clubsuit$ {\textcolor{red}{$\theta$ may be multiple:} $\Theta\subset \mathbb{R}^p$ with $p>1$\\[.1in]
\no  {\textcolor{magenta}{Example:} Let $X_{1}, \ldots, X_{n}$ be i.i.d $N(\mu, \sigma^2)$, both $\mu$ and $\sigma^{2}$ are unknown. \\[.1in]
\no $x_1,\cdots, x_n$ are the data/sample values of $X_1,\cdots, X_n$\\[.1in]
\no $\heartsuit$ {\textcolor{magenta}{What is the pdf of normal random variable}? \\[.1in]
\no $\diamondsuit$ Since we have values from $n$ independent variables, the Likelihood function is a
product of $n$ densities:
\[
L(\mu, \sigma^{2}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}}
e^{-\frac{(x_{i}-\mu)^{2}}{2 \sigma^{2}}} =
(2 \pi \sigma^{2})^{n/2} \cdot e^{- \sum_{i=1}^{n}\frac{(x_{i}-\mu)^{2}}{2 \sigma^{2}}}
\]
\no $\diamondsuit$ Log-Likelihood is:
\[
l(\mu,\sigma^2)=\log L (\mu, \sigma^{2}) = -\frac{n}{2} \ln (2 \pi \sigma^{2}) -
\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}
\]
\no $\diamondsuit$ Since we have now two parameters, $\mu$ and
$\sigma^{2}$, we need to get 2 partial derivatives of  the
log-Likelihood:
\begin{eqnarray*}
    \frac{\partial}{\partial \mu} \log L (\mu, \sigma^{2}) &=& 0 -
    \frac{1}{2 \sigma^{2}} \cdot
    \sum_{i=1}^{n}(x_{i}-\mu) \cdot (-2)
      = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu) \\
    \frac{\partial}{\partial \sigma^{2}} \log L (\mu, \sigma^{2}) &=& -\frac{n}{2}
    \frac{1}{\sigma^{2}} + \frac{1}{2 (\sigma^{2})^{2}} \sum_{i=1}^{n}(x_{i}-\mu)^{2}
\end{eqnarray*}
\no $\diamondsuit$  Need find values for $\mu$ and $\sigma^{2}$, that yield zeros for both derivatives at the same time\\[.1in]
\no $\diamondsuit$ Setting $\frac{d}{d \mu} \log L (\mu, \sigma^{2}) = 0$ gives
\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n}x_{i}=\bar{x},
\]
\no $\diamondsuit$ Plugging this value into the derivative for $\sigma^{2}$ and setting
$\frac{d}{d \sigma^{2}} \log L (\hat{\mu}, \sigma^{2}) = 0$ gives
\[
\hat{\sigma^{2}} = \frac{1}{n} \sum_{i=1}^{n}(x_{i}-\hat{\mu})^{2}
\]
\no $\spadesuit$ Do you find something different? $\hat{\sigma^2}\neq s^2\ (\text{divisor is}\ n-1\ \text{not}\ n)$! {\textcolor{magenta}{the MLE is biased}!\\[.1in]
\no $\spadesuit$ However, bias does not ruin MLE's because of other nice features like small MSE etc. 

\foilhead[-.8in]{\textcolor{blue}{Topic 2: Confidence intervals:}}
\no {\textcolor{magenta}{Motivations:} The last lectures have provide a way to compute point estimate for parameters. Based on that, it is natural to ask "how good is this point estimate?" Or "how close is the estimate to the true value of
the parameter?"\\[.1in]
\no {\textcolor{magenta}{Further thoughts:}
Instead of just looking at the point estimate, we will now try to compute an interval around the estimated parameter value, in which the true parameter is "likely" to fall.
An interval like that is called confidence interval.\\[.1in]
\no {\textcolor{magenta}{Definition:} An interval $(L,U)$ is an $(1-\alpha)\cdot 100\%$ confidence interval for the parameter $\theta$ if it contains the parameter with probability $(1-\alpha)$
$$P(L< \theta< U)=1-\alpha.$$ The coverage probability $1-\alpha$ is called a {\textcolor{magenta}{confidence level}.\\[.1in]
\no {\textcolor{magenta}{\underline{Equivalent definition}:} Let $\hat{\theta}$ be an estimate of $\theta$. If $$P( | \hat{\theta} - \theta | < e) \geq 1-\alpha$$ we say, that the
    interval $(\hat{\theta} - e, \hat{\theta} + e)$ is an $ (1-\alpha)\cdot 100 \% $ {\textcolor{magenta}{confidence interval of $\theta$}, $2e$ is the size of confidence interval.\\[.1in]
\no $\spadesuit$ {\textcolor{red}{Remark 1:} $L, U$ in the first definition are functions of sample, that is, $L=L(X_1,\cdots, X_n)$ and $U=U(X_1,\cdots, X_n)$. In other words, $L, U$ are {\textcolor{magenta}{random variables}. \\[.1in]
\no $\spadesuit$ {\textcolor{red}{Remark 2:} In the above definitions, $\alpha$ is a value near 0.\\[.1in]
\no $\spadesuit$ {\textcolor{red}{Remark 3:} The true value $\theta$ is either within the confidence interval or not.\\[.1in]
\no $\spadesuit$ {\textcolor{red}{Remark 4:} $\min\limits_{\theta\in \Theta} P_\theta(\theta\in (L,U))$ is also called \textcolor{magenta}{confidence coefficients} (short for C.C.).


\foilhead[-.8in]{\textcolor{blue}{Interpretation of confidence intervals (CI):}}
\no {\textcolor{magenta}{Wrong interpretation:}
I computed a $95\%$ CI, it is $(2,8)$, so I can say my parameter $\theta$ must be in this interval with probability $95\%$. \\[.1in]
\no $\spadesuit$ {\textcolor{red}{This is WRONG!} $(2,8)$ is a fixed interval, it either contains or not contain $\theta$ (in other word, the probability for $\theta$ in this particular interval is either 0 or 1)\\[.1in]
\no {\textcolor{magenta}{Right interpretation:} \\[.1in]
\no {\textcolor{red}{(a)} The confidence interval can be expressed in terms of samples (or repeated samples): " Were this procedure to be repeated on multiple samples, the calculated confidence interval (which would differ for each sample) would encompass the true population parameter $90\%$ of the time"\\[.1in]
\no {\textcolor{red}{(b)} The probability associated with a confidence interval may also be considered from a pre-experiment point of view, in the same context in which arguments for the random allocation of treatments to study items are made. Here the experimenter sets out the way in which they intend to calculate a confidence interval and know, before they do the actual experiment, that the interval they will end up calculating has a certain chance of covering the true but unknown value. This is very similar to the "repeated sample" interpretation above, except that it avoids replying on considering hypothetical repeats of a sampling procedure that may not be repeatable in any meaningful sense.


\begin{figure}[h]
  \centering
  \epsfig{file=f7.pdf, height=8cm,width=16cm}
\end{figure}


\foilhead[-.8in]{\textcolor{blue}{Construct CI:}}
%\no $\spadesuit$ If the true value of the parameter lies outside the $90\%$ CI once it has been calculated, then an event has occurred which had a probability of $10\%$ (or less) of happening by chance\\[.1in]
%\no $\heartsuit$ However, we do not know the true value. For different parameters, we have different method to construct CI. \\[.1in]
\no {\textcolor{magenta}{Large sample CI for $\mu$:} \\[.1in]
\no $\spadesuit$ Situation: we have a large set of observed values ($n \ge 30$, usually) $x_1,\cdots,x_n$, that these values are realizations of i.i.d $X_{1}, \ldots, X_{n}$ with $E[{X}_i] = \mu$ and
$Var[{X}_i] = \sigma^{2}$.\\[.1in]
\no $\spadesuit$  $\bar{X}$ is an unbiased estimator for $\mu$\\[.1in]
\no $\spadesuit$ By CLT,
$\bar{X}$ is an
approximately normal distributed random variable with $E[\bar{X}] = \mu$ and $Var[\bar{X}] = \frac{\sigma^{2}}{n}$, i.e. $\bar{X} \sim N(\mu, \sigma^{2}/n)$ then $Z :=
\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}  \sim N(0,1) = \Phi$\\[.1in]
\newpage
\no {\textcolor{magenta}{Large sample CI of $\mu$:} An $(1-\alpha) \cdot$ 100\% confidence interval for $\mu$ is given
as
\[
\left (\bar{x} - z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}},\quad \bar{x} + z_{\alpha/2} \cdot
\frac{\sigma}{\sqrt{n}} \right )
\]
where $z_{\alpha/2} = \Phi^{-1}(1-{\alpha}/{2})$, $-z_{\alpha/2}=\Phi^{-1}(\alpha/2)$.\\[.1in]
 \no $\spadesuit$ In practice, we plug-in $s=\sqrt{\frac{1}{n-1}
\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}}$ for $\sigma$, when a value for it is not available. \\[.1in]
\no $\spadesuit$ How to find $\Phi^{-1}(p)$ from table? Some useful critical values, $z_{\alpha/2}$, depending on $\alpha$ are:

\begin{center}
\begin{tabular}{cc||cc}
    $\alpha$ &$z_{\alpha/2}$  & $\alpha$ & $z_{\alpha/2}$\\ \hline
    0.1 & 1.65 &0.02 & 2.33\\
    0.05 & 1.96 &  0.01 & 2.58
    \end{tabular}\end{center}
\no $\spadesuit$ Recall $\Phi^{-1}(1-\alpha/2)$ is defined by $P(Z\leq z_{\alpha/2})=1-\alpha/2$



\foilhead[-.8in]{\textcolor{blue}{Examples for CI:}}
\no {\textcolor{magenta}{Example 1:}
Suppose, we want to find a $95\%$ confidence interval for the mean salary of an ISU employee. A random sample of 100 ISU employees gives us a sample mean salary of 21543 = $\bar{x}$.   Suppose, the standard deviation of salaries is known to be 3000.\\[.1in]
\no $\heartsuit$  By using the above expression, we get a $95\%$ confidence interval as:
    \[
    21543 \pm \Phi^{-1}\left (1-\frac{\alpha}2 \right ) \cdot
    \frac{3000}{\sqrt{100}} = 21543 \pm \Phi^{-1}(0.975) \cdot
    300
    \]
    where $\Phi^{-1}(1-\alpha/2)=\Phi^{-1}(1-0.05/2)=\Phi^{-1}(0.975)=1.96$\\[.1in]
\no $\heartsuit$ The $95\%$  confidence interval is then:
$    21543 \pm 588$
    i.e. if we repeat this study 100 times (with 100 different
    employees each time), can say:
    in 95 out of 100 studies, the true parameter $\mu$ falls into a $588$ range around $\bar{x}$.

\no {\textcolor{magenta}{Example 2:}    Suppose, we want to analyze some complicated queueing system, for which we have no formulas
    and theory. We are interested in the mean queue length of the system after reaching steady state.\\[.1in]
\no $\spadesuit$ The only thing possible for us is to run simulations of this system and look at the queue length at some large time $t$, e.g. $t=1000$ hrs.\\[.1in]
\no $\spadesuit$    After 50 simulations, we have got data:\\
    \begin{tabular}{cl}
	$x_{1}$ & = number in queue at time 1000 hrs in 1st simulation \\
	$x_{2}$ & = number in queue at time 1000 hrs in 2nd simulation \\
	\ldots \\
	$x_{50}$ & = number in queue at time 1000 hrs in 50th simulation \\
    \end{tabular}\\
\no $\spadesuit$  Our observations yield an average queue length of $\bar{x} = 21.5$
    and  $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}} = 15$.\\[.1in]
\no $\spadesuit$  A $90\%$ confidence interval is given as ($\alpha=1-0.9=0.1$)
    \begin{eqnarray*}
    \left (\bar{x} - z_{0.05} \cdot \frac{s}{\sqrt{n}},\ \bar{x} + z_{0.05} \cdot
\frac{s}{\sqrt{n}} \right ) &=& \left (21.5 - 1.65 \cdot \frac{15}{\sqrt{50}},\ 21.5 +
1.65 \cdot \frac{15}{\sqrt{50}} \right )  \\
&=& (17.9998, 25.0002)
    \end{eqnarray*}

\foilhead[-.8in]{\textcolor{blue}{Construct CI (Cont'd):}}
\no {\textcolor{magenta}{Large sample CI for proportion $p$:} \\[.1in]
\no $\spadesuit$ $p$ denote a proportion of a large population or a probability\\[.1in]
\no $\spadesuit$ In order to get an estimate for this proportion, we can take a sample of $n$ individuals from the population
and check each one of them, whether or not they fulfill the criterion to be in that proportion of interest.\\[.1in]
\no $\spadesuit$ This corresponds to a Bernoulli-n-sequence, where we are only interested in the number of successes, X, which in our case corresponds to the number of individuals that qualify for the interesting subgroup.\\[.1in]
\no $\heartsuit$ Recall: Bernoulli random variable: $E(X)=p$ and $\text{Var}(X)=p(1-p)$\\[.1in]
\no $\spadesuit$ Unbiased estimator of $p$ is $\hat{p}=\bar{X}$ whose value is $\hat{p}=\bar{x}$. \\[.1in]
\no $\heartsuit$ $X_1,\cdots, X_n$ \emph{i.i.d.}, $E(\bar{X})=\frac1n\sum\limits_{i=1}^n E(X_i)=p$, and $\text{Var}(\bar{X})=\frac1n\text{Var}(X_1)=p(1-p)/n$\\[.1in]
\no $\heartsuit$ Using \emph{C.L.T.}, the $(1-\alpha)\cdot 100\%$ CI is $\hat{p}\pm e$ where $e=z_{\alpha/2}\cdot \sqrt{\frac{p(1-p)}n}$.\\[.1in]
\no $\diamondsuit$ In the above expression, we need use an appropriate value to substitute $p$:
\begin{enumerate}
\item \textcolor{magenta}{Conservative} method: $p:=0.5$, so that $(1-\alpha)\cdot 100\%$ CI for $p$ is $\hat{p}\pm z_{\alpha/2}/(2\sqrt{n})$ (why conservative?).
\item \textcolor{magenta}{Substitution} method: using $\hat{p}$ instead of $p$,  so that $(1-\alpha)\cdot 100\%$ CI for $p$ is $\hat{p}\pm z_{\alpha/2}\sqrt{\hat{p}(1-\hat{p})/n}$.
\end{enumerate}
\no $\diamondsuit$ {\textcolor{magenta}{Remarks:}
\begin{enumerate}
\item for large n or $\hat{p}$ is close to 0.5, there is almost no difference at all
\item conservative C.I. are larger than C.I. found
by substitution.
\end{enumerate}


\foilhead[-.8in]{\textcolor{blue}{Example:}}
\no  {\textcolor{magenta}{Example:} In the 2002 season the baseball player Sammy Sosa had a batting average of 0.288. (The batting average is the ratio of the number of hits and the times at bat.) Sammy Sosa was at bats 555 times in the 2002 season. Could the "true" batting average still be 0.300?\\[.1in]
\no $\spadesuit$ Compute a $95\%$ CI for the true batting average to study the above question. \\[.1in]
\no $\heartsuit$ Conservative Method gives:
$0.288\pm 1.96/({2\sqrt{555}})\Rightarrow 0.288\pm 0.042$ \\[.1in]
\no $\heartsuit$ Substitution Method gives:
$0.288\pm 1.96\cdot \sqrt{0.288(1-0.288)/555}\Rightarrow 0.288\pm 0.038$\\[.1in]
\no $\spadesuit$ The substitution method gives a slightly smaller confidence interval, but both intervals contain 0.3.\\[.1in]
 \no $\heartsuit$ There are not enough evidences to allow the conclusion that the true average is NOT 0.3, namely, $0.3$ could be the true average in 2002.


\end{document}





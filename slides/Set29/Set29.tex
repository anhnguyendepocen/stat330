\documentclass[20pt,landscape]{foils}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{pause}
\usepackage{graphicx}
\usepackage{epsfig}
%\usepackage{geometry}
%\geometry{headsep=3ex,hscale=0.9}
\newcommand{\bd}{\textbf}
\newcommand{\no}{\noindent}
\newcommand{\un}{\underline}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand \h {\hspace*{.3in}}
\newcommand{\bul}{\hspace*{.1in}{\textcolor{red}{$\bullet$ \ }}}
\newcommand{\xbar}{\bar{x}}
\rightheader{Stat 330 (Fall 2015): slide set 29}


\begin{document}
\LogoOff

\foilhead[1.3in]{}
\centerline{\LARGE \textcolor{blue}{Slide set 29}}
\vspace{0.3in}
\centerline{\large Stat 330 (Fall 2015)}
\vspace{0.2in}
\centerline{\tiny Last update: \today}
\setcounter{page}{0}

\foilhead[-.8in]{\textcolor{blue}{Topic 4: Regression}}
\no {\textcolor{magenta}{Motivations:} Statistical investigations only rarely focus on the distribution
of a single variable. We are often interested in comparisons among several variables, in changes in a variable over time, or in relationships among several variables.\\[.1in]
\no {\textcolor{magenta}{Ideas:} The idea of regression is that we have a random vector $(X_{1}, \ldots, X_{k})$ whose realization is $(x_1,\cdots, x_k)$ and try to approximate the behavior of  $Y$ by finding a function $g(X_{1}, \ldots, X_{k})$ such that $Y \approx g(X_{1}, \ldots, X_{k})$.\\[.1in]
\no \textcolor{magenta}{Target:}  We are going to talk about simple linear regression:  $k=1$ and $Y$ is approximately linearly related to $X$, e.g. $y=g(x) = b_{0} + b_{1}x$ is a linear function.\\[.1in]
\no (1). Scatterplot of $Y$ v.s. $X$ ($(x_i,y_i)$ on $x-y$ plane) should show the linear
    relationship.\\[.1in]
\no (2). linear relationship may be true only after a transformation of $X$ and/or $Y$, i.e. one needs to find the "right" scale for the variables.

\foilhead[-.8in]{\textcolor{blue}{Example}}
\no  $\spadesuit$ {\textcolor{magenta}{What does that mean by "right" scale?} For example,  $y \approx c x^{b}$ is nonlinear in $x$, but it
    implies that
    \[
    \underbrace{\ln y}_{=: y^{'}} \approx b \underbrace{\ln x}_{=:x^{'}} + \ln c,
    \]
    so on a log scale for both $x$ and $y$-axis, one gets a linear
    relationship.\\[1in]
%\no  {\textcolor{magenta}{Example (Mileage v.s. Weight):}
% Measurements on 38 1978-79 model automobiles.
%    Gas mileage in miles per gallon as measured by
%    Consumers' Union on a test track. Weight
%    as reported by automobile manufacturer.\\[.1in]
%    
%    \newpage
%\no $\diamond$ A scatterplot of mpg versus weight shows an indirect proportional relationship. \\[.1in]
%\no $\diamond$  However, via transforming \textcolor{magenta}{weight} by $\frac{1}{x}$ to \textcolor{magenta}{weight$^{-1}$}, a scatterplot of mpg versus weight$^{-1}$ reveals a linear relationship.
%\begin{figure}[h]
%  \centering
%  \epsfig{file=f10.pdf, height=10cm}
%\end{figure}

\newpage
\no $\heartsuit$ {\textcolor{magenta}{Example (Olympics - long jump):}   Results for the long jump for all olympic games between 1900 and 1996 are:\\

 \centerline{\footnotesize \begin{tabular}{cc|cc|cc|cc}
	year & long jump (in m) & year & long jump (in m) & year & long jump (in m) & year & long jump (in m)\\ \hline\hline
	1900 & 7.19 & 1904 & 7.34 & 1908 & 7.48 & 1912 & 7.60 \\
 1920 & 7.15 &1924 & 7.45 &1928 & 7.74 &1932 & 7.64  \\
 1936 & 8.06 & 1948 & 7.82 &1952 & 7.57 & 1956 & 7.83  \\
 1960 & 8.12 &  1964 & 8.07 & 1968 & 8.90 &1972 & 8.24 \\
1976 & 8.34 & 1980 & 8.54& 	1984 & 8.54&  1988 & 8.72 \\
	 1992 & 8.67& 	1996 & 8.50\\
     \end{tabular}}
   
   
   \newpage
\no  A scatterplot of long jump versus year shows:
 \begin{figure}[h]
  \centering
  \epsfig{file=f12.pdf, height=8cm,width=12cm}
\end{figure}
\no   The plot shows that it is perhaps reasonable to say that
    \[
    y \approx \beta_{0} + \beta_{1} x
    \]

\foilhead[-.8in]{\textcolor{blue}{Regression via least square}}
\no  $\spadesuit$ {\textcolor{magenta}{Least square:} The first issue is: if we accept
that $y \approx \beta_{0} + \beta_{1} x$, how do we derive estimates
of $\beta_{0}, \beta_{1}$ from $n$ data points?
The standard answer is the "least squares" principle:\\[.1in]
\no $\spadesuit$ $b_0$ and $b_1$ are estimates for $\beta_0$ and $\beta_1$ given the data (sometimes, denoted by $\hat{\beta}_0$ and $\hat{\beta}_1$)
 \begin{figure}[h]
  \centering
  \epsfig{file=f13.pdf, height=7cm,width=14cm}
\end{figure}
\no $\spadesuit$ The least square solution will produce the ``best fitting line''.\\
\no $\spadesuit$ In comparing lines that might be drawn through the plot we look at:
\[
Q(b_{0}, b_{1}) = \sum_{i=1}^{n}\left( y_{i} - (b_{0} + b_{1}x_{i})
\right )^{2}
\]
\no $\spadesuit$  So, we look at the sum of squared vertical distances from points to
the line and attempt to minimize this sum of squares:
 \begin{eqnarray*}
    \frac{\partial}{\partial b_{0}} Q(b_{0}, b_{1}) &=& - 2 \sum_{i=1}^{n} \left( y_{i} - (b_{0} + b_{1}x_{i})
\right ) \\
\frac{\partial}{\partial b_{1}} Q(b_{0}, b_{1}) &=& - 2 \sum_{i=1}^{n} x_{i} \left( y_{i} - (b_{0} + b_{1}x_{i})
\right ) \\
\end{eqnarray*}
\no $\spadesuit$ Setting the derivatives to zero gives:
\begin{eqnarray*}
    nb_{0} - b_{1} \sum_{i=1}^{n} x_{i} &=& \sum_{i=1}^{n}y_{i} \\
    b_{0} \sum_{i=1}^{n} x_{i} - b_{1} \sum_{i=1}^{n} x_{i}^{2} &=&
    \sum_{i=1}^{n} x_{i}y_{i}
\end{eqnarray*}
\no $\spadesuit$ Least squares solutions for $b_{0}$ and $b_{1}$ are:
\begin{eqnarray*}
    b_{1} &=& \frac{\sum_{i=1}^{n} (x_{i} - \bar{x})(y_{i} -
    \bar{y})}{\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}} =
    \frac{\sum_{i=1}^{n}x_{i}y_{i} - \frac{1}{n}\sum_{i=1}^{n}x_{i}
    \cdot \sum_{i=1}^{n}y_{i}}{\sum_{i=1}^{n}x_{i}^{2} - \frac{1}{n}
    \left ( \sum_{i=1}^{n}x_{i} \right )^{2}}= \text{\textcolor{magenta}{slope}} \\
    b_{0} &=& \bar{y} - \bar{x} b_{1} = \frac{1}{n}
    \sum_{i=1}^{n}y_{i} - b_{1} \frac{1}{n} \sum_{i=1}^{n}x_{i}=\text{\textcolor{magenta}{$y-$ intercept}} \\
\end{eqnarray*}

\foilhead[-.8in]{\textcolor{blue}{Example on regression}}
\no  $\spadesuit$ {\textcolor{magenta}{Example (Olympics long jump game):}  $X$ := \# of years from 1900 (sample value denoted by $x=year-1900$), $Y$ := long jump (value: $y$), so
 \begin{eqnarray*}
	&& \sum_{i=1}^{n} x_{i} = 1100, \hspace{1cm}
	\sum_{i=1}^{n}x_{i}^{2 } = 74608 \\
	&& \sum_{i=1}^{n} y_{i} = 175.518, \hspace{1cm}
	\sum_{i=1}^{n}y_{i}^{2} = 1406.109, \hspace{1cm}
	\sum_{i=1}^{n}x_{i}y_{i} = 9079.584
    \end{eqnarray*}
\no $\spadesuit$   The parameters for the best fitting line are:
  $$
	b_{1}= \frac{9079.584 - \frac{1100 \cdot 175.518}{22}}{74608 -
	\frac{1100^{2}}{22}} = 0.0155  , \
	b_{0} =\frac{175.518}{22} - \frac{1100}{22} \cdot 0.0155 = 7.2037
 $$

 \newpage
\no $\spadesuit$   The regression equation is
 ``high jump = $7.204 + 0.016\ (year-1900)$".

 \no$\heartsuit$ Predict $Y$ when $X=1986-1900=86$:
 \[ b_0 + b_1\times 86 = 7.2037 + 0.0155\times 86 = 8.537 \]

\foilhead[-.8in]{\textcolor{blue}{Correlation and regression line}}
%\no  $\spadesuit$ {\textcolor{magenta}{Motivations:}  It is useful for addition, to be able to judge how well the line
%describes the data - i.e. how ``linear looking'' a plot really is.\\[.1in]
\no $\spadesuit$ To measure linear association between random variables $X$ and $Y$, we
would compute correlation $\rho$ if we had their joint distribution.\\[.1in]
\no $\spadesuit$ {\textcolor{magenta}{The sample correlation $r$} is what
  we would get from the sample.\\[.1in]
\no $\heartsuit$ {\textcolor{magenta}{Formula for $r$}
\begin{align*}
r &:= \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-
\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2} \cdot
\sum_{i=1}^{n}(y_{i}- \bar{y})^2}}\\
& =
\frac{\sum_{i=1}^{n}x_{i}y_{i} - \frac{1}{n}\sum_{i=1}^{n}x_{i} \cdot
\sum_{i=1}^{n}y_{i}}{\sqrt{\left ( \sum_{i=1}^{n}x_{i}^{2} -
\frac{1}{n} \left ( \sum_{i=1}^{n} x_{i} \right )^{2} \right ) \left ( \sum_{i=1}^{n}y_{i}^{2} -
\frac{1}{n} \left ( \sum_{i=1}^{n} y_{i} \right )^{2} \right )}}
\end{align*}
\no $\heartsuit$ The numerator is the numerator of $b_{1}$, one part under the root of
the denominator is the denominator of $b_{1}$.
\\[.1in]
\no The sample correlation $r$ is connected to the theoretical correlation $\rho$, so some nontrivial results are expected
\begin{itemize}
    \item $ -1 \le r \le 1$
    \item $r = \pm 1$ exactly, when all $(x,y)$ data pairs fall on a
    single straight line.
    \item $r$ has the same sign as $b_{1}$.
\end{itemize}
\no $\spadesuit$ {\textcolor{magenta}{Example (Olympics-continued):}
\[
    r = \frac{9079.584 - \frac{1100 \cdot
    175.518}{22}}{\sqrt{(74608 - \frac{1100^{2}}{22})(1406.109 -
    \frac{175.518^{2}}{22})}} = 0.8997
    \]
\no $\spadesuit$ Both $b_1>0$, and $r>0$, which corresponds to positive correlation or increasing trend





\end{document}




